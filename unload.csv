link|title|description
http://arxiv.org/abs/1903.11593|What  does  AI  see?  Deep  segmentation  networks  discover  biomarkers  for  lung  cancer  survival.  (arXiv:1903.11593v1  [eess.IV])|<p>Non-small-cell  lung  cancer  (NSCLC)  represents  approximately  80-85%  of  lungcancer  diagnoses  and  is  the  leading  cause  of  cancer-related  death  worldwide.Recent  studies  indicate  that  image-based  radiomics  features  from  positronemission  tomography-computed  tomography  (PET/CT)  images  have  predictive  poweron  NSCLC  outcomes.  To  this  end,  easily  calculated  functional  features  such  asthe  maximum  and  the  mean  of  standard  uptake  value  (SUV)  and  total  lesionglycolysis  (TLG)  are  most  commonly  used  for  NSCLC  prognostication,  but  theirprognostic  value  remains  controversial.  Meanwhile,  convolutional  neuralnetworks  (CNN)  are  rapidly  emerging  as  a  new  premise  for  cancer  image  analysis,with  significantly  enhanced  predictive  power  compared  to  other  hand-craftedradiomics  features.  Here  we  show  that  CNN  trained  to  perform  the  tumorsegmentation  task,  with  no  other  information  than  physician  contours,  identifya  rich  set  of  survival-related  image  features  with  remarkable  prognostic  value.In  a  retrospective  study  on  96  NSCLC  patients  before  stereotactic-bodyradiotherapy  (SBRT),  we  found  that  the  CNN  segmentation  algorithm  (U-Net)trained  for  tumor  segmentation  in  PET/CT  images,  contained  features  havingstrong  correlation  with  2-  and  5-year  overall  and  disease-specific  survivals.The  U-net  algorithm  has  not  seen  any  other  clinical  information  (e.g.  survival,age,  smoking  history)  than  the  images  and  the  corresponding  tumor  contoursprovided  by  physicians.  Furthermore,  through  visualization  of  the  U-Net,  wealso  found  convincing  evidence  that  the  regions  of  progression  appear  to  matchwith  the  regions  where  the  U-Net  features  identified  patterns  that  predictedhigher  likelihood  of  death.  We  anticipate  our  findings  will  be  a  starting  pointfor  more  sophisticated  non-intrusive  patient  specific  cancer  prognosisdetermination.</p>
http://arxiv.org/abs/1903.11596|Detecting  service  provider  alliances.  (arXiv:1903.11596v1  [cs.GT])|<p>We  present  an  algorithm  for  detecting  service  provider  alliances.  To  performthis,  we  modelize  a  cooperative  game-theoretic  model  for  competitor  serviceproviders.  A  choreography  (a  peer-to-peer  service  composition  model)  needs  aset  of  services  to  fulfill  its  requirements.  Users  must  choose,  for  eachrequirement,  which  service  providers  will  be  used  to  enact  the  choreography  atlowest  cost.  Due  to  the  lack  of  centralization,  vendors  can  form  alliances  tocontrol  the  market.  We  propose  a  novel  algorithm  capable  of  detecting  alliancesamong  service  providers,  based  on  our  findings  showing  that  this  game  has  anempty  core,  but  a  non-empty  bargaining  set.</p>
http://arxiv.org/abs/1903.11598|A  Simple  Haploid-Diploid  Evolutionary  Algorithm.  (arXiv:1903.11598v1  [cs.NE])|<p>It  has  recently  been  suggested  that  evolution  exploits  a  form  of  fitnesslandscape  smoothing  within  eukaryotic  sex  due  to  the  haploid-diploid  cycle.This  short  paper  presents  a  simple  modification  to  the  standard  evolutionarycomputing  algorithm  to  similarly  benefit  from  the  process.  Using  the  well-knownNK  model  of  fitness  landscapes  it  is  shown  that  the  benefit  emerges  asruggedness  is  increased.</p>
http://arxiv.org/abs/1903.11621|Self-adaptive  decision-making  mechanisms  to  balance  the  execution  of  multiple  tasks  for  a  multi-robots  team.  (arXiv:1903.11621v1  [cs.RO])|<p>This  work  addresses  the  coordination  problem  of  multiple  robots  with  the  goalof  finding  specific  hazardous  targets  in  an  unknown  area  and  dealing  with  themcooperatively.  The  desired  behaviour  for  the  robotic  system  entails  multiplerequirements,  which  may  also  be  conflicting.  The  paper  presents  the  problem  asa  constrained  bi-objective  optimization  problem  in  which  mobile  robots  mustperform  two  specific  tasks  of  exploration  and  at  same  time  cooperation  andcoordination  for  disarming  the  hazardous  targets.  These  objectives  are  opposedgoals,  in  which  one  may  be  favored,  but  only  at  the  expense  of  the  other.Therefore,  a  good  trade-off  must  be  found.  For  this  purpose,  a  nature-inspiredapproach  and  an  analytical  mathematical  model  to  solve  this  problem  consideringa  single  equivalent  weighted  objective  function  are  presented.  The  results  ofproposed  coordination  model,  simulated  in  a  two  dimensional  terrain,  are  showedin  order  to  assess  the  behaviour  of  the  proposed  solution  to  tackle  thisproblem.  We  have  analyzed  the  performance  of  the  approach  and  the  influence  ofthe  weights  of  the  objective  function  under  different  conditions:  static  anddynamic.  In  this  latter  situation,  the  robots  may  fail  under  the  stringentlimited  budget  of  energy  or  for  hazardous  events.  The  paper  concludes  with  acritical  discussion  of  the  experimental  results.</p>
http://arxiv.org/abs/1903.11626|Bridging  Adversarial  Robustness  and  Gradient  Interpretability.  (arXiv:1903.11626v1  [cs.LG])|<p>Adversarial  training  is  a  training  scheme  designed  to  counter  adversarialattacks  by  augmenting  the  training  dataset  with  adversarial  examples.Surprisingly,  several  studies  have  observed  that  loss  gradients  fromadversarially  trained  DNNs  are  visually  more  interpretable  than  those  fromstandard  DNNs.  Although  this  phenomenon  is  interesting,  there  are  only  fewworks  that  have  offered  an  explanation.  In  this  paper,  we  attempted  to  bridgethis  gap  between  adversarial  robustness  and  gradient  interpretability.  To  thisend,  we  identified  that  loss  gradients  from  adversarially  trained  DNNs  alignbetter  with  human  perception  because  adversarial  training  restricts  gradientscloser  to  the  image  manifold.  We  then  demonstrated  that  adversarial  trainingcauses  loss  gradients  to  be  quantitatively  meaningful.  Finally,  we  showed  thatunder  the  adversarial  training  framework,  there  exists  an  empirical  trade-offbetween  test  accuracy  and  loss  gradient  interpretability  and  proposed  twopotential  approaches  to  resolving  this  trade-off.</p>
http://arxiv.org/abs/1903.11633|Laplace  Landmark  Localization.  (arXiv:1903.11633v1  [cs.CV])|<p>Landmark  localization  in  images  and  videos  is  a  classic  problem  solved  invarious  ways.  Nowadays,  with  deep  networks  prevailing  throughout  machinelearning,  there  are  revamped  interests  in  pushing  facial  landmark  detectiontechnologies  to  handle  more  challenging  data.  Most  efforts  use  networkobjectives  based  on  L1  or  L2  norms,  which  have  several  disadvantages.  First  ofall,  the  locations  of  landmarks  are  determined  from  generated  heatmaps  (i.e.,confidence  maps)  from  which  predicted  landmark  locations  (i.e.,  the  means)  getpenalized  without  accounting  for  the  spread:  a  high  scatter  corresponds  to  lowconfidence  and  vice-versa.  For  this,  we  introduce  a  LaplaceKL  objective  thatpenalizes  for  a  low  confidence.  Another  issue  is  a  dependency  on  labeled  data,which  are  expensive  to  obtain  and  susceptible  to  error.  To  address  both  issueswe  propose  an  adversarial  training  framework  that  leverages  unlabeled  data  toimprove  model  performance.  Our  method  claims  state-of-the-art  on  all  of  the300W  benchmarks  and  ranks  second-to-best  on  the  Annotated  Facial  Landmarks  inthe  Wild  (AFLW)  dataset.  Furthermore,  our  model  is  robust  with  a  reduced  size:1/8  the  number  of  channels  (i.e.,  0.0398MB)  is  comparable  to  state-of-that-artin  real-time  on  CPU.  Thus,  we  show  that  our  method  is  of  high  practical  valueto  real-life  application.</p>
http://arxiv.org/abs/1903.11635|Fourier  Entropy-Influence  Conjecture  for  Random  Linear  Threshold  Functions.  (arXiv:1903.11635v1  [cs.CC])|<p>The  Fourier-Entropy  Influence  (FEI)  Conjecture  states  that  for  any  Booleanfunction  $f:{+1,-1}^n  	o  {+1,-1}$,  the  Fourier  entropy  of  $f$  is  at  mostits  influence  up  to  a  universal  constant  factor.  While  the  FEI  conjecture  hasbeen  proved  for  many  classes  of  Boolean  functions,  it  is  still  not  knownwhether  it  holds  for  the  class  of  Linear  Threshold  Functions.  A  naturalquestion  is:  Does  the  FEI  conjecture  hold  for  a  `random'  linear  thresholdfunction?  In  this  paper,  we  answer  this  question  in  the  affirmative.  Weconsider  two  natural  distributions  on  the  weights  defining  a  linear  thresholdfunction,  namely  uniform  distribution  on  $[-1,1]$  and  Normal  distribution.</p>
http://arxiv.org/abs/1903.11640|Fundamental  Limits  of  Covert  Packet  Insertion.  (arXiv:1903.11640v1  [cs.CR])|<p>Covert  communication  conceals  the  existence  of  the  transmission  from  awatchful  adversary.  We  consider  the  fundamental  limits  for  covertcommunications  via  packet  insertion  over  packet  channels  whose  packet  timingsare  governed  by  a  renewal  process  of  rate  $lambda$.  Authorized  transmitterJack  sends  packets  to  authorized  receiver  Steve,  and  covert  transmitter  Alicewishes  to  transmit  packets  to  covert  receiver  Bob  without  being  detected  bywatchful  adversary  Willie.  Willie  cannot  authenticate  the  source  of  thepackets.  Hence,  he  looks  for  statistical  anomalies  in  the  packet  stream  fromJack  to  Steve  to  attempt  detection  of  unauthorized  packet  insertion.  First,  weconsider  a  special  case  where  the  packet  timings  are  governed  by  a  Poissonprocess  and  we  show  that  Alice  can  covertly  insert  $mathcal{O}(sqrt{lambdaT})$  packets  for  Bob  in  a  time  interval  of  length  $T$;  conversely,  if  Aliceinserts  $omega(sqrt{lambda  T})$,  she  will  be  detected  by  Willie  with  highprobability.  Then,  we  extend  our  results  to  general  renewal  channels  and  showthat  in  a  stream  of  $N$  packets  transmitted  by  Jack,  Alice  can  covertly  insert$mathcal{O}(sqrt{N})$  packets;  if  she  inserts  $omega(sqrt{N})$  packets,  shewill  be  detected  by  Willie  with  high  probability.</p>
http://arxiv.org/abs/1903.11649|Align2Ground:  Weakly  Supervised  Phrase  Grounding  Guided  by  Image-Caption  Alignment.  (arXiv:1903.11649v1  [cs.CV])|<p>We  address  the  problem  of  grounding  free-form  textual  phrases  by  using  weaksupervision  from  image-caption  pairs.  We  propose  a  novel  end-to-end  model  thatuses  caption-to-image  retrieval  as  a  `downstream'  task  to  guide  the  process  ofphrase  localization.  Our  method,  as  a  first  step,  infers  the  latentcorrespondences  between  regions-of-interest  (RoIs)  and  phrases  in  the  captionand  creates  a  discriminative  image  representation  using  these  matched  RoIs.  Ina  subsequent  step,  this  (learned)  representation  is  aligned  with  the  caption.Our  key  contribution  lies  in  building  this  `caption-conditioned'  image  encodingwhich  tightly  couples  both  the  tasks  and  allows  the  weak  supervision  toeffectively  guide  visual  grounding.  We  provide  an  extensive  empirical  andqualitative  analysis  to  investigate  the  different  components  of  our  proposedmodel  and  compare  it  with  competitive  baselines.  For  phrase  localization,  wereport  an  improvement  of  4.9%  (absolute)  over  the  prior  state-of-the-art  on  theVisualGenome  dataset.  We  also  report  results  that  are  at  par  with  thestate-of-the-art  on  the  downstream  caption-to-image  retrieval  task  on  COCO  andFlickr30k  datasets.</p>
http://arxiv.org/abs/1903.11650|Lens-based  Millimeter  Wave  Reconfigurable  Antenna  NOMA.  (arXiv:1903.11650v1  [cs.IT])|<p>This  paper  proposes  a  new  multiple  access  technique  based  on  the  millimeterwave  lens-based  reconfigurable  antenna  systems.  In  particular,  to  support  alarge  number  of  groups  of  users  with  different  angles  of  departures  (AoDs),  weintegrate  recently  proposed  reconfigurable  antenna  multiple  access  (RAMA)  intonon-orthogonal  multiple  access  (NOMA).  The  proposed  technique,  namedreconfigurable  antenna  NOMA  (RA-NOMA),  divides  the  users  with  respect  to  theirAoDs  and  channel  gains.  Users  with  different  AoDs  and  comparable  channel  gainsare  served  via  RAMA  while  users  with  the  same  AoDs  but  different  channel  gainsare  served  via  NOMA.  This  technique  results  in  the  independence  of  the  numberof  radio  frequency  chains  from  the  number  of  NOMA  groups.  Further,  we  derivethe  feasibility  conditions  and  show  that  the  power  allocation  for  RA-NOMA  is  aconvex  problem.  We  then  derive  the  maximum  achievable  sum-rate  of  RA-NOMA.Simulation  results  show  that  RA-NOMA  outperforms  conventional  orthogonalmultiple  access  (OMA)  as  well  as  the  combination  of  RAMA  with  the  OMAtechniques.</p>
http://arxiv.org/abs/1903.11665|Microservice  Transition  and  its  Granularity  Problem:  A  Systematic  Mapping  Study.  (arXiv:1903.11665v1  [cs.SE])|"<p>Microservices  have  gained  wide  recognition  and  acceptance  in  softwareindustries  as  an  emerging  architectural  style  for  autonomic,  scalable,  and  morereliable  computing.  The  transition  to  microservices  has  been  highly  motivatedby  the  need  for  better  alignment  of  technical  design  decisions  with  improvingvalue  potentials  of  architectures.  Despite  microservices'  popularity,  researchstill  lacks  disciplined  understanding  of  transition  and  consensus  on  theprinciples  and  activities  underlying  ""micro-ing""  architectures.  In  this  paper,we  report  on  a  systematic  mapping  study  that  consolidates  various  views,approaches  and  activities  that  commonly  assist  in  the  transition  tomicroservices.  The  study  aims  to  provide  a  better  understanding  of  thetransition;  it  also  contributes  a  working  definition  of  the  transition  andtechnical  activities  underlying  it.  We  term  the  transition  and  technicalactivities  leading  to  microservice  architectures  as  microservitization.  We  thenshed  light  on  a  fundamental  problem  of  microservitization:  microservicegranularity  and  reasoning  about  its  adaptation  as  first-class  entities.  Thisstudy  reviews  state-of-the-art  and  -practice  related  to  reasoning  aboutmicroservice  granularity;  it  reviews  modelling  approaches,  aspects  considered,guidelines  and  processes  used  to  reason  about  microservice  granularity.  Thisstudy  identifies  opportunities  for  future  research  and  development  related  toreasoning  about  microservice  granularity.</p>"
http://arxiv.org/abs/1903.11670|The  minimum  value  of  the  Colless  index.  (arXiv:1903.11670v1  [q-bio.PE])|<p>The  Colless  index  is  one  of  the  oldest  and  most  widely  used  balance  indicesfor  rooted  bifurcating  trees.  Despite  its  popularity,  its  minimum  value  on  thespace  $mathcal{T}_n$  of  rooted  bifurcating  trees  with  $n$  leaves  is  only  knownwhen  $n$  is  a  power  of  2.  In  this  paper  we  fill  this  gap  in  the  literature,  byproviding  a  formula  that  computes,  for  each  $n$,  the  minimum  Colless  index  on$mathcal{T}_n$,  and  characterizing  those  trees  where  this  minimum  value  isreached.</p>
http://arxiv.org/abs/1903.11672|MuSE-ing  on  the  Impact  of  Utterance  Ordering  On  Crowdsourced  Emotion  Annotations.  (arXiv:1903.11672v1  [cs.SD])|"<p>Emotion  recognition  algorithms  rely  on  data  annotated  with  high  qualitylabels.  However,  emotion  expression  and  perception  are  inherently  subjective.There  is  generally  not  a  single  annotation  that  can  be  unambiguously  declared""correct"".  As  a  result,  annotations  are  colored  by  the  manner  in  which  theywere  collected.  In  this  paper,  we  conduct  crowdsourcing  experiments  toinvestigate  this  impact  on  both  the  annotations  themselves  and  on  theperformance  of  these  algorithms.  We  focus  on  one  critical  question:  the  effectof  context.  We  present  a  new  emotion  dataset,  Multimodal  Stressed  Emotion(MuSE),  and  annotate  the  dataset  using  two  conditions:  randomized,  in  whichannotators  are  presented  with  clips  in  random  order,  and  contextualized,  inwhich  annotators  are  presented  with  clips  in  order.  We  find  that  contextuallabeling  schemes  result  in  annotations  that  are  more  similar  to  a  speaker's  ownself-reported  labels  and  that  labels  generated  from  randomized  schemes  are  mosteasily  predictable  by  automated  systems.</p>"
http://arxiv.org/abs/1903.11673|Adversarial  Deep  Learning  in  EEG  Biometrics.  (arXiv:1903.11673v1  [cs.LG])|<p>Deep  learning  methods  for  person  identification  based  onelectroencephalographic  (EEG)  brain  activity  encounters  the  problem  ofexploiting  the  temporally  correlated  structures  or  recording  session  specificvariability  within  EEG.  Furthermore,  recent  methods  have  mostly  trained  andevaluated  based  on  single  session  EEG  data.  We  address  this  problem  from  aninvariant  representation  learning  perspective.  We  propose  an  adversarialinference  approach  to  extend  such  deep  learning  models  to  learnsession-invariant  person-discriminative  representations  that  can  providerobustness  in  terms  of  longitudinal  usability.  Using  adversarial  learningwithin  a  deep  convolutional  network,  we  empirically  assess  and  showimprovements  with  our  approach  based  on  longitudinally  collected  EEG  data  forperson  identification  from  half-second  EEG  epochs.</p>
http://arxiv.org/abs/1903.11674|On  Inversely  Proportional  Hypermutations  with  Mutation  Potential.  (arXiv:1903.11674v1  [cs.NE])|<p>Artificial  Immune  Systems  (AIS)  employing  hypermutations  with  linear  staticmutation  potential  have  recently  been  shown  to  be  very  effective  at  escapinglocal  optima  of  combinatorial  optimisation  problems  at  the  expense  of  beingslower  during  the  exploitation  phase  compared  to  standard  evolutionaryalgorithms.  In  this  paper  we  prove  that  considerable  speed-ups  in  theexploitation  phase  may  be  achieved  with  dynamic  inversely  proportional  mutationpotentials  (IPM)  and  argue  that  the  potential  should  decrease  inversely  to  thedistance  to  the  optimum  rather  than  to  the  difference  in  fitness.  Afterwards  wedefine  a  simple  (1+1)~Opt-IA,  that  uses  IPM  hypermutations  and  ageing,  forrealistic  applications  where  optimal  solutions  are  unknown.  The  aim  of  the  AISis  to  approximate  the  ideal  behaviour  of  the  inversely  proportionalhypermutations  better  and  better  as  the  search  space  is  explored.  We  prove  thatsuch  desired  behaviour,  and  related  speed-ups,  occur  for  a  well-studied  bimodalbenchmark  function  called  	extsc{TwoMax}.  Furthermore,  we  prove  that  the(1+1)~Opt-IA  with  IPM  efficiently  optimises  a  third  bimodal  function,	extsc{Cliff},  by  escaping  its  local  optima  while  Opt-IA  with  static  potentialcannot,  thus  requires  exponential  expected  runtime  in  the  distance  between  thecliff  and  the  optimum.</p>
http://arxiv.org/abs/1903.11677|Exact  Byzantine  Consensus  on  Undirected  Graphs  under  Local  Broadcast  Model.  (arXiv:1903.11677v1  [cs.DC])|<p>This  paper  considers  the  Byzantine  consensus  problem  for  nodes  with  binaryinputs.  The  nodes  are  interconnected  by  a  network  represented  as  an  undirectedgraph,  and  the  system  is  assumed  to  be  synchronous.  Under  the  classicalpoint-to-point  communication  model,  it  is  well-known  [7]  that  the  following  twoconditions  are  both  necessary  and  sufficient  to  achieve  Byzantine  consensusamong  $n$  nodes  in  the  presence  of  up  to  $f$  Byzantine  faulty  nodes:  $n  ge3f+1$  and  vertex  connectivity  at  least  $2f+1$.  In  the  classical  point-to-pointcommunication  model,  it  is  possible  for  a  faulty  node  to  equivocate,  i.e.,transmit  conflicting  information  to  different  neighbors.  Such  equivocation  ispossible  because  messages  sent  by  a  node  to  one  of  its  neighbors  are  notoverheard  by  other  neighbors.</p><p>This  paper  considers  the  local  broadcast  model.  In  contrast  to  thepoint-to-point  communication  model,  in  the  local  broadcast  model,  messages  sentby  a  node  are  received  identically  by  all  of  its  neighbors.  Thus,  under  thelocal  broadcast  model,  attempts  by  a  node  to  send  conflicting  information  canbe  detected  by  its  neighbors.  Under  this  model,  we  show  that  the  following  twoconditions  are  both  necessary  and  sufficient  for  Byzantine  consensus:  vertexconnectivity  at  least  $lfloor  3f/2  floor  +  1$  and  minimum  node  degree  atleast  $2f$.  Observe  that  the  local  broadcast  model  results  in  a  lowerrequirement  for  connectivity  and  the  number  of  nodes  $n$,  as  compared  to  thepoint-to-point  communication  model.</p><p>We  extend  the  above  results  to  a  hybrid  model  that  allows  some  of  theByzantine  faulty  nodes  to  equivocate.  The  hybrid  model  bridges  the  gap  betweenthe  point-to-point  and  local  broadcast  models,  and  helps  to  preciselycharacterize  the  trade-off  between  equivocation  and  network  requirements.</p>
http://arxiv.org/abs/1903.11678|Tree  Search  vs  Optimization  Approaches  for  Map  Generation.  (arXiv:1903.11678v1  [cs.AI])|<p>Search-based  procedural  content  generation  uses  stochastic  globaloptimization  algorithms  to  search  spaces  of  game  content.  However,  it  has  beenfound  that  tree  search  can  be  competitive  with  evolution  on  certainoptimization  problems.  We  investigate  the  applicability  of  several  tree  searchmethods  to  map  generation  and  compare  them  systematically  with  severaloptimization  algorithms,  including  evolutionary  algorithms.  For  purposes  ofcomparison,  we  use  a  simplified  map  generation  problem  where  only  passable  andimpassable  tiles  exist,  three  different  map  representations,  and  a  set  ofobjectives  that  are  representative  of  those  commonly  found  in  actual  levelgeneration  problem.  While  the  results  suggest  that  evolutionary  algorithmsproduce  good  maps  faster,  several  tree  search  methods  can  perform  very  wellgiven  sufficient  time,  and  there  are  interesting  differences  in  the  characterof  the  generated  maps  depending  on  the  algorithm  chosen,  even  for  the  samerepresentation  and  objective.</p>
http://arxiv.org/abs/1903.11680|Gradient  Descent  with  Early  Stopping  is  Provably  Robust  to  Label  Noise  for  Overparameterized  Neural  Networks.  (arXiv:1903.11680v1  [cs.LG])|<p>Modern  neural  networks  are  typically  trained  in  an  over-parameterized  regimewhere  the  parameters  of  the  model  far  exceed  the  size  of  the  training  data.  Dueto  over-parameterization  these  neural  networks  in  principle  have  the  capacityto  (over)fit  any  set  of  labels  including  pure  noise.  Despite  this  high  fittingcapacity,  somewhat  paradoxically,  neural  network  models  trained  via  first-ordermethods  continue  to  predict  well  on  yet  unseen  test  data.  In  this  paper  we  takea  step  towards  demystifying  this  phenomena.  In  particular  we  show  that  firstorder  methods  such  as  gradient  descent  are  provably  robust  to  noise/corruptionon  a  constant  fraction  of  the  labels  despite  over-parametrization  under  a  richdataset  model.  In  particular:  i)  First,  we  show  that  in  the  first  fewiterations  where  the  updates  are  still  in  the  vicinity  of  the  initializationthese  algorithms  only  fit  to  the  correct  labels  essentially  ignoring  the  noisylabels.  ii)  Secondly,  we  prove  that  to  start  to  overfit  to  the  noisy  labelsthese  algorithms  must  stray  rather  far  from  from  the  initial  model  which  canonly  occur  after  many  more  iterations.  Together,  these  show  that  gradientdescent  with  early  stopping  is  provably  robust  to  label  noise  and  shed  light  onempirical  robustness  of  deep  networks  as  well  as  commonly  adopted  heuristics  toprevent  overfitting.</p>
http://arxiv.org/abs/1903.11682|From  closed  to  open  access:  A  case  study  of  flipped  journals.  (arXiv:1903.11682v1  [cs.DL])|"<p>In  recent  years,  increased  stakeholder  pressure  to  transition  research  toOpen  Access  has  led  to  many  journals  ""flipping""  from  a  toll  access  to  an  openaccess  publishing  model.  Changing  the  publishing  model  can  influence  thedecision  of  authors  to  submit  their  papers  to  a  journal,  and  increased  articleaccessibility  may  influence  citation  behaviour.  The  aim  of  this  paper  is  toshow  changes  in  the  number  of  published  articles  and  citations  after  theflipping  of  a  journal.  We  analysed  a  set  of  171  journals  in  the  Web  of  Science(WoS)  which  flipped  to  open  access.  In  addition  to  comparing  the  number  ofarticles,  average  relative  citation  (ARC)  and  normalized  impact  factor  (IF)  areapplied,  respectively,  as  bibliometric  indicators  at  the  article  and  journallevel,  to  trace  the  transformation  of  flipped  journals  covered.  Our  resultsshow  that  flipping  mostly  has  had  positive  effects  on  journal's  IF.  But  it  hashad  no  obvious  citation  advantage  for  the  articles.  We  also  see  a  decline  inthe  number  of  published  articles  after  flipping.  We  can  conclude  that  flippingto  open  access  can  improve  the  performance  of  journals,  despite  decreasing  thetendency  of  authors  to  submit  their  articles  and  no  better  citation  advantagesfor  articles.</p>"
http://arxiv.org/abs/1903.11683|Outlier-Robust  Spatial  Perception:  Hardness,  General-Purpose  Algorithms,  and  Guarantees.  (arXiv:1903.11683v1  [stat.ML])|<p>Spatial  perception  is  the  backbone  of  many  robotics  applications,  and  spans  abroad  range  of  research  problems,  including  localization  and  mapping,  pointcloud  alignment,  and  relative  pose  estimation  from  camera  images.  Robustspatial  perception  is  jeopardized  by  the  presence  of  incorrect  dataassociation,  and  in  general,  outliers.  Although  techniques  to  handle  outliersdo  exist,  they  can  fail  in  unpredictable  manners  (e.g.,  RANSAC,  robustestimators),  or  can  have  exponential  runtime  (e.g.,  branch-and-bound).  In  thispaper,  we  advance  the  state  of  the  art  in  outlier  rejection  by  making  threecontributions.  First,  we  show  that  even  a  simple  linear  instance  of  outlierrejection  is  inapproximable:  in  the  worst-case  one  cannot  design  aquasi-polynomial  time  algorithm  that  computes  an  approximate  solutionefficiently.  Our  second  contribution  is  to  provide  the  first  per-instancesub-optimality  bounds  to  assess  the  approximation  quality  of  a  given  outlierrejection  outcome.  Our  third  contribution  is  to  propose  a  simplegeneral-purpose  algorithm,  named  adaptive  trimming,  to  remove  outliers.  Ouralgorithm  leverages  recently-proposed  global  solvers  that  are  able  to  solveoutlier-free  problems,  and  iteratively  removes  measurements  with  large  errors.We  demonstrate  the  proposed  algorithm  on  three  spatial  perception  problems:  3Dregistration,  two-view  geometry,  and  SLAM.  The  results  show  that  our  algorithmoutperforms  several  state-of-the-art  methods  across  applications  while  being  ageneral-purpose  method.</p>
http://arxiv.org/abs/1903.11688|Rallying  Adversarial  Techniques  against  Deep  Learning  for  Network  Security.  (arXiv:1903.11688v1  [cs.CR])|<p>Recent  advances  in  artificial  intelligence  and  the  increasing  need  forpowerful  defensive  measures  in  the  domain  of  network  security,  have  led  to  theadoption  of  deep  learning  approaches  for  use  in  network  intrusion  detectionsystems.  These  methods  have  achieved  superior  performance  against  conventionalnetwork  attacks,  which  enable  the  deployment  of  practical  security  systems  tounique  and  dynamic  sectors.  Adversarial  machine  learning,  unfortunately,  hasrecently  shown  that  deep  learning  models  are  inherently  vulnerable  toadversarial  modifications  on  their  input  data.  Because  of  this  susceptibility,the  deep  learning  models  deployed  to  power  a  network  defense  could  in  fact  bethe  weakest  entry  point  for  compromising  a  network  system.  In  this  paper,  weshow  that  by  modifying  on  average  as  little  as  1.38  of  the  input  features,  anadversary  can  generate  malicious  inputs  which  effectively  fool  a  deep  learningbased  NIDS.  Therefore,  when  designing  such  systems,  it  is  crucial  to  considerthe  performance  from  not  only  the  conventional  network  security  perspective  butalso  the  adversarial  machine  learning  domain.</p>
http://arxiv.org/abs/1903.11690|Optimization  of  Inf-Convolution  Regularized  Nonconvex  Composite  Problems.  (arXiv:1903.11690v1  [math.OC])|<p>In  this  work,  we  consider  nonconvex  composite  problems  that  involveinf-convolution  with  a  Legendre  function,  which  gives  rise  to  an  anisotropicgeneralization  of  the  proximal  mapping  and  Moreau-envelope.  In  a  convex  settingsuch  problems  can  be  solved  via  alternating  minimization  of  a  splittingformulation,  where  the  consensus  constraint  is  penalized  with  a  Legendrefunction.  In  contrast,  for  nonconvex  models  it  is  in  general  unclear  that  thisapproach  yields  stationary  points  to  the  infimal  convolution  problem.  To  thisend  we  analytically  investigate  local  regularity  properties  of  theMoreau-envelope  function  under  prox-regularity,  which  allows  us  to  establishthe  equivalence  between  stationary  points  of  the  splitting  model  and  theoriginal  inf-convolution  model.  We  apply  our  theory  to  characterize  stationarypoints  of  the  penalty  objective,  which  is  minimized  by  the  elastic  averagingSGD  (EASGD)  method  for  distributed  training.  Numerically,  we  demonstrate  thepractical  relevance  of  the  proposed  approach  on  the  important  task  ofdistributed  training  of  deep  neural  networks.</p>
http://arxiv.org/abs/1903.11691|Echo  State  Networks  with  Self-Normalizing  Activations  on  the  Hyper-Sphere.  (arXiv:1903.11691v1  [cs.NE])|<p>Among  the  various  architectures  of  Recurrent  Neural  Networks,  Echo  StateNetworks  (ESNs)  emerged  due  to  their  simplified  and  inexpensive  trainingprocedure.  These  networks  are  known  to  be  sensitive  to  the  setting  ofhyper-parameters,  which  critically  affect  their  behaviour.  Results  show  thattheir  performance  is  usually  maximized  in  a  narrow  region  of  hyper-parameterspace  called  edge  of  chaos.  Finding  such  a  region  requires  searching  inhyper-parameter  space  in  a  sensible  way:  hyper-parameter  configurationsmarginally  outside  such  a  region  might  yield  networks  exhibiting  fullydeveloped  chaos,  hence  producing  unreliable  computations.  The  performance  gaindue  to  optimizing  hyper-parameters  can  be  studied  by  considering  thememory--nonlinearity  trade-off,  i.e.,  the  fact  that  increasing  the  nonlinearbehavior  of  the  network  degrades  its  ability  to  remember  past  inputs,  andvice-versa.  In  this  paper,  we  propose  a  model  of  ESNs  that  eliminates  criticaldependence  on  hyper-parameters,  resulting  in  networks  that  provably  cannotenter  a  chaotic  regime  and,  at  the  same  time,  denotes  nonlinear  behaviour  inphase  space  characterised  by  a  large  memory  of  past  inputs,  comparable  to  theone  of  linear  networks.  Our  contribution  is  supported  by  experimentscorroborating  our  theoretical  findings,  showing  that  the  proposed  modeldisplays  dynamics  that  are  rich-enough  to  approximate  many  common  nonlinearsystems  used  for  benchmarking.</p>
http://arxiv.org/abs/1903.11693|Highly  cited  references  in  PLOS  ONE  and  their  in-text  usage  over  time.  (arXiv:1903.11693v1  [cs.DL])|<p>In  this  article,  we  describe  highly  cited  publications  in  a  PLOS  ONEfull-text  corpus.  For  these  publications,  we  analyse  the  citation  contextsconcerning  their  position  in  the  text  and  their  age  at  the  time  of  citing.  Byselecting  the  perspective  of  highly  cited  papers,  we  can  distinguish  them  basedon  the  context  during  citation  even  if  we  do  not  have  any  other  informationsource  or  metrics.  We  describe  the  top  cited  references  based  on  how,  when  andin  which  context  they  are  cited.  The  focus  of  this  study  is  on  a  timeperspective  to  explain  the  nature  of  the  reception  of  highly  cited  papers.  Wehave  found  that  these  references  are  distinguishable  by  the  IMRaD  sections  oftheir  citation.  And  further,  we  can  show  that  the  section  usage  of  highly  citedpapers  is  time-dependent.  The  longer  the  citation  interval,  the  higher  theprobability  that  a  reference  is  cited  in  a  method  section.</p>
http://arxiv.org/abs/1903.11694|Studying  the  Impact  of  Power  Capping  on  MapReduce-based,  Data-intensive  Mini-applications  on  Intel  KNL  and  KNM  Architectures.  (arXiv:1903.11694v1  [cs.DC])|<p>In  this  poster,  we  quantitatively  measure  the  impacts  of  data  movement  onperformance  in  MapReduce-based  applications  when  executed  on  HPC  systems.  Weleverage  the  PAPI  'powercap'  component  to  identify  ideal  conditions  forexecution  of  our  applications  in  terms  of  (1)  dataset  characteristics  (i.e.,unique  words);  (2)  HPC  system  (i.e.,  KNL  and  KNM);  and  (3)  implementation  ofthe  MapReduce  programming  model  (i.e.,  with  or  without  combiner  optimizations).Results  confirm  the  high  energy  and  runtime  costs  of  data  movement,  and  thebenefits  of  the  combiner  optimization  on  these  costs.</p>
http://arxiv.org/abs/1903.11696|Stable  prediction  with  radiomics  data.  (arXiv:1903.11696v1  [stat.ML])|<p>Motivation:  Radiomics  refers  to  the  high-throughput  mining  of  quantitativefeatures  from  radiographic  images.  It  is  a  promising  field  in  that  it  mayprovide  a  non-invasive  solution  for  screening  and  classification.  Standardmachine  learning  classification  and  feature  selection  techniques,  however,  tendto  display  inferior  performance  in  terms  of  (the  stability  of)  predictiveperformance.  This  is  due  to  the  heavy  multicollinearity  present  in  radiomicdata.  We  set  out  to  provide  an  easy-to-use  approach  that  deals  with  thisproblem.</p><p>Results:  We  developed  a  four-step  approach  that  projects  the  originalhigh-dimensional  feature  space  onto  a  lower-dimensional  latent-feature  space,while  retaining  most  of  the  covariation  in  the  data.  It  consists  of  (i)penalized  maximum  likelihood  estimation  of  a  redundancy  filtered  correlationmatrix.  The  resulting  matrix  (ii)  is  the  input  for  a  maximum  likelihood  factoranalysis  procedure.  This  two-stage  maximum-likelihood  approach  can  be  used  to(iii)  produce  a  compact  set  of  stable  features  that  (iv)  can  be  directly  usedin  any  (regression-based)  classifier  or  predictor.  It  outperforms  otherclassification  (and  feature  selection)  techniques  in  both  external  and  internalvalidation  settings  regarding  survival  in  squamous  cell  cancers.</p>
http://arxiv.org/abs/1903.11700|A  Conceptual  Framework  for  Assessing  Anonymization-Utility  Trade-Offs  Based  on  Principal  Component  Analysis.  (arXiv:1903.11700v1  [cs.CR])|<p>An  anonymization  technique  for  databases  is  proposed  that  employs  PrincipalComponent  Analysis.  The  technique  aims  at  releasing  the  least  possible  amountof  information,  while  preserving  the  utility  of  the  data  released  in  responseto  queries.  The  general  scheme  is  described,  and  alternative  metrics  areproposed  to  assess  utility,  based  respectively  on  matrix  norms;  correlationcoefficients;  divergence  measures,  and  quality  indices  of  database  images.  Thisapproach  allows  to  properly  measure  the  utility  of  output  data  and  incorporatethat  measure  in  the  anonymization  method.</p>
http://arxiv.org/abs/1903.11701|Zero-shot  Image  Recognition  Using  Relational  Matching,  Adaptation  and  Calibration.  (arXiv:1903.11701v1  [cs.CV])|<p>Zero-shot  learning  (ZSL)  for  image  classification  focuses  on  recognizingnovel  categories  that  have  no  labeled  data  available  for  training.  The  learningis  generally  carried  out  with  the  help  of  mid-level  semantic  descriptorsassociated  with  each  class.  This  semantic-descriptor  space  is  generally  sharedby  both  seen  and  unseen  categories.  However,  ZSL  suffers  from  hubness,  domaindiscrepancy  and  biased-ness  towards  seen  classes.  To  tackle  these  problems,  wepropose  a  three-step  approach  to  zero-shot  learning.  Firstly,  a  mapping  islearned  from  the  semantic-descriptor  space  to  the  image-feature  space.  Thismapping  learns  to  minimize  both  one-to-one  and  pairwise  distances  betweensemantic  embeddings  and  the  image  features  of  the  corresponding  classes.Secondly,  we  propose  test-time  domain  adaptation  to  adapt  the  semanticembedding  of  the  unseen  classes  to  the  test  data.  This  is  achieved  by  findingcorrespondences  between  the  semantic  descriptors  and  the  image  features.Thirdly,  we  propose  scaled  calibration  on  the  classification  scores  of  the  seenclasses.  This  is  necessary  because  the  ZSL  model  is  biased  towards  seen  classesas  the  unseen  classes  are  not  used  in  the  training.  Finally,  to  validate  theproposed  three-step  approach,  we  performed  experiments  on  four  benchmarkdatasets  where  the  proposed  method  outperformed  previous  results.  We  alsostudied  and  analyzed  the  performance  of  each  component  of  our  proposed  ZSLframework.</p>
http://arxiv.org/abs/1903.11702|Efficient  Nonlinear  Fourier  Transform  Algorithms  of  Order  Four  on  Equispaced  Grid.  (arXiv:1903.11702v1  [cs.NA])|<p>We  explore  two  classes  of  exponential  integrators  in  this  letter  to  designnonlinear  Fourier  transform  (NFT)  algorithms  with  a  desired  accuracy-complexitytrade-off  and  a  convergence  order  of  $4$  on  an  equispaced  grid.  The  integratingfactor  based  method  in  the  class  of  Runge-Kutta  methods  yield  algorithms  withcomplexity  $O(Nlog^2N)$  (where  $N$  is  the  number  of  samples  of  the  signal)which  have  superior  accuracy-complexity  trade-off  than  any  of  the  fast  methodsknown  currently.  The  integrators  based  on  Magnus  series  expansion,  namely,standard  and  commutator-free  Magnus  methods  yield  algorithms  of  complexity$O(N^2)$  that  have  superior  error  behavior  even  for  moderately  small  step-sizesand  higher  signal  strengths.</p>
http://arxiv.org/abs/1903.11703|Recurrent  Neural  Networks  For  Accurate  RSSI  Indoor  Localization.  (arXiv:1903.11703v1  [eess.SP])|<p>This  paper  proposes  recurrent  neuron  networks  (RNNs)  for  a  fingerprintingindoor  localization  using  WiFi.  Instead  of  locating  user's  position  one  at  atime  as  in  the  cases  of  conventional  algorithms,  our  RNN  solution  aims  attrajectory  positioning  and  takes  into  account  the  relation  among  the  receivedsignal  strength  indicator  (RSSI)  measurements  in  a  trajectory.  Furthermore,  aweighted  average  filter  is  proposed  for  both  input  RSSI  data  and  sequentialoutput  locations  to  enhance  the  accuracy  among  the  temporal  fluctuations  ofRSSI.  The  results  using  different  types  of  RNN  including  vanilla  RNN,  longshort-term  memory  (LSTM),  gated  recurrent  unit  (GRU)  and  bidirectional  LSTM(BiLSTM)  are  presented.  On-site  experiments  demonstrate  that  the  proposedstructure  achieves  an  average  localization  error  of  $0.75$  m  with  $80%$  of  theerrors  under  $1$  m,  which  outperforms  the  conventional  KNN  algorithms  andprobabilistic  algorithms  by  approximately  $30%$  under  the  same  testenvironment.</p>
http://arxiv.org/abs/1903.11712|A  Multi  Hidden  Recurrent  Neural  Network  with  a  Modified  Grey  Wolf  Optimizer.  (arXiv:1903.11712v1  [cs.NE])|<p>Identifying  university  students'  weaknesses  results  in  better  learning  andcan  function  as  an  early  warning  system  to  enable  students  to  improve.  However,the  satisfaction  level  of  existing  systems  is  not  promising.  New  and  dynamichybrid  systems  are  needed  to  imitate  this  mechanism.  A  hybrid  system  (amodified  Recurrent  Neural  Network  with  an  adapted  Grey  Wolf  Optimizer)  is  usedto  forecast  students'  outcomes.  This  proposed  system  would  improve  instructionby  the  faculty  and  enhance  the  students'  learning  experiences.  The  results  showthat  a  modified  recurrent  neural  network  with  an  adapted  Grey  Wolf  Optimizerhas  the  best  accuracy  when  compared  with  other  models.</p>
http://arxiv.org/abs/1903.11714|High  Performance  Monte  Carlo  Simulation  of  Ising  Model  on  TPU  Clusters.  (arXiv:1903.11714v1  [cs.DC])|<p>Large  scale  deep  neural  networks  profited  from  an  emerging  class  of  AIaccelerators.  Although  the  accelerators  are  specialized  for  machine  learning,some  of  their  designs  are  general  enough  for  other  computing  intensiveapplications.  Cloud  TPU,  as  one  of  them,  offers  tremendous  computing  resourcesand  is  easily  accessible  through  TensorFlow  by  expressing  the  computation  in  agraph.  In  this  paper,  we  leverage  this  powerful  hardware  combined  with  theexpressiveness  of  TensorFlow  to  simulate  the  Ising  model  on  a  $2$-dimensionallattice.  We  modify  the  computationally  intensive  part  of  the  checkerboardalgorithm  into  matrix  operations  to  exploit  Cloud  TPU's  highly  efficient  matrixunit.  In  our  experiments,  we  demonstrate  that  our  implementation  outperformsthe  best  published  benchmarks  to  our  knowledge  by  60%  in  single  core  and  250%in  multiple  cores  with  linear  scaling.  We  also  show  the  performance  improvementof  using  low  precision  arithmetic---bfloat16  instead  of  float32---withoutsacrificing  any  accuracy.</p>
http://arxiv.org/abs/1903.11719|Fairness  in  Algorithmic  Decision  Making:  An  Excursion  Through  the  Lens  of  Causality.  (arXiv:1903.11719v1  [cs.LG])|<p>As  virtually  all  aspects  of  our  lives  are  increasingly  impacted  byalgorithmic  decision  making  systems,  it  is  incumbent  upon  us  as  a  society  toensure  such  systems  do  not  become  instruments  of  unfair  discrimination  on  thebasis  of  gender,  race,  ethnicity,  religion,  etc.  We  consider  the  problem  ofdetermining  whether  the  decisions  made  by  such  systems  are  discriminatory,through  the  lens  of  causal  models.  We  introduce  two  definitions  of  groupfairness  grounded  in  causality:  fair  on  average  causal  effect  (FACE),  and  fairon  average  causal  effect  on  the  treated  (FACT).  We  use  the  Rubin-Neymanpotential  outcomes  framework  for  the  analysis  of  cause-effect  relationships  torobustly  estimate  FACE  and  FACT.  We  demonstrate  the  effectiveness  of  ourproposed  approach  on  synthetic  data.  Our  analyses  of  two  real-world  data  sets,the  Adult  income  data  set  from  the  UCI  repository  (with  gender  as  the  protectedattribute),  and  the  NYC  Stop  and  Frisk  data  set  (with  race  as  the  protectedattribute),  show  that  the  evidence  of  discrimination  obtained  by  FACE  and  FACT,or  lack  thereof,  is  often  in  agreement  with  the  findings  from  other  studies.  Wefurther  show  that  FACT,  being  somewhat  more  nuanced  compared  to  FACE,  can  yieldfindings  of  discrimination  that  differ  from  those  obtained  using  FACE.</p>
http://arxiv.org/abs/1903.11720|Performance  Analysis  and  Enhancements  for  In-Band  Full-Duplex  Wireless  Local  Area  Networks.  (arXiv:1903.11720v1  [cs.NI])|<p>In-Band  Full-Duplex  (IBFD)  is  a  technique  that  enables  a  wireless  node  tosimultaneously  transmit  a  signal  and  receive  another  on  the  same  assignedfrequency.  Thus,  IBFD  wireless  systems  can  provide  up  to  twice  the  channelcapacity  compared  to  conventional  Half-Duplex  (HD)  systems.  In  order  to  studythe  feasibility  of  IBFD  networks,  reliable  models  are  needed  to  captureanticipated  benefits  of  IBFD  above  the  physical  layer  (PHY).  In  this  paper,  anaccurate  analytical  model  based  on  Discrete-Time  Markov  Chain  (DTMC)  analysisfor  IEEE  802.11  Distributed  Coordination  Function  (DCF)  with  IBFD  capabilitiesis  proposed.  The  model  captures  all  parameters  necessary  to  calculate  importantperformance  metrics  which  quantify  enhancements  introduced  as  a  result  of  IBFDsolutions.  Additionally,  two  frame  aggregation  schemes  for  Wireless  Local  AreaNetworks  (WLANs)  with  IBFD  features  are  proposed  to  increase  the  efficiency  ofdata  transmission.  Matching  analytical  and  simulation  results  with  less  than  1%average  errors  confirm  that  the  proposed  frame  aggregation  schemes  furtherimprove  the  overall  throughput  by  up  to  24%  and  reduce  latency  by  up  to  47%  inpractical  IBFD-WLANs.  More  importantly,  the  results  assert  that  IBFDtransmission  can  only  reduce  latency  to  a  suboptimal  point  in  WLANs,  but  frameaggregation  is  necessary  to  minimize  it.</p>
http://arxiv.org/abs/1903.11722|Resource  Allocation  Mechanism  for  Media  Handling  Services  in  Cloud  Multimedia  Conferencing.  (arXiv:1903.11722v1  [cs.MM])|<p>Multimedia  conferencing  is  the  conversational  exchange  of  multimedia  contentbetween  multiple  parties.  It  has  a  wide  range  of  applications  (e.g.,  MassivelyMultiplayer  Online  Games  (MMOGs)  and  distance  learning).  Media  handlingservices  (e.g.,  video  mixing,  transcoding,  and  compressing)  are  critical  tomultimedia  conferencing.  However,  efficient  resource  usage  and  scalabilitystill  remain  important  challenges.  Unfortunately,  the  cloud-based  approachesproposed  so  far  have  several  deficiencies  in  terms  of  efficiency  in  resourceusage  and  scaling,  while  meeting  Quality  of  Service  (QoS)  requirements.  Thispaper  proposes  a  solution  which  optimizes  resource  allocation  and  scales  interms  of  the  number  of  participants  while  guaranteeing  QoS.  Moreover,  oursolution  composes  different  media  handling  services  to  support  theparticipants'  demands.  We  formulate  the  resource  allocation  problemmathematically  as  an  Integer  Linear  Programming  (ILP)  problem  and  design  aheuristic  for  it.  We  evaluate  our  proposed  solution  for  different  numbers  ofparticipants  and  different  participants'  geographical  distributions.  Simulationresults  show  that  our  resource  allocation  mechanism  can  compose  the  mediahandling  services  and  allocate  the  required  resources  in  an  optimal  mannerwhile  honoring  the  QoS  in  terms  of  end-to-end  delay.</p>
http://arxiv.org/abs/1903.11723|The  Semantic  Web  Rule  Language  Expressiveness  Extensions-A  Survey.  (arXiv:1903.11723v1  [cs.AI])|<p>The  Semantic  Web  Rule  Language  (SWRL)  is  a  direct  extension  of  OWL  2  DL  witha  subset  of  RuleML,  and  it  is  designed  to  be  the  rule  language  of  the  SemanticWeb.  This  paper  explores  the  state-of-the-art  of  SWRL's  expressivenessextensions  proposed  over  time.  As  a  motivation,  the  effectiveness  of  theSWRL/OWL  combination  in  modeling  domain  facts  is  discussed  while  some  of  thecommon  expressive  limitations  of  the  combination  are  also  highlighted.  Thepaper  then  classifies  and  presents  the  relevant  language  extensions  of  the  SWRLand  their  added  expressive  powers  to  the  original  SWRL  definition.  Furthermore,it  provides  a  comparative  analysis  of  the  syntax  and  semantics  of  the  proposedextensions.  In  conclusion,  the  decidability  requirement  and  usability  of  eachexpressiveness  extension  are  evaluated  towards  an  efficient  inclusion  into  theOWL  ontologies.</p>
http://arxiv.org/abs/1903.11725|Skill  Acquisition  via  Automated  Multi-Coordinate  Cost  Balancing.  (arXiv:1903.11725v1  [cs.RO])|<p>We  propose  a  learning  framework,  named  Multi-Coordinate  Cost  Balancing(MCCB),  to  address  the  problem  of  acquiring  point-to-point  movement  skills  fromdemonstrations.  MCCB  encodes  demonstrations  simultaneously  in  multipledifferential  coordinates  that  specify  local  geometric  properties.  MCCBgenerates  reproductions  by  solving  a  convex  optimization  problem  with  amulti-coordinate  cost  function  and  linear  constraints  on  the  reproductions,such  as  initial,  target,  and  via  points.  Further,  since  the  relative  importanceof  each  coordinate  system  in  the  cost  function  might  be  unknown  for  a  givenskill,  MCCB  learns  optimal  weighting  factors  that  balance  the  cost  function.  Wedemonstrate  the  effectiveness  of  MCCB  via  detailed  experiments  conducted  on  onehandwriting  dataset  and  three  complex  skill  datasets.</p>
http://arxiv.org/abs/1903.11726|Radiological  images  and  machine  learning:  trends,  perspectives,  and  prospects.  (arXiv:1903.11726v1  [eess.IV])|<p>The  application  of  machine  learning  to  radiological  images  is  an  increasinglyactive  research  area  that  is  expected  to  grow  in  the  next  five  to  ten  years.Recent  advances  in  machine  learning  have  the  potential  to  recognize  andclassify  complex  patterns  from  different  radiological  imaging  modalities  suchas  x-rays,  computed  tomography,  magnetic  resonance  imaging  and  positronemission  tomography  imaging.  In  many  applications,  machine  learning  basedsystems  have  shown  comparable  performance  to  human  decision-making.  Theapplications  of  machine  learning  are  the  key  ingredients  of  future  clinicaldecision  making  and  monitoring  systems.  This  review  covers  the  fundamentalconcepts  behind  various  machine  learning  techniques  and  their  applications  inseveral  radiological  imaging  areas,  such  as  medical  image  segmentation,  brainfunction  studies  and  neurological  disease  diagnosis,  as  well  as  computer-aidedsystems,  image  registration,  and  content-based  image  retrieval  systems.Synchronistically,  we  will  briefly  discuss  current  challenges  and  futuredirections  regarding  the  application  of  machine  learning  in  radiologicalimaging.  By  giving  insight  on  how  take  advantage  of  machine  learning  poweredapplications,  we  expect  that  clinicians  can  prevent  and  diagnose  diseases  moreaccurately  and  efficiently.</p>
http://arxiv.org/abs/1903.11728|Network  Slimming  by  Slimmable  Networks:  Towards  One-Shot  Architecture  Search  for  Channel  Numbers.  (arXiv:1903.11728v1  [cs.CV])|<p>We  study  how  to  set  channel  numbers  in  a  neural  network  to  achieve  betteraccuracy  under  constrained  resources  (e.g.,  FLOPs,  latency,  memory  footprint  ormodel  size).  A  simple  and  one-shot  solution,  named  AutoSlim,  is  presented.Instead  of  training  many  network  samples  and  searching  with  reinforcementlearning,  we  train  a  single  slimmable  network  to  approximate  the  networkaccuracy  of  different  channel  configurations.  We  then  iteratively  evaluate  thetrained  slimmable  model  and  greedily  slim  the  layer  with  minimal  accuracy  drop.By  this  single  pass,  we  can  obtain  the  optimized  channel  configurations  underdifferent  resource  constraints.  We  present  experiments  with  MobileNet  v1,MobileNet  v2,  ResNet-50  and  RL-searched  MNasNet  on  ImageNet  classification.  Weshow  significant  improvements  over  their  default  channel  configurations.  Wealso  achieve  better  accuracy  than  recent  channel  pruning  methods  and  neuralarchitecture  search  methods.</p><p>Notably,  by  setting  optimized  channel  numbers,  our  AutoSlim-MobileNet-v2  at305M  FLOPs  achieves  74.2%  top-1  accuracy,  2.4%  better  than  default  MobileNet-v2(301M  FLOPs),  and  even  0.2%  better  than  RL-searched  MNasNet  (317M  FLOPs).  OurAutoSlim-ResNet-50  at  570M  FLOPs,  without  depthwise  convolutions,  achieves  1.3%better  accuracy  than  MobileNet-v1  (569M  FLOPs).  Code  and  models  will  beavailable  at:  https://github.com/JiahuiYu/slimmable_networks</p>
http://arxiv.org/abs/1903.11729|Simulating  Imperial  Dynamics  and  Conflict  in  the  Ancient  World.  (arXiv:1903.11729v1  [physics.soc-ph])|"<p>The  development  of  models  to  capture  large-scale  dynamics  in  human  history  isone  of  the  core  contributions  of  the  cliodynamics  field.  Crucially  and  mostoften,  these  models  are  assessed  by  their  predictive  capability  on  somemacro-scale  and  aggregated  measure,  compared  to  manually  curated  historicaldata.  We  consider  the  model  predicting  large-scale  complex  societies  fromTurchin  et  al.  (2013),  where  the  evaluation  is  done  on  the  prediction  of""imperial  density"":  the  relative  frequency  with  which  a  geographical  areabelonged  to  large-scale  polities  over  a  certain  time  window.  We  implement  themodel  and  release  both  code  and  data  for  reproducibility.  Furthermore,  weassess  its  behaviour  against  three  historical  data  sets:  the  relative  size  ofsimulated  polities  vs  historical  ones;  the  spatial  correlation  of  simulatedimperial  density  with  historical  population  density;  the  spatial  correlation  ofsimulated  conflict  vs  historical  conflict.  At  the  global  level,  we  show  goodagreement  with  the  population  density  (R2&lt;0.75),  and  some  agreement  withhistorical  conflict  in  Europe  (R2&lt;0.42,  a  lower  result  possibly  due  tohistorical  data  bias).  Despite  being  overall  good  at  capturing  these  importanteffects,  the  model  currently  fails  to  reproduce  the  shapes  of  individualimperial  polities.  Nonetheless,  we  discuss  a  way  forward  by  matching  theprobabilistic  imperial  strength  from  simulations  to  inferred  networkedcommunities  from  real  settlement  data.</p>"
http://arxiv.org/abs/1903.11741|InfoMask:  Masked  Variational  Latent  Representation  to  Localize  Chest  Disease.  (arXiv:1903.11741v1  [cs.CV])|<p>The  scarcity  of  richly  annotated  medical  images  is  limiting  supervised  deeplearning  based  solutions  to  medical  image  analysis  tasks,  such  as  localizingdiscriminatory  radiomic  disease  signatures.  Therefore,  it  is  desirable  toleverage  unsupervised  and  weakly  supervised  models.  Most  recent  weaklysupervised  localization  methods  apply  attention  maps  or  region  proposals  in  amultiple  instance  learning  formulation.  While  attention  maps  can  be  noisy,leading  to  erroneously  highlighted  regions,  it  is  not  simple  to  decide  on  anoptimal  window/bag  size  for  multiple  instance  learning  approaches.  In  thispaper,  we  propose  a  learned  spatial  masking  mechanism  to  filter  out  irrelevantbackground  signals  from  attention  maps.  The  proposed  method  minimizes  mutualinformation  between  a  masked  variational  representation  and  the  input  whilemaximizing  the  information  between  the  masked  representation  and  class  labels.This  results  in  more  accurate  localization  of  discriminatory  regions.  We  testedthe  proposed  model  on  the  ChestX-ray8  dataset  to  localize  pneumonia  from  chestX-ray  images  without  using  any  pixel-level  or  bounding-box  annotations.</p>
http://arxiv.org/abs/1903.11748|Medical  Time  Series  Classification  with  Hierarchical  Attention-based  Temporal  Convolutional  Networks:  A  Case  Study  of  Myotonic  Dystrophy  Diagnosis.  (arXiv:1903.11748v1  [cs.LG])|<p>Myotonia,  which  refers  to  delayed  muscle  relaxation  after  contraction,  is  themain  symptom  of  myotonic  dystrophy  patients.  We  propose  a  hierarchicalattention-based  temporal  convolutional  network  (HA-TCN)  for  myotonic  dystrohpydiagnosis  from  handgrip  time  series  data,  and  introduce  mechanisms  that  enablemodel  explainability.  We  compare  the  performance  of  the  HA-TCN  model  againstthat  of  benchmark  TCN  models,  LSTM  models  with  and  without  attentionmechanisms,  and  SVM  approaches  with  handcrafted  features.  In  terms  ofclassification  accuracy  and  F1  score,  we  found  all  deep  learning  models  havesimilar  levels  of  performance,  and  they  all  outperform  SVM.  Further,  the  HA-TCNmodel  outperforms  its  TCN  counterpart  with  regards  to  computational  efficiencyregardless  of  network  depth,  and  in  terms  of  performance  particularly  when  thenumber  of  hidden  layers  is  small.  Lastly,  HA-TCN  models  can  consistentlyidentify  relevant  time  series  segments  in  the  relaxation  phase  of  the  handgriptime  series,  and  exhibit  increased  robustness  to  noise  when  compared  toattention-based  LSTM  models.</p>
http://arxiv.org/abs/1903.11749|Distributed  Algorithms  for  Fully  Personalized  PageRank  on  Large  Graphs.  (arXiv:1903.11749v1  [cs.SI])|<p>Personalized  PageRank  (PPR)  has  enormous  applications,  such  as  linkprediction  and  recommendation  systems  for  social  networks,  which  often  requirethe  fully  PPR  to  be  known.  Besides,  most  of  real-life  graphs  are  edge-weighted,e.g.,  the  interaction  between  users  on  the  Facebook  network.  However,  it  iscomputationally  difficult  to  compute  the  fully  PPR,  especially  on  large  graphs,not  to  mention  that  most  existing  approaches  do  not  consider  the  weights  ofedges.  In  particular,  the  existing  approach  cannot  handle  graphs  with  billionedges  on  a  moderate-size  cluster.  To  address  this  problem,  this  paper  presentsa  novel  study  on  the  computation  of  fully  edge-weighted  PPR  on  large  graphsusing  the  distributed  computing  framework.  Specifically,  we  employ  the  MonteCarlo  approximation  that  performs  a  large  number  of  random  walks  from  each  nodeof  the  graph,  and  exploits  the  parallel  pipeline  framework  to  reduce  theoverall  running  time  of  the  fully  PPR.  Based  on  that,  we  develop  severaloptimization  techniques  which  (i)  alleviate  the  issue  of  large  nodes  that  couldexplode  the  memory  space,  (ii)  pre-compute  short  walks  for  small  nodes  thatlargely  speedup  the  computation  of  random  walks,  and  (iii)  optimize  the  amountof  random  walks  to  compute  in  each  pipeline  that  significantly  reduces  theoverhead.  With  extensive  experiments  on  a  variety  of  real-life  graph  datasets,we  demonstrate  that  our  solution  is  several  orders  of  magnitude  faster  than  thestate-of-the-arts,  and  meanwhile,  largely  outperforms  the  baseline  algorithmsin  terms  of  accuracy.</p>
http://arxiv.org/abs/1903.11750|Navigation  in  the  Presence  of  Obstacles  for  an  Agile  Autonomous  Underwater  Vehicle.  (arXiv:1903.11750v1  [cs.RO])|"<p>Navigation  underwater  traditionally  is  done  by  keeping  a  safe  distance  fromobstacles,  resulting  in  ""fly-overs""  of  the  area  of  interest.  An  AutonomousUnderwater  Vehicle  (AUV)  moving  through  a  cluttered  space,  such  as  a  shipwreck,or  a  decorated  cave  is  an  extremely  challenging  problem  and  has  not  beenaddressed  in  the  past.  This  paper  proposed  a  novel  navigation  frameworkutilizing  an  enhanced  version  of  Trajopt  for  fast  3D  path-optimization  withnear-optimal  guarantees  for  AUVs.  A  sampling  based  correction  procedure  ensuresthat  the  planning  is  not  limited  by  local  minima,  enabling  navigation  throughnarrow  spaces.  The  method  is  shown,  both  on  simulation  and  in-pool  experiments,to  be  fast  enough  to  enable  real-time  autonomous  navigation  for  an  Aqua2  AUVwith  strong  safety  guarantees.</p>"
http://arxiv.org/abs/1903.11751|Regularized  Stochastic  Block  Model  for  robust  community  detection  in  complex  networks.  (arXiv:1903.11751v1  [cs.SI])|<p>The  stochastic  block  model  is  able  to  generate  different  network  partitions,ranging  from  traditional  assortative  communities  to  disassortative  structures.Since  the  degree-corrected  stochastic  block  model  does  not  specify  which  mixingpattern  is  desired,  the  inference  algorithms,  which  discover  the  most  likelypartition  of  the  networks  nodes,  are  likely  to  get  trapped  in  the  local  optimaof  the  log-likelihood.  Here  we  introduce  a  new  model  constraining  nodes'internal  degrees  ratios  in  the  objective  function  to  stabilize  the  inference  ofblock  models  from  the  observed  network  data.  Given  the  regularized  model,  theinference  algorithms,  such  as  Markov  chain  Monte  Carlo,  reliably  findsassortative  or  disassortive  structure  as  directed  by  the  value  of  a  singleparameter.  We  show  experimentally  that  the  inference  of  our  proposed  modelquickly  converges  to  the  desired  assortative  or  disassortative  partition  whilethe  inference  of  degree-corrected  stochastic  block  model  gets  often  trapped  atthe  inferior  local  optimal  partitions  when  the  traditional  assortativecommunity  structure  is  not  strong  in  the  observed  networks.</p>
http://arxiv.org/abs/1903.11752|ThunderNet:  Towards  Real-time  Generic  Object  Detection.  (arXiv:1903.11752v1  [cs.CV])|<p>Real-time  generic  object  detection  on  mobile  platforms  is  a  crucial  butchallenging  computer  vision  task.  However,  previous  CNN-based  detectors  sufferfrom  enormous  computational  cost,  which  hinders  them  from  real-time  inferencein  computation-constrained  scenarios.  In  this  paper,  we  investigate  theeffectiveness  of  two-stage  detectors  in  real-time  generic  detection  and  proposea  lightweight  two-stage  detector  named  ThunderNet.  In  the  backbone  part,  weanalyze  the  drawbacks  in  previous  lightweight  backbones  and  present  alightweight  backbone  designed  for  object  detection.  In  the  detection  part,  weexploit  an  extremely  efficient  RPN  and  detection  head  design.  To  generate  morediscriminative  feature  representation,  we  design  two  efficient  architectureblocks,  Context  Enhancement  Module  and  Spatial  Attention  Module.  At  last,  weinvestigate  the  balance  between  the  input  resolution,  the  backbone,  and  thedetection  head.  Compared  with  lightweight  one-stage  detectors,  ThunderNetachieves  superior  performance  with  only  40%  of  the  computational  cost  on  PASCALVOC  and  COCO  benchmarks.  Without  bells  and  whistles,  our  model  runs  at  24.1  fpson  an  ARM-based  device.  To  the  best  of  our  knowledge,  this  is  the  firstreal-time  detector  reported  on  ARM  platforms.  Code  will  be  released  for  paperreproduction.</p>
http://arxiv.org/abs/1903.11763|Finite  Time  Encryption  Schedule  in  the  Presence  of  an  Eavesdropper  with  Operation  Cost.  (arXiv:1903.11763v1  [cs.SY])|<p>In  this  paper,  we  consider  a  remote  state  estimation  problem  in  the  presenceof  an  eavesdropper.  A  smart  sensor  takes  measurement  of  a  discrete  lineartime-invariant  (LTI)  process  and  sends  its  local  state  estimate  through  awireless  network  to  a  remote  estimator.  An  eavesdropper  can  overhear  the  sensortransmissions  with  a  certain  probability.  To  enhance  the  system  privacy  level,we  propose  a  novel  encryption  strategy  to  minimize  a  linear  combination  of  theexpected  error  covariance  at  the  remote  estimator  and  the  negative  of  theexpected  error  covariance  at  the  eavesdropper,  taking  into  account  the  cost  ofthe  encryption  process.  We  prove  the  existence  of  an  optimal  deterministic  andMarkovian  policy  for  such  an  encryption  strategy  over  a  finite  time  horizon.Two  situations,  namely,  with  or  without  knowledge  of  the  eavesdropperestimation  error  covariance  are  studied  and  the  optimal  schedule  is  shown  tosatisfy  the  threshold-like  structure  in  both  cases.  Numerical  examples  aregiven  to  illustrate  the  results.</p>
http://arxiv.org/abs/1903.11764|When  an  attacker  meets  a  cipher-image  in  2018:  A  Year  in  Review.  (arXiv:1903.11764v1  [cs.CR])|<p>This  paper  aims  to  review  the  encountered  technical  contradictions  when  anattacker  meets  the  cipher-images  encrypted  by  the  image  encryption  schemes(algorithms)  proposed  in  2018  from  the  viewpoint  of  an  image  cryptanalyst.  Themost  representative  works  among  them  are  selected  and  classified  according  totheir  essential  structures.  Almost  all  image  cryptanalysis  works  published  in2018  are  surveyed  due  to  their  small  number.  The  challenging  problems  on  designand  analysis  of  image  encryption  schemes  are  summarized  to  receive  theattentions  of  both  designers  and  attackers  (cryptanalysts)  of  image  encryptionschemes,  which  may  promote  solving  scenario-oriented  image  security  problemswith  new  technologies.</p>
http://arxiv.org/abs/1903.11765|Connecting  Program  Synthesis  and  Reachability:  Automatic  Program  Repair  using  Test-Input  Generation.  (arXiv:1903.11765v1  [cs.PL])|<p>We  prove  that  certain  formulations  of  program  synthesis  and  reachability  areequivalent.  Specifically,  our  constructive  proof  shows  the  reductions  betweenthe  template-based  synthesis  problem,  which  generates  a  program  in  apre-specified  form,  and  the  reachability  problem,  which  decides  thereachability  of  a  program  location.  This  establishes  a  link  between  the  tworesearch  fields  and  allows  for  the  transfer  of  techniques  and  results  betweenthem.</p><p>To  demonstrate  the  equivalence,  we  develop  a  program  repair  prototype  usingreachability  tools.  We  transform  a  buggy  program  and  its  required  specificationinto  a  specific  program  containing  a  location  reachable  only  when  the  originalprogram  can  be  repaired,  and  then  apply  an  off-the-shelf  test-input  generationtool  on  the  transformed  program  to  find  test  values  to  reach  the  desiredlocation.  Those  test  values  correspond  to  repairs  for  the  original  programm.Preliminary  results  suggest  that  our  approach  compares  favorably  to  otherrepair  methods.</p>
http://arxiv.org/abs/1903.11768|SymInfer:  Inferring  Program  Invariants  using  Symbolic  States.  (arXiv:1903.11768v1  [cs.SE])|<p>We  introduce  a  new  technique  for  inferring  program  invariants  that  usessymbolic  states  generated  by  symbolic  execution.  Symbolic  states,  which  consistof  path  conditions  and  constraints  on  local  variables,  are  a  compactdescription  of  sets  of  concrete  program  states  and  they  can  be  used  for  bothinvariant  inference  and  invariant  verification.  Our  technique  uses  acounterexample-based  algorithm  that  creates  concrete  states  from  symbolicstates,  infers  candidate  invariants  from  concrete  states,  and  then  verifies  orrefutes  candidate  invariants  using  symbolic  states.  The  refutation  caseproduces  concrete  counterexamples  that  prevent  spurious  results  and  allow  thetechnique  to  obtain  more  precise  invariants.  This  process  stops  when  thealgorithm  reaches  a  stable  set  of  invariants.</p><p>We  present  SymInfer,  a  tool  that  implements  these  ideas  to  automaticallygenerate  invariants  at  arbitrary  locations  in  a  Java  program.  The  tool  obtainssymbolic  states  from  Symbolic  PathFinder  and  uses  existing  algorithms  to  infercomplex  (potentially  nonlinear)  numerical  invariants.  Our  preliminary  resultsshow  that  SymInfer  is  effective  in  using  symbolic  states  to  generate  preciseand  useful  invariants  for  proving  program  safety  and  analyzing  program  runtimecomplexity.  We  also  show  that  SymInfer  outperforms  existing  invariantgeneration  systems.</p>
http://arxiv.org/abs/1903.11770|An  Improved  Approach  for  Semantic  Graph  Composition  with  CCG.  (arXiv:1903.11770v1  [cs.CL])|<p>This  paper  builds  on  previous  work  using  Combinatory  Categorial  Grammar  (CCG)to  derive  a  transparent  syntax-semantics  interface  for  Abstract  MeaningRepresentation  (AMR)  parsing.  We  define  new  semantics  for  the  CCG  combinatorsthat  is  better  suited  to  deriving  AMR  graphs.  In  particular,  we  definesymmetric  alternatives  for  the  application  and  composition  combinators:  theserequire  that  the  two  constituents  being  combined  overlap  in  one  AMR  relation.We  also  provide  a  new  semantics  for  type  raising,  which  is  necessary  forcertain  constructions.  Using  these  mechanisms,  we  suggest  an  analysis  ofeventive  nouns,  which  present  a  challenge  for  deriving  AMR  graphs.  Ourtheoretical  analysis  will  facilitate  future  work  on  robust  and  transparent  AMRparsing  using  CCG.</p>
http://arxiv.org/abs/1903.11771|A  Large-Scale  Multi-Length  Headline  Corpus  for  Improving  Length-Constrained  Headline  Generation  Model  Evaluation.  (arXiv:1903.11771v1  [cs.CL])|<p>Browsing  news  articles  on  multiple  devices  is  now  possible.  The  lengths  ofnews  article  headlines  have  precise  upper  bounds,  dictated  by  the  size  of  thedisplay  of  the  relevant  device  or  interface.  Therefore,  controlling  the  lengthof  headlines  is  essential  when  applying  the  task  of  headline  generation  to  newsproduction.  However,  because  there  is  no  corpus  of  headlines  of  multiplelengths  for  a  given  article,  prior  researches  on  controlling  output  length  inheadline  generation  have  not  discussed  whether  the  evaluation  of  the  settingthat  uses  a  single  length  reference  can  evaluate  multiple  length  outputsappropriately.  In  this  paper,  we  introduce  two  corpora  (JNC  and  JAMUL)  toconfirm  the  validity  of  prior  experimental  settings  and  provide  for  the  nextstep  toward  the  goal  of  controlling  output  length  in  headline  generation.  TheJNC  provides  common  supervision  data  for  headline  generation.  The  JAMUL  is  alarge-scale  evaluation  dataset  for  headlines  of  three  different  lengthscomposed  by  professional  editors.  We  report  new  findings  on  these  corpora;  forexample,  while  the  longest  length  reference  summary  can  appropriately  evaluatethe  existing  methods  controlling  output  length,  the  methods  do  not  manage  theselection  of  words  according  to  length  constraint.</p>
http://arxiv.org/abs/1903.11774|How  to  pick  the  domain  randomization  parameters  for  sim-to-real  transfer  of  reinforcement  learning  policies?.  (arXiv:1903.11774v1  [cs.LG])|<p>Recently,  reinforcement  learning  (RL)  algorithms  have  demonstrated  remarkablesuccess  in  learning  complicated  behaviors  from  minimally  processed  input.However,  most  of  this  success  is  limited  to  simulation.  While  there  arepromising  successes  in  applying  RL  algorithms  directly  on  real  systems,  theirperformance  on  more  complex  systems  remains  bottle-necked  by  the  relative  datainefficiency  of  RL  algorithms.  Domain  randomization  is  a  promising  direction  ofresearch  that  has  demonstrated  impressive  results  using  RL  algorithms  tocontrol  real  robots.  At  a  high  level,  domain  randomization  works  by  training  apolicy  on  a  distribution  of  environmental  conditions  in  simulation.  If  theenvironments  are  diverse  enough,  then  the  policy  trained  on  this  distributionwill  plausibly  generalize  to  the  real  world.  A  human-specified  design  choice  indomain  randomization  is  the  form  and  parameters  of  the  distribution  ofsimulated  environments.  It  is  unclear  how  to  the  best  pick  the  form  andparameters  of  this  distribution  and  prior  work  uses  hand-tuned  distributions.This  extended  abstract  demonstrates  that  the  choice  of  the  distribution  plays  amajor  role  in  the  performance  of  the  trained  policies  in  the  real  world  andthat  the  parameter  of  this  distribution  can  be  optimized  to  maximize  theperformance  of  the  trained  policies  in  the  real  world</p>
http://arxiv.org/abs/1903.11775|Atrial  Fibrillation  Detection  Using  Deep  Features  and  Convolutional  Networks.  (arXiv:1903.11775v1  [cs.LG])|<p>Atrial  fibrillation  is  a  cardiac  arrhythmia  that  affects  an  estimated  33.5million  people  globally  and  is  the  potential  cause  of  1  in  3  strokes  in  peopleover  the  age  of  60.  Detection  and  diagnosis  of  atrial  fibrillation  (AFIB)  isdone  noninvasively  in  the  clinical  environment  through  the  evaluation  ofelectrocardiograms  (ECGs).  Early  research  into  automated  methods  for  thedetection  of  AFIB  in  ECG  signals  focused  on  traditional  bio-medical  signalanalysis  to  extract  important  features  for  use  in  statistical  classificationmodels.  Artificial  intelligence  models  have  more  recently  been  used  that  employconvolutional  and/or  recurrent  network  architectures.  In  this  work,  significanttime  and  frequency  domain  characteristics  of  the  ECG  signal  are  extracted  byapplying  the  short-time  Fourier  trans-form  and  then  visually  representing  theinformation  in  a  spectrogram.  Two  different  classification  approaches  wereinvestigated  that  utilized  deep  features  in  the  spectrograms  construct-ed  fromECG  segments.  The  first  approach  used  a  pretrained  DenseNet  model  to  extractfeatures  that  were  then  classified  using  Support  Vector  Machines,  and  thesecond  approach  used  the  spectrograms  as  direct  input  into  a  convolutionalnetwork.  Both  approaches  were  evaluated  against  the  MIT-BIH  AFIB  dataset,  wherethe  convolutional  network  approach  achieved  a  classification  accuracy  of93.16%.  While  these  results  do  not  surpass  established  automated  atrialfibrillation  detection  methods,  they  are  promising  and  warrant  furtherinvestigation  given  they  did  not  require  any  noise  prefiltering,  hand-craftedfeatures,  nor  a  reliance  on  beat  detection.</p>
http://arxiv.org/abs/1903.11777|What  you  get  is  what  you  see:  Decomposing  Epistemic  Planning  using  Functional  STRIPS.  (arXiv:1903.11777v1  [cs.AI])|<p>Epistemic  planning  ---  planning  with  knowledge  and  belief  ---  is  essential  inmany  multi-agent  and  human-agent  interaction  domains.  Most  state-of-the-artepistemic  planners  solve  this  problem  by  compiling  to  propositional  classicalplanning,  for  example,  generating  all  possible  knowledge  atoms,  or  compilingepistemic  formula  to  normal  forms.  However,  these  methods  becomecomputationally  infeasible  as  problems  grow.  In  this  paper,  we  decomposeepistemic  planning  by  delegating  reasoning  about  epistemic  formula  to  anexternal  solver.  We  do  this  by  modelling  the  problem  using  emph{functionalSTRIPS},  which  is  more  expressive  than  standard  STRIPS  and  supports  the  use  ofexternal,  black-box  functions  within  action  models.  Exploiting  recent  work  thatdemonstrates  the  relationship  between  what  an  agent  `sees'  and  what  it  knows,we  allow  modellers  to  provide  new  implementations  of  externals  functions.  Thesedefine  what  agents  see  in  their  environment,  allowing  new  epistemic  logics  tobe  defined  without  changing  the  planner.  As  a  result,  it  increases  thecapability  and  flexibility  of  the  epistemic  model  itself,  and  avoids  theexponential  pre-compilation  step.  We  ran  evaluations  on  well-known  epistemicplanning  benchmarks  to  compare  with  an  existing  state-of-the-art  planner,  andon  new  scenarios  based  on  different  external  functions.  The  results  show  thatour  planner  scales  significantly  better  than  the  state-of-the-art  planneragainst  which  we  compared,  and  can  express  problems  more  succinctly.</p>
http://arxiv.org/abs/1903.11779|BubbleNets:  Learning  to  Select  the  Guidance  Frame  in  Video  Object  Segmentation  by  Deep  Sorting  Frames.  (arXiv:1903.11779v1  [cs.CV])|<p>Semi-supervised  video  object  segmentation  has  made  significant  progress  onreal  and  challenging  videos  in  recent  years.  The  current  paradigm  forsegmentation  methods  and  benchmark  datasets  is  to  segment  objects  in  videoprovided  a  single  annotation  in  the  first  frame.  However,  we  find  thatsegmentation  performance  across  the  entire  video  varies  dramatically  whenselecting  an  alternative  frame  for  annotation.  This  paper  address  the  problemof  learning  to  suggest  the  single  best  frame  across  the  video  for  userannotation---this  is,  in  fact,  never  the  first  frame  of  video.  We  achieve  thisby  introducing  BubbleNets,  a  novel  deep  sorting  network  that  learns  to  selectframes  using  a  performance-based  loss  function  that  enables  the  conversion  ofexpansive  amounts  of  training  examples  from  already  existing  datasets.  UsingBubbleNets,  we  are  able  to  achieve  an  11%  relative  improvement  in  segmentationperformance  on  the  DAVIS  benchmark  without  any  changes  to  the  underlying  methodof  segmentation.</p>
http://arxiv.org/abs/1903.11780|Wasserstein  Dependency  Measure  for  Representation  Learning.  (arXiv:1903.11780v1  [cs.LG])|<p>Mutual  information  maximization  has  emerged  as  a  powerful  learning  objectivefor  unsupervised  representation  learning  obtaining  state-of-the-art  performancein  applications  such  as  object  recognition,  speech  recognition,  andreinforcement  learning.  However,  such  approaches  are  fundamentally  limitedsince  a  tight  lower  bound  of  mutual  information  requires  sample  sizeexponential  in  the  mutual  information.  This  limits  the  applicability  of  theseapproaches  for  prediction  tasks  with  high  mutual  information,  such  as  in  videounderstanding  or  reinforcement  learning.  In  these  settings,  such  techniques  areprone  to  overfit,  both  in  theory  and  in  practice,  and  capture  only  a  few  of  therelevant  factors  of  variation.  This  leads  to  incomplete  representations  thatare  not  optimal  for  downstream  tasks.  In  this  work,  we  empirically  demonstratethat  mutual  information-based  representation  learning  approaches  do  fail  tolearn  complete  representations  on  a  number  of  designed  and  real-world  tasks.  Tomitigate  these  problems  we  introduce  the  Wasserstein  dependency  measure,  whichlearns  more  complete  representations  by  using  the  Wasserstein  distance  insteadof  the  KL  divergence  in  the  mutual  information  estimator.  We  show  that  apractical  approximation  to  this  theoretically  motivated  solution,  constructedusing  Lipschitz  constraint  techniques  from  the  GAN  literature,  achievessubstantially  improved  results  on  tasks  where  incomplete  representations  are  amajor  challenge.</p>
http://arxiv.org/abs/1903.11782|Anywhere  Decoding:  Low-Overhead  Uplink  Interference  Management  for  Wireless  Networks.  (arXiv:1903.11782v1  [cs.IT])|<p>Inter-cell  interference  (ICI)  is  one  of  the  major  performance-limitingfactors  in  the  context  of  modern  cellular  systems.  To  tackle  ICI,  coordinatedmulti-point  (CoMP)  schemes  have  been  proposed  as  a  key  technology  fornext-generation  mobile  communication  systems.  Although  CoMP  schemes  offerpromising  theoretical  gains,  their  performance  could  degrade  significantlybecause  of  practical  issues  such  as  limited  backhaul.  To  address  this  issue,  weexplore  a  novel  uplink  interference  management  scheme  called  anywhere  decoding,which  requires  exchanging  just  a  few  bits  of  information  per  coding  intervalamong  the  base  stations  (BSs).  In  spite  of  the  low  overhead  of  anywheredecoding,  we  observe  considerable  gains  in  the  outage  probability  performanceof  cell-edge  users,  compared  to  no  cooperation  between  BSs.  Additionally,asymptotic  results  of  the  outage  probability  for  high-SNR  regimes  demonstratethat  anywhere  decoding  schemes  achieve  full  spatial  diversity  through  multipledecoding  opportunities,  and  they  are  within  1.5  dB  of  full  cooperation.</p>
http://arxiv.org/abs/1903.11783|A  dataset  for  resolving  referring  expressions  in  spoken  dialogue  via  contextual  query  rewrites  (CQR).  (arXiv:1903.11783v1  [cs.CL])|<p>We  present  Contextual  Query  Rewrite  (CQR)  a  dataset  for  multi-domaintask-oriented  spoken  dialogue  systems  that  is  an  extension  of  the  Stanforddialog  corpus  (Eric  et  al.,  2017a).  While  previous  approaches  have  addressedthe  issue  of  diverse  schemas  by  learning  candidate  transformations  (Naik  etal.,  2018),  we  instead  model  the  reference  resolution  task  as  a  user  queryreformulation  task,  where  the  dialog  state  is  serialized  into  a  naturallanguage  query  that  can  be  executed  by  the  downstream  spoken  languageunderstanding  system.  In  this  paper,  we  describe  our  methodology  for  creatingthe  query  reformulation  extension  to  the  dialog  corpus,  and  present  an  initialset  of  experiments  to  establish  a  baseline  for  the  CQR  task.  We  have  releasedthe  corpus  to  the  public  [1]  to  support  further  research  in  this  area.</p>
http://arxiv.org/abs/1903.11785|A  Fast  Free-viewpoint  Video  Synthesis  Algorithm  for  Sports  Scenes.  (arXiv:1903.11785v1  [cs.CV])|<p>In  this  paper,  we  report  on  a  parallel  freeviewpoint  video  synthesisalgorithm  that  can  efficiently  reconstruct  a  high-quality  3D  scenerepresentation  of  sports  scenes.  The  proposed  method  focuses  on  a  scene  that  iscaptured  by  multiple  synchronized  cameras  featuring  wide-baselines.  Thefollowing  strategies  are  introduced  to  accelerate  the  production  of  afree-viewpoint  video  taking  the  improvement  of  visual  quality  into  account:  (1)a  sparse  point  cloud  is  reconstructed  using  a  volumetric  visual  hull  approach,and  an  exact  3D  ROI  is  found  for  each  object  using  an  efficient  connectedcomponents  labeling  algorithm.  Next,  the  reconstruction  of  a  dense  point  cloudis  accelerated  by  implementing  visual  hull  only  in  the  ROIs;  (2)  an  accuratepolyhedral  surface  mesh  is  built  by  estimating  the  exact  intersections  betweengrid  cells  and  the  visual  hull;  (3)  the  appearance  of  the  reconstructedpresentation  is  reproduced  in  a  view-dependent  manner  that  respectively  rendersthe  non-occluded  and  occluded  region  with  the  nearest  camera  and  itsneighboring  cameras.  The  production  for  volleyball  and  judo  sequencesdemonstrates  the  effectiveness  of  our  method  in  terms  of  both  execution  timeand  visual  quality.</p>
http://arxiv.org/abs/1903.11787|Successive-Cancellation  Decoding  of  Linear  Source  Code.  (arXiv:1903.11787v1  [cs.IT])|<p>This  paper  investigates  the  error  probability  of  several  decoding  methods  fora  source  code  with  decoder  side  information,  where  the  decoding  methods  are:  1)symbol-wise  maximum  a  posteriori  decoding,  2)  successive-cancellation  decoding,and  3)  stochastic  successive-cancellation  decoding.  The  proof  of  theeffectiveness  of  a  decoding  method  is  reduced  to  that  for  an  arbitrary  decodingmethod,  where  `effective'  means  that  the  error  probability  goes  to  zero  as  $n$goes  to  infinity.  Furthermore,  we  revisit  the  polar  source  code  showing  thatstochastic  successive-cancellation  decoding,  as  well  as  successive-cancellationdecoding,  is  effective  for  this  code.</p>
http://arxiv.org/abs/1903.11788|Cherenkov  Detectors  Fast  Simulation  Using  Neural  Networks.  (arXiv:1903.11788v1  [hep-ex])|<p>We  propose  a  way  to  simulate  Cherenkov  detector  response  using  a  generativeadversarial  neural  network  to  bypass  low-level  details.  This  network  is  trainedto  reproduce  high  level  features  of  the  simulated  detector  events  based  oninput  observables  of  incident  particles.  This  allows  the  dramatic  increase  ofsimulation  speed.  We  demonstrate  that  this  approach  provides  simulationprecision  which  is  consistent  with  the  baseline  and  discuss  possibleimplications  of  these  results.</p>
http://arxiv.org/abs/1903.11789|Step  Change  Improvement  in  ADMET  Prediction  with  PotentialNet  Deep  Featurization.  (arXiv:1903.11789v1  [cs.LG])|<p>The  Absorption,  Distribution,  Metabolism,  Elimination,  and  Toxicity  (ADMET)properties  of  drug  candidates  are  estimated  to  account  for  up  to  50%  of  allclinical  trial  failures.  Predicting  ADMET  properties  has  therefore  been  ofgreat  interest  to  the  cheminformatics  and  medicinal  chemistry  communities  inrecent  decades.  Traditional  cheminformatics  approaches,  whether  the  learner  isa  random  forest  or  a  deep  neural  network,  leverage  fixed  fingerprint  featurerepresentations  of  molecules.  In  contrast,  in  this  paper,  we  learn  the  featuresmost  relevant  to  each  chemical  task  at  hand  by  representing  each  moleculeexplicitly  as  a  graph,  where  each  node  is  an  atom  and  each  edge  is  a  bond.  Byapplying  graph  convolutions  to  this  explicit  molecular  representation,  weachieve,  to  our  knowledge,  unprecedented  accuracy  in  prediction  of  ADMETproperties.  By  challenging  our  methodology  with  rigorous  cross-validationprocedures  and  prospective  analyses,  we  show  that  deep  featurization  betterenables  molecular  predictors  to  not  only  interpolate  but  also  extrapolate  tonew  regions  of  chemical  space.</p>
http://arxiv.org/abs/1903.11791|Hierarchical  Pooling  Structure  for  Weakly  Labeled  Sound  Event  Detection.  (arXiv:1903.11791v1  [cs.SD])|<p>Sound  event  detection  with  weakly  labeled  data  is  considered  as  a  problem  ofmulti-instance  learning.  And  the  choice  of  pooling  function  is  the  key  tosolving  this  problem.  In  this  paper,  we  proposed  a  hierarchical  poolingstructure  to  improve  the  performance  of  weakly  labeled  sound  event  detectionsystem.  Proposed  pooling  structure  has  made  remarkable  improvements  on  threetypes  of  pooling  function  without  adding  any  parameters.  Moreover,  our  systemhas  achieved  competitive  performance  on  Task  4  of  Detection  and  Classificationof  Acoustic  Scenes  and  Events  (DCASE)  2017  Challenge  using  hierarchical  poolingstructure.</p>
http://arxiv.org/abs/1903.11800|Pyramid  Mask  Text  Detector.  (arXiv:1903.11800v1  [cs.CV])|<p>Scene  text  detection,  an  essential  step  of  scene  text  recognition  system,  isto  locate  text  instances  in  natural  scene  images  automatically.  Some  recentattempts  benefiting  from  Mask  R-CNN  formulate  scene  text  detection  task  as  aninstance  segmentation  problem  and  achieve  remarkable  performance.  In  thispaper,  we  present  a  new  Mask  R-CNN  based  framework  named  Pyramid  Mask  TextDetector  (PMTD)  to  handle  the  scene  text  detection.  Instead  of  binary  text  maskgenerated  by  the  existing  Mask  R-CNN  based  methods,  our  PMTD  performspixel-level  regression  under  the  guidance  of  location-aware  supervision,yielding  a  more  informative  soft  text  mask  for  each  text  instance.  As  for  thegeneration  of  text  boxes,  PMTD  reinterprets  the  obtained  2D  soft  mask  into  3Dspace  and  introduces  a  novel  plane  clustering  algorithm  to  derive  the  optimaltext  box  on  the  basis  of  3D  shape.  Experiments  on  standard  datasets  demonstratethat  the  proposed  PMTD  brings  consistent  and  noticeable  gain  and  clearlyoutperforms  state-of-the-art  methods.  Specifically,  it  achieves  an  F-measure  of80.13%  on  ICDAR  2017  MLT  dataset.</p>
http://arxiv.org/abs/1903.11807|On  the  Spectral  Efficiency  for  Massive  MIMO  Systems  With  Imperfect  Spacial  Covariance  Information.  (arXiv:1903.11807v1  [cs.IT])|<p>This  paper  studies  the  impact  of  imperfect  channel  covariance  information  onthe  uplink  and  downlink  spectral  efficiencies  of  a  time  division  duplexed  (TDD)massive  multiple-input  multiple-output  (MIMO)  system.  Specifically,  we  deriveanalytical  lower  bounds  on  the  uplink  and  downlink  spectral  efficiencies  of  auser  with  imperfect  knowledge  on  channel  vectors,  as  well  as  that  of  thechannel  covariance  matrices,  of  the  users.  We  consider  a  linear  minimum  meansquared  estimator  (LMMSE)  and  element-wise  LMMSE  channel  estimation  for  theunknown  channel  in  this  paper.  These  analytical  bounds  enable  us  to  choose  thesample  size  for  covariance  matrix  estimation  to  meet  spectral  efficiencyrequirements.  The  accurate  agreement  between  derived  bounds  and  simulatedbounds  based  on  random  samples  of  channel  vector  and  covariance  matrices  isshown.</p>
http://arxiv.org/abs/1903.11808|Exploiting  the  Shipping  Lane  Information  for  Energy-Efficient  Maritime  Communications.  (arXiv:1903.11808v1  [cs.IT])|<p>Energy  efficiency  is  a  crucial  issue  for  maritime  communications,  due  to  thelimitation  of  geographically  available  base  station  sites.  Different  fromprevious  studies,  we  promote  the  energy  efficiency  by  exploiting  the  specificcharacteristics  of  maritime  channels  and  user  mobility.  Particularly,  weutilize  the  shipping  lane  information  to  obtain  the  long-term  positioninformation  of  marine  users,  from  which  the  large-scale  channel  stateinformation  is  estimated.  Based  on  that,  the  resource  allocation  is  jointlyoptimized  for  all  users  and  all  time  slots  during  the  voyage,  leading  to  amixed  0-1  non-convex  programming  problem.  We  transform  this  problem  into  aconvex  one  by  means  of  variable  substitution  and  time-sharing  relaxation,  andpropose  an  iterative  algorithm  to  solve  it  based  on  the  Lagrangian  dualdecomposition  method.  Simulation  results  demonstrate  that  the  proposed  schemecan  significantly  reduce  the  power  consumption  compared  with  existingapproaches,  due  to  the  global  optimization  over  a  much  larger  time  span  byutilizing  the  shipping  lane  information.</p>
http://arxiv.org/abs/1903.11812|Analysis  of  distracted  pedestrians'  waiting  time:  Head-Mounted  Immersive  Virtual  Reality  application.  (arXiv:1903.11812v1  [cs.HC])|<p>This  paper  analyzes  the  distracted  pedestrians'  waiting  time  before  crossingthe  road  in  three  conditions:  1)  not  distracted,  2)  distracted  with  asmartphone  and  3)  distracted  with  a  smartphone  in  the  presence  of  virtualflashing  LED  lights  on  the  crosswalk  as  a  safety  measure.  For  the  means  of  datacollection,  we  adapted  an  in-house  developed  virtual  immersive  realityenvironment  (VIRE).  A  total  of  42  volunteers  participated  in  the  experiment.Participants'  positions  and  head  movements  were  recorded  and  used  to  calculatewalking  speeds,  acceleration  and  deceleration  rates,  surrogate  safety  measures,time  spent  playing  smartphone  game,  etc.  After  a  descriptive  analysis  on  thedata,  the  effects  of  these  variables  on  pedestrians'  waiting  time  are  analyzedby  employing  a  cox  proportional  hazard  model.  Several  factors  were  identifiedas  having  impact  on  waiting  time.  The  results  show  that  an  increase  in  initialwalk  speed,  percentage  of  time  the  head  was  oriented  toward  smartphone  duringcrossing,  bigger  minimum  missed  gaps  and  unsafe  crossings  resulted  in  shorterwaiting  times.  On  the  other  hand,  an  increase  in  the  percentage  of  time  thehead  was  oriented  toward  smartphone  during  waiting  time,  crossing  time  and  mazesolving  time,  means  longer  waiting  times  for  participants.</p>
http://arxiv.org/abs/1903.11814|Hybrid  Satellite-Terrestrial  Communication  Networks  for  the  Maritime  Internet  of  Things:  Key  Technologies,  Opportunities,  and  Challenges.  (arXiv:1903.11814v1  [cs.NI])|<p>With  the  rapid  development  of  marine  activities,  there  has  been  an  increasingnumber  of  maritime  mobile  terminals,  as  well  as  a  growing  demand  for  high-speedand  ultra-reliable  maritime  communications  to  keep  them  connected.Traditionally,  the  maritime  Internet  of  Things  (IoT)  is  enabled  by  maritimesatellites.  However,  satellites  are  seriously  restricted  by  their  high  latencyand  relatively  low  data  rate.  As  an  alternative,  shore  &amp;  island-based  basestations  (BSs)  can  be  built  to  extend  the  coverage  of  terrestrial  networksusing  fourth-generation  (4G),  fifth-generation  (5G),  and  beyond  5G  services.Unmanned  aerial  vehicles  can  also  be  exploited  to  serve  as  aerial  maritime  BSs.Despite  of  all  these  approaches,  there  are  still  open  issues  for  an  efficientmaritime  communication  network  (MCN).  For  example,  due  to  the  complicatedelectromagnetic  propagation  environment,  the  limited  geometrically  available  BSsites,  and  rigorous  service  demands  from  mission-critical  applications,conventional  communication  and  networking  theories  and  methods  should  betailored  for  maritime  scenarios.  Towards  this  end,  we  provide  a  survey  on  thedemand  for  maritime  communications,  the  state-of-the-art  MCNs,  and  keytechnologies  for  enhancing  transmission  efficiency,  extending  network  coverage,and  provisioning  maritime-specific  services.  Future  challenges  in  developing  anenvironment-aware,  service-driven,  and  integrated  satellite-air-ground  MCN  tobe  smart  enough  to  utilize  external  auxiliary  information,  e.g.,  sea  state  andatmosphere  conditions,  are  also  discussed.</p>
http://arxiv.org/abs/1903.11816|FastFCN:  Rethinking  Dilated  Convolution  in  the  Backbone  for  Semantic  Segmentation.  (arXiv:1903.11816v1  [cs.CV])|<p>Modern  approaches  for  semantic  segmentation  usually  employ  dilatedconvolutions  in  the  backbone  to  extract  high-resolution  feature  maps,  whichbrings  heavy  computation  complexity  and  memory  footprint.  To  replace  the  timeand  memory  consuming  dilated  convolutions,  we  propose  a  novel  joint  upsamplingmodule  named  Joint  Pyramid  Upsampling  (JPU)  by  formulating  the  task  ofextracting  high-resolution  feature  maps  into  a  joint  upsampling  problem.  Withthe  proposed  JPU,  our  method  reduces  the  computation  complexity  by  more  thanthree  times  without  performance  loss.  Experiments  show  that  JPU  is  superior  toother  upsampling  modules,  which  can  be  plugged  into  many  existing  approaches  toreduce  computation  complexity  and  improve  performance.  By  replacing  dilatedconvolutions  with  the  proposed  JPU  module,  our  method  achieves  thestate-of-the-art  performance  in  Pascal  Context  dataset  (mIoU  of  53.13%)  andADE20K  dataset  (final  score  of  0.5584)  while  running  3  times  faster.</p>
http://arxiv.org/abs/1903.11821|SRDGAN:  learning  the  noise  prior  for  Super  Resolution  with  Dual  Generative  Adversarial  Networks.  (arXiv:1903.11821v1  [cs.MM])|<p>Single  Image  Super  Resolution  (SISR)  is  the  task  of  producing  a  highresolution  (HR)  image  from  a  given  low-resolution  (LR)  image.  It  is  a  wellresearched  problem  with  extensive  commercial  applications  such  as  digitalcamera,  video  compression,  medical  imaging  and  so  on.  Most  super  resolutionworks  focus  on  the  features  learning  architecture,  which  can  recover  thetexture  details  as  close  as  possible.  However,  these  works  suffer  from  thefollowing  challenges:  (1)  The  low-resolution  (LR)  training  images  areartificially  synthesized  using  HR  images  with  bicubic  downsampling,  which  havemuch  richer-information  than  real  demosaic-upscaled  mobile  images.  The  mismatchbetween  training  and  inference  mobile  data  heavily  blocks  the  improvement  ofpractical  super  resolution  algorithms.  (2)  These  methods  cannot  effectivelyhandle  the  blind  distortions  during  super  resolution  in  practical  applications.In  this  work,  an  end-to-end  novel  framework,  including  high-to-low  network  andlow-to-high  network,  is  proposed  to  solve  the  above  problems  with  dualGenerative  Adversarial  Networks  (GAN).  First,  the  above  mismatch  problems  arewell  explored  with  the  high-to-low  network,  where  clear  high-resolution  imageand  the  corresponding  realistic  low-resolution  image  pairs  can  be  generated.Moreover,  a  large-scale  General  Mobile  Super  Resolution  Dataset,  GMSR,  isproposed,  which  can  be  utilized  for  training  or  as  a  fair  comparison  benchmarkfor  super  resolution  methods.  Second,  an  effective  low-to-high  network  (superresolution  network)  is  proposed  in  the  framework.  Benefiting  from  the  GMSRdataset  and  novel  training  strategies,  the  super  resolution  model  caneffectively  handle  detail  recovery  and  denoising  at  the  same  time.</p>
http://arxiv.org/abs/1903.11833|Skip  prediction  using  boosting  trees  based  on  acoustic  features  of  tracks  in  sessions.  (arXiv:1903.11833v1  [cs.IR])|"<p>The  Spotify  Sequential  Skip  Prediction  Challenge  focuses  on  predicting  if  atrack  in  a  session  will  be  skipped  by  the  user  or  not.  In  this  paper,  wedescribe  our  approach  to  this  problem  and  the  final  system  that  was  submittedto  the  challenge  by  our  team  from  the  Music  Technology  Group  (MTG)  under  thename  ""aferraro"".  This  system  consists  in  combining  the  predictions  of  multipleboosting  trees  models  trained  with  features  extracted  from  the  sessions  and  thetracks.  The  proposed  approach  achieves  good  overall  performance  (MAA  of  0.554),with  our  model  ranked  14th  out  of  more  than  600  submissions  in  the  finalleaderboard.</p>"
http://arxiv.org/abs/1903.11834|Feature  Fusion  Encoder  Decoder  Network  For  Automatic  Liver  Lesion  Segmentation.  (arXiv:1903.11834v1  [cs.CV])|<p>Liver  lesion  segmentation  is  a  difficult  yet  critical  task  for  medical  imageanalysis.  Recently,  deep  learning  based  image  segmentation  methods  haveachieved  promising  performance,  which  can  be  divided  into  three  categories:  2D,2.5D  and  3D,  based  on  the  dimensionality  of  the  models.  However,  2.5D  and  3Dmethods  can  have  very  high  complexity  and  2D  methods  may  not  performsatisfactorily.  To  obtain  competitive  performance  with  low  complexity,  in  thispaper,  we  propose  a  Feature-fusion  Encoder-Decoder  Network  (FED-Net)  based  2Dsegmentation  model  to  tackle  the  challenging  problem  of  liver  lesionsegmentation  from  CT  images.  Our  feature  fusion  method  is  based  on  theattention  mechanism,  which  fuses  high-level  features  carrying  semanticinformation  with  low-level  features  having  image  details.  Additionally,  tocompensate  for  the  information  loss  during  the  upsampling  process,  a  denseupsampling  convolution  and  a  residual  convolutional  structure  are  proposed.  Wetested  our  method  on  the  dataset  of  MICCAI  2017  Liver  Tumor  Segmentation  (LiTS)Challenge  and  achieved  competitive  results  compared  with  other  state-of-the-artmethods.</p>
http://arxiv.org/abs/1903.11835|A  Survey  on  Graph  Kernels.  (arXiv:1903.11835v1  [cs.LG])|<p>Graph  kernels  have  become  an  established  and  widely-used  technique  forsolving  classification  tasks  on  graphs.  This  survey  gives  a  comprehensiveoverview  of  techniques  for  kernel-based  graph  classification  developed  in  thepast  15  years.  We  describe  and  categorize  graph  kernels  based  on  propertiesinherent  to  their  design,  such  as  the  nature  of  their  extracted  graph  features,their  method  of  computation  and  their  applicability  to  problems  in  practice.  Inan  extensive  experimental  evaluation,  we  study  the  classification  accuracy  of  alarge  suite  of  graph  kernels  on  established  benchmarks  as  well  as  new  datasets.We  compare  the  performance  of  popular  kernels  with  several  baseline  methods  andstudy  the  effect  of  applying  a  Gaussian  RBF  kernel  to  the  metric  induced  by  agraph  kernel.  In  doing  so,  we  find  that  simple  baselines  become  competitiveafter  this  transformation  on  some  datasets.  Moreover,  we  study  the  extent  towhich  existing  graph  kernels  agree  in  their  predictions  (and  prediction  errors)and  obtain  a  data-driven  categorization  of  kernels  as  result.  Finally,  based  onour  experimental  results,  we  derive  a  practitioner's  guide  to  kernel-basedgraph  classification.</p>
http://arxiv.org/abs/1903.11844|DDoS  Attack  Detection  Method  Based  on  Network  Abnormal  Behavior  in  Big  Data  Environment.  (arXiv:1903.11844v1  [cs.CR])|<p>Distributed  denial  of  service  (DDoS)  attack  becomes  a  rapidly  growing  problemwith  the  fast  development  of  the  Internet.  The  existing  DDoS  attack  detectionmethods  have  time-delay  and  low  detection  rate.  This  paper  presents  a  DDoSattack  detection  method  based  on  network  abnormal  behavior  in  a  big  dataenvironment.  Based  on  the  characteristics  of  flood  attack,  the  method  filtersthe  network  flows  to  leave  only  the  'many-to-one'  network  flows  to  reduce  theinterference  from  normal  network  flows  and  improve  the  detection  accuracy.  Wedefine  the  network  abnormal  feature  value  (NAFV)  to  reflect  the  state  changesof  the  old  and  new  IP  address  of  'many-to-one'  network  flows.  Finally,  the  DDoSattack  detection  method  based  on  NAFV  real-time  series  is  built  to  identify  theabnormal  network  flow  states  caused  by  DDoS  attacks.  The  experiments  show  thatcompared  with  similar  methods,  this  method  has  higher  detection  rate,  lowerfalse  alarm  rate  and  missing  rate.</p>
http://arxiv.org/abs/1903.11848|Sogou  Machine  Reading  Comprehension  Toolkit.  (arXiv:1903.11848v1  [cs.CL])|<p>Machine  reading  comprehension  have  been  intensively  studied  in  recent  years,and  neural  network-based  models  have  shown  dominant  performances.  In  thispaper,  we  present  a  Sogou  Machine  Reading  Comprehension  (SMRC)  toolkit  that  canbe  used  to  provide  the  fast  and  efficient  development  of  modern  machinecomprehension  models,  including  both  published  models  and  original  prototypes.To  achieve  this  goal,  the  toolkit  provides  dataset  readers,  a  flexiblepreprocessing  pipeline,  necessary  neural  network  components,  and  built-inmodels,  which  make  the  whole  process  of  data  preparation,  model  construction,and  training  easier.</p>
http://arxiv.org/abs/1903.11849|Inertial  Sensor  Aided  mmWave  Beam  Tracking  to  Support  Cooperative  Autonomous  Driving.  (arXiv:1903.11849v1  [eess.SP])|<p>This  paper  presents  an  inertial  sensor  aided  technique  for  beam  alignment  andtracking  in  massive  multiple-input  multiple-output  (MIMO)  vehicle-to-vehicle(V2V)  communications  based  on  millimeter  waves  (mmWave).  Since  directionalcommunications  in  vehicular  scenarios  are  severely  hindered  by  beam  pointingissues,  a  beam  alignment  procedure  has  to  be  periodically  carried  out  toguarantee  the  communication  reliability.  When  dealing  with  massive  MIMO  links,the  beam  sweeping  approach  is  known  to  be  time  consuming  and  often  unfeasibledue  to  latency  constraints.  To  speed  up  the  process,  we  propose  a  method  thatexploits  a-priori  information  on  array  dynamics  provided  by  an  inertial  sensoron  transceivers  to  assist  the  beam  alignment  procedure.  The  proposed  inertialsensor  aided  technique  allows  a  continuous  tracking  of  the  beam  whiletransmitting,  avoiding  frequent  realignment  phases.  Numerical  results  based  onreal  measurements  of  on-transceiver  accelerometers  demonstrate  a  significantgain  in  terms  of  V2V  communication  throughput  with  respect  to  conventional  beamalignment  protocols.</p>
http://arxiv.org/abs/1903.11850|Mining  Discourse  Markers  for  Unsupervised  Sentence  Representation  Learning.  (arXiv:1903.11850v1  [cs.CL])|<p>Current  state  of  the  art  systems  in  NLP  heavily  rely  on  manually  annotateddatasets,  which  are  expensive  to  construct.  Very  little  work  adequatelyexploits  unannotated  data  --  such  as  discourse  markers  between  sentences  --mainly  because  of  data  sparseness  and  ineffective  extraction  methods.  In  thepresent  work,  we  propose  a  method  to  automatically  discover  sentence  pairs  withrelevant  discourse  markers,  and  apply  it  to  massive  amounts  of  data.  Ourresulting  dataset  contains  174  discourse  markers  with  at  least  10k  exampleseach,  even  for  rare  markers  such  as  coincidentally  or  amazingly  We  use  theresulting  data  as  supervision  for  learning  transferable  sentence  embeddings.  Inaddition,  we  show  that  even  though  sentence  representation  learning  throughprediction  of  discourse  markers  yields  state  of  the  art  results  acrossdifferent  transfer  tasks,  it  is  not  clear  that  our  models  made  use  of  thesemantic  relation  between  sentences,  thus  leaving  room  for  furtherimprovements.  Our  datasets  are  publicly  available(https://github.com/synapse-developpement/Discovery)</p>
http://arxiv.org/abs/1903.11851|Feature  Intertwiner  for  Object  Detection.  (arXiv:1903.11851v1  [cs.CV])|<p>A  well-trained  model  should  classify  objects  with  a  unanimous  score  for  everycategory.  This  requires  the  high-level  semantic  features  should  be  as  muchalike  as  possible  among  samples.  To  achive  this,  previous  works  focus  onre-designing  the  loss  or  proposing  new  regularization  constraints.  In  thispaper,  we  provide  a  new  perspective.  For  each  category,  it  is  assumed  thatthere  are  two  feature  sets:  one  with  reliable  information  and  the  other  withless  reliable  source.  We  argue  that  the  reliable  set  could  guide  the  featurelearning  of  the  less  reliable  set  during  training  -  in  spirit  of  studentmimicking  teacher  behavior  and  thus  pushing  towards  a  more  compact  classcentroid  in  the  feature  space.  Such  a  scheme  also  benefits  the  reliable  setsince  samples  become  closer  within  the  same  category  -  implying  that  it  iseasier  for  the  classifier  to  identify.  We  refer  to  this  mutual  learning  processas  feature  intertwiner  and  embed  it  into  object  detection.  It  is  well-knownthat  objects  of  low  resolution  are  more  difficult  to  detect  due  to  the  loss  ofdetailed  information  during  network  forward  pass  (e.g.,  RoI  operation).  We  thusregard  objects  of  high  resolution  as  the  reliable  set  and  objects  of  lowresolution  as  the  less  reliable  set.  Specifically,  an  intertwiner  is  designedto  minimize  the  distribution  divergence  between  two  sets.  The  choice  ofgenerating  an  effective  feature  representation  for  the  reliable  set  is  furtherinvestigated,  where  we  introduce  the  optimal  transport  (OT)  theory  into  theframework.  Samples  in  the  less  reliable  set  are  better  aligned  with  aid  of  OTmetric.  Incorporated  with  such  a  plug-and-play  intertwiner,  we  achieve  anevident  improvement  over  previous  state-of-the-arts.</p>
http://arxiv.org/abs/1903.11854|Knowledge  Management  in  Medium-Sized  Software  Consulting  Companies:  An  investigation  of  Intranet-based  Knowledge  Management  Tools  for  Knowledge  Cartography  and  Knowledge  Repositories  for  Learning  Software  Organisations.  (arXiv:1903.11854v1  [cs.SE])|<p>Companies  that  develop  software  have  a  pressure  from  customers  to  deliverbetter  solutions,  and  to  deliver  solutions  faster  and  cheaper.  Many  researchershave  worked  with  suggestions  on  how  to  improve  the  development  process;software  process  improvement.  As  software  development  is  a  very  knowledgeintensive  task,  both  researchers  and  industry  have  recently  turned  theirattention  to  knowledge  management  as  a  means  to  improve  software  development.This  often  involves  developing  technical  tools,  which  many  companies  have  spentresources  on.  But  the  tools  are  often  not  used  in  practise  by  developers  andmanagers  in  the  companies,  and  it  is  often  unknown  if  the  tools  improve  howknowledge  is  managed.  In  order  to  build  efficient  knowledge  management  tools,we  need  a  better  understanding  of  how  the  tools  that  exist  are  applied  and  usedin  software  development.  We  present  and  analyse  eight  case  studies  of  knowledgemanagement  initiatives  from  the  literature.  We  found  evidence  of  improvedsoftware  quality,  reduced  development  costs  and  evidence  of  a  better  workingenvironment  for  developers  as  a  result  of  these  initiatives.  Further,  weexamine  success  criteria  in  knowledge  management  codification  initiatives,based  on  Intranet  tools  in  medium-sized  software  companies.  In  addition,  weinvestigate  how  knowledge  management  tools  are  used  for  different  purposes  bydifferent  groups  of  users  in  two  software  consulting  companies.  They  use  toolsboth  as  support  for  personalization  and  codification  strategies.  The  consultingcompanies  are  two  medium-sized  Norwegian  companies  with  40  and  150  employees,which  work  in  development  projects  that  lasts  from  a  few  weeks  to  severalyears.</p>
http://arxiv.org/abs/1903.11857|Analysis  and  Extension  of  the  Evidential  Reasoning  Algorithm  for  Multiple  Attribute  Decision  Analysis  with  Uncertainty.  (arXiv:1903.11857v1  [cs.AI])|<p>In  multiple  attribute  decision  analysis  (MADA)  problems,  one  often  needs  todeal  with  assessment  information  with  uncertainty.  The  evidential  reasoningapproach  is  one  of  the  most  effective  methods  to  deal  with  such  MADA  problems.As  kernel  of  the  evidential  reasoning  approach,  an  original  evidentialreasoning  (ER)  algorithm  was  firstly  proposed  by  Yang  et  al,  and  later  theymodified  the  ER  algorithm  in  order  to  satisfy  the  proposed  four  synthesisaxioms  with  which  a  rational  aggregation  process  needs  to  satisfy.  However,  upto  present,  the  essential  difference  of  the  two  ER  algorithms  as  well  as  therationality  of  the  synthesis  axioms  are  still  unclear.  In  this  paper,  weanalyze  the  ER  algorithms  in  Dempster-Shafer  theory  (DST)  framework  and  provethat  the  original  ER  algorithm  follows  the  reliability  discounting  andcombination  scheme,  while  the  modified  one  follows  the  importance  discountingand  combination  scheme.  Further  we  reveal  that  the  four  synthesis  axioms  arenot  valid  criteria  to  check  the  rationality  of  one  attribute  aggregationalgorithm.  Based  on  these  new  findings,  an  extended  ER  algorithm  is  proposed  totake  into  account  both  the  reliability  and  importance  of  different  attributes,which  provides  a  more  general  attribute  aggregation  scheme  for  MADA  withuncertainty.  A  motorcycle  performance  assessment  problem  is  examined  toillustrate  the  proposed  algorithm.</p>
http://arxiv.org/abs/1903.11860|Complete  Disjoint  coNP-Pairs  but  no  Complete  Total  Polynomial  Search  Problems  Relative  to  an  Oracle.  (arXiv:1903.11860v1  [cs.CC])|<p>Consider  the  following  conjectures:</p><p>H1:  the  set  TFNP  of  all  total  polynomial  search  problems  has  no  completeproblems  with  respect  to  polynomial  reductions.</p><p>H2:  there  exists  no  many-one  complete  disjoint  coNP-pair.</p><p>We  construct  an  oracle  relative  to  which  H1  holds  and  H2  does  not  hold.  Thispartially  answers  a  question  by  Pudl'ak  [Pud17],  who  lists  several  hypothesesand  asks  for  oracles  that  show  corresponding  relativized  hypotheses  to  bedifferent.  As  there  exists  a  relativizable  proof  for  the  implication  H1  -&gt;  H2[Pud17],  the  relativizations  of  the  hypotheses  H1  and  H2  are  neitherindependent  nor  equivalent.</p>
http://arxiv.org/abs/1903.11862|Smooth  Adversarial  Examples.  (arXiv:1903.11862v1  [cs.CV])|<p>This  paper  investigates  the  visual  quality  of  the  adversarial  examples.Recent  papers  propose  to  smooth  the  perturbations  to  get  rid  of  high  frequencyartefacts.  In  this  work,  smoothing  has  a  different  meaning  as  it  perceptuallyshapes  the  perturbation  according  to  the  visual  content  of  the  image  to  beattacked.  The  perturbation  becomes  locally  smooth  on  the  flat  areas  of  theinput  image,  but  it  may  be  noisy  on  its  textured  areas  and  sharp  across  itsedges.</p><p>This  operation  relies  on  Laplacian  smoothing,  well-known  in  graph  signalprocessing,  which  we  integrate  in  the  attack  pipeline.  We  benchmark  severalattacks  with  and  without  smoothing  under  a  white-box  scenario  and  evaluatetheir  transferability.  Despite  the  additional  constraint  of  smoothness,  ourattack  has  the  same  probability  of  success  at  lower  distortion.</p>
http://arxiv.org/abs/1903.11863|On  Inertial  Navigation  and  Attitude  Initialization  in  Polar  Areas.  (arXiv:1903.11863v1  [cs.RO])|<p>Inertial  navigation  and  attitude  initialization  in  polar  areas  become  a  hottopic  in  recent  years  in  the  navigation  community,  as  the  widely-usednavigation  mechanization  of  the  local  level  frame  encounters  the  inherentsingularity  when  the  latitude  approaches  90  degrees.  Great  endeavors  have  beendevoted  to  devising  novel  navigation  mechanizations  such  as  the  grid  ortransversal  frames.  This  paper  highlights  the  fact  that  the  common  Earth-framemechanization  is  sufficiently  good  to  well  handle  the  singularity  problem  inpolar  areas.  Simulation  results  are  reported  to  demonstrate  the  singularityproblem  and  the  effectiveness  of  the  Earth-frame  mechanization.</p>
http://arxiv.org/abs/1903.11873|Inconsistency  indices  for  incomplete  pairwise  comparisons  matrices.  (arXiv:1903.11873v1  [cs.DM])|<p>Comparing  alternatives  in  pairs  is  a  very  well  known  technique  of  rankingcreation.  The  answer  to  how  reliable  and  trustworthy  ranking  is  depends  on  theinconsistency  of  the  data  from  which  it  was  created.  There  are  many  indicesused  for  determining  the  level  of  inconsistency  among  compared  alternatives.Unfortunately,  most  of  them  assume  that  the  set  of  comparisons  is  complete,i.e.  every  single  alternative  is  compared  to  each  other.  This  is  not  true  andthe  ranking  must  sometimes  be  made  based  on  incomplete  data.  In  order  to  fillthis  gap,  this  work  aims  to  adapt  the  selected  twelve  existing  inconsistencyindices  for  the  purpose  of  analyzing  incomplete  data  sets.  The  modified  indicesare  subjected  to  Monte  Carlo  experiments.  Those  of  them  that  achieved  the  bestresults  in  the  experiments  carried  out  are  recommended  for  use  in  practice.</p>
http://arxiv.org/abs/1903.11874|Block  stochastic  gradient  descent  for  large-scale  tomographic  reconstruction  in  a  parallel  network.  (arXiv:1903.11874v1  [cs.DC])|<p>Iterative  algorithms  have  many  advantages  for  linear  tomographic  imagereconstruction  when  compared  to  back-projection  based  methods.  However,iterative  methods  tend  to  have  significantly  higher  computational  complexity.To  overcome  this,  parallel  processing  schemes  that  can  utilise  severalcomputing  nodes  are  desirable.  Popular  methods  here  are  row  action  methods,which  update  the  entire  image  simultaneously  and  column  action  methods,  whichrequire  access  to  all  measurements  at  each  node.  In  large  scale  tomographicreconstruction  with  limited  storage  capacity  of  each  node,  data  communicationoverheads  between  nodes  becomes  a  significant  performance  limiting  factor.  Toreduce  this  overhead,  we  proposed  a  row  action  method  BSGD.  The  method  is  basedon  the  stochastic  gradient  descent  method  but  it  does  not  update  the  entireimage  at  each  iteration,  which  reduces  between  node  communication.  To  furtherincrease  convergence  speeds,  an  importance  sampling  strategy  is  proposed.  Wecompare  BSGD  to  other  existing  stochastic  methods  and  show  its  effectivenessand  efficiency.  Other  properties  of  BSGD  are  also  explored,  including  itsability  to  incorporate  total  variation  (TV)  regularization  and  automaticparameter  tuning.</p>
http://arxiv.org/abs/1903.11875|A  Noise  Mitigation  Approach  for  VLC  Systems.  (arXiv:1903.11875v1  [cs.NI])|<p>Visible  Light  Communication  (VLC)  is  based  on  the  dual  use  of  theillumination  infrastructure  for  wireless  data  communication.  The  major  intereston  this  communication  technology  lies  on  its  specific  features  to  be  a  secure,cost-effective  wireless  technology.  Recently,  this  technology  has  gained  animportant  role  as  potential  candidate  for  complementing  traditional  RFcommunication  systems.  Anyway  a  major  issue  for  the  VLC  development  is  a  deepcomprehension  of  the  noise  and  its  impact  on  the  received  signal  at  thereceiver.  In  this  work,  we  present  a  simple  but  effective  approach  to  analyzethe  noise  and  drastically  reduce  it  through  a  signal  processing  method.  Inorder  to  validate  the  effectiveness  of  this  analytical  approach,  we  havedeveloped  an  USRP-based  testbed.  Experimental  results  have  been  carried  out  byevaluating  the  symbol  error  rate  (SER)  and  show  the  effectiveness  of  the  noisemitigation  approach  in  different  interference  conditions  and  at  differentdistance  between  the  transmitter  and  the  receiver.</p>
http://arxiv.org/abs/1903.11891|AED-Net:  An  Abnormal  Event  Detection  Network.  (arXiv:1903.11891v1  [cs.CV])|<p>It  is  challenging  to  detect  the  anomaly  in  crowded  scenes  for  quite  a  longtime.  In  this  paper,  a  self-supervised  framework,  abnormal  event  detectionnetwork  (AED-Net),  which  is  composed  of  PCAnet  and  kernel  principal  componentanalysis  (kPCA),  is  proposed  to  address  this  problem.  Using  surveillance  videosequences  of  different  scenes  as  raw  data,  PCAnet  is  trained  to  extracthigh-level  semantics  of  crowd's  situation.  Next,  kPCA,a  one-class  classifier,is  trained  to  determine  anomaly  of  the  scene.  In  contrast  to  some  prevailingdeep  learning  methods,the  framework  is  completely  self-supervised  because  itutilizes  only  video  sequences  in  a  normal  situation.  Experiments  of  global  andlocal  abnormal  event  detection  are  carried  out  on  UMN  and  UCSD  datasets,  andcompetitive  results  with  higher  EER  and  AUC  compared  to  other  state-of-the-artmethods  are  observed.  Furthermore,  by  adding  local  response  normalization  (LRN)layer,  we  propose  an  improvement  to  original  AED-Net.  And  it  is  proved  toperform  better  by  promoting  the  framework's  generalization  capacity  accordingto  the  experiments.</p>
http://arxiv.org/abs/1903.11899|Using  Blockchain  to  Rein  in  The  New  Post-Truth  World  and  Check  The  Spread  of  Fake  News.  (arXiv:1903.11899v1  [cs.CR])|<p>In  recent  years,  `fake  news'  has  become  a  global  issue  that  raisesunprecedented  challenges  for  human  society  and  democracy.  This  problem  hasarisen  due  to  the  emergence  of  various  concomitant  phenomena  such  as  (1)  thedigitization  of  human  life  and  the  ease  of  disseminating  news  through  socialnetworking  applications  (such  as  Facebook  and  WhatsApp);  (2)  the  availabilityof  `big  data'  that  allows  customization  of  news  feeds  and  the  creation  ofpolarized  so-called  `filter-bubbles';  and  (3)  the  rapid  progress  made  bygenerative  machine  learning  (ML)  and  deep  learning  (DL)  algorithms  in  creatingrealistic-looking  yet  fake  digital  content  (such  as  text,  images,  and  videos).There  is  a  crucial  need  to  combat  the  rampant  rise  of  fake  news  anddisinformation.  In  this  paper,  we  propose  a  high-level  overview  of  ablockchain-based  framework  for  fake  news  prevention  and  highlight  the  variousdesign  issues  and  consideration  of  such  a  blockchain-based  framework  fortackling  fake  news.</p>
http://arxiv.org/abs/1903.11900|Model  Vulnerability  to  Distributional  Shifts  over  Image  Transformation  Sets.  (arXiv:1903.11900v1  [cs.LG])|<p>We  are  concerned  with  the  vulnerability  of  computer  vision  models  todistributional  shifts.  We  cast  this  problem  in  terms  of  combinatorialoptimization,  evaluating  the  regions  in  the  input  space  where  a  (black-box)model  is  more  vulnerable.  This  is  carried  out  by  combining  imagetransformations  from  a  given  set  and  standard  search  algorithms.  We  embed  thisidea  in  a  training  procedure,  where  we  define  new  data  augmentation  rules  overiterations,  accordingly  to  the  image  transformations  that  the  current  model  ismost  vulnerable  to.  An  empirical  evaluation  on  classification  and  semanticsegmentation  problems  suggests  that  the  devised  algorithm  allows  to  trainmodels  more  robust  against  content-preserving  image  transformations,  and  ingeneral,  against  distributional  shifts.</p>
http://arxiv.org/abs/1903.11907|Meta-Learning  surrogate  models  for  sequential  decision  making.  (arXiv:1903.11907v1  [stat.ML])|<p>Meta-learning  methods  leverage  past  experience  to  learn  data-driven  inductivebiases  from  related  problems,  increasing  learning  efficiency  on  new  tasks.  Thisability  renders  them  particularly  suitable  for  sequential  decision  making  withlimited  experience.  Within  this  problem  family,  we  argue  for  the  use  of  suchapproaches  in  the  study  of  model-based  approaches  to  Bayesian  Optimisation,contextual  bandits  and  Reinforcement  Learning.  We  approach  the  problem  bylearning  distributions  over  functions  using  Neural  Processes  (NPs),  a  recentlyintroduced  probabilistic  meta-learning  method.  This  allows  the  treatment  ofmodel  uncertainty  to  tackle  the  exploration/exploitation  dilemma.  We  show  thatNPs  are  suitable  for  sequential  decision  making  on  a  diverse  set  of  domains,including  adversarial  task  search,  recommender  systems  and  model-basedreinforcement  learning.</p>
http://arxiv.org/abs/1903.11916|Intelligent  Processing  in  Vehicular  Ad  hoc  Networks:  a  Survey.  (arXiv:1903.11916v1  [cs.NI])|<p>The  intelligent  Processing  technique  is  more  and  more  attractive  toresearchers  due  to  its  ability  to  deal  with  key  problems  in  Vehicular  Ad  hocnetworks.  However,  several  problems  in  applying  intelligent  processingtechnologies  in  VANETs  remain  open.  The  existing  applications  arecomprehensively  reviewed  and  discussed,  and  classified  into  differentcategories  in  this  paper.  Their  strategies,  advantages/disadvantages,  andperformances  are  elaborated.  By  generalizing  different  tactics  in  variousapplications  related  to  different  scenarios  of  VANETs  and  evaluating  theirperformances,  several  promising  directions  for  future  research  have  beensuggested.</p>
http://arxiv.org/abs/1903.11919|Imbalanced  Sentiment  Classification  Enhanced  with  Discourse  Marker.  (arXiv:1903.11919v1  [cs.CL])|"<p>Imbalanced  data  commonly  exists  in  real  world,  espacially  insentiment-related  corpus,  making  it  difficult  to  train  a  classifier  todistinguish  latent  sentiment  in  text  data.  We  observe  that  humans  often  expresstransitional  emotion  between  two  adjacent  discourses  with  discourse  markerslike  ""but"",  ""though"",  ""while"",  etc,  and  the  head  discourse  and  the  taildiscourse  3  usually  indicate  opposite  emotional  tendencies.  Based  on  thisobservation,  we  propose  a  novel  plug-and-play  method,  which  first  samplesdiscourses  according  to  transitional  discourse  markers  and  then  validatessentimental  polarities  with  the  help  of  a  pretrained  attention-based  model.  Ourmethod  increases  sample  diversity  in  the  first  place,  can  serve  as  a  upstreampreprocessing  part  in  data  augmentation.  We  conduct  experiments  on  three  publicsentiment  datasets,  with  several  frequently  used  algorithms.  Results  show  thatour  method  is  found  to  be  consistently  effective,  even  in  highly  imbalancedscenario,  and  easily  be  integrated  with  oversampling  method  to  boost  theperformance  on  imbalanced  sentiment  classification.</p>"
http://arxiv.org/abs/1903.11936|Evolving  Boolean  Functions  with  Conjunctions  and  Disjunctions  via  Genetic  Programming.  (arXiv:1903.11936v1  [cs.NE])|<p>Recently  it  has  been  proved  that  simple  GP  systems  can  efficiently  evolve  theconjunction  of  $n$  variables  if  they  are  equipped  with  the  minimal  requiredcomponents.  In  this  paper,  we  make  a  considerable  step  forward  by  analysing  thebehaviour  and  performance  of  the  GP  system  for  evolving  a  Boolean  function  withunknown  components,  i.e.,  the  function  may  consist  of  both  conjunctions  anddisjunctions.  We  rigorously  prove  that  if  the  target  function  is  theconjunction  of  $n$  variables,  then  the  RLS-GP  using  the  complete  truth  table  toevaluate  program  quality  evolves  the  exact  target  function  in  $O(ell  n  log^2n)$  iterations  in  expectation,  where  $ell  geq  n$  is  a  limit  on  the  size  ofany  accepted  tree.  When,  as  in  realistic  applications,  only  a  polynomial  sampleof  possible  inputs  is  used  to  evaluate  solution  quality,  we  show  how  RLS-GP  canevolve  a  conjunction  with  any  polynomially  small  generalisation  error  withprobability  $1  -  O(log^2(n)/n)$.  To  produce  our  results  we  introduce  asuper-multiplicative  drift  theorem  that  gives  significantly  stronger  runtimebounds  when  the  expected  progress  is  only  slightly  super-linear  in  the  distancefrom  the  optimum.</p>
http://arxiv.org/abs/1903.11941|Application  of  Deep  Learning  Long  Short-Term  Memory  in  Energy  Demand  Forecasting.  (arXiv:1903.11941v1  [cs.LG])|<p>The  smart  metering  infrastructure  has  changed  how  electricity  is  measured  inboth  residential  and  industrial  application.  The  large  amount  of  data  collectedby  smart  meter  per  day  provides  a  huge  potential  for  analytics  to  support  theoperation  of  a  smart  grid,  an  example  of  which  is  energy  demand  forecasting.Short  term  energy  forecasting  can  be  used  by  utilities  to  assess  if  anyforecasted  peak  energy  demand  would  have  an  adverse  effect  on  the  power  systemtransmission  and  distribution  infrastructure.  It  can  also  help  in  loadscheduling  and  demand  side  management.  Many  techniques  have  been  proposed  toforecast  time  series  including  Support  Vector  Machine,  Artificial  NeuralNetwork  and  Deep  Learning.  In  this  work  we  use  Long  Short  Term  Memoryarchitecture  to  forecast  3-day  ahead  energy  demand  across  each  month  in  theyear.  The  results  show  that  3-day  ahead  demand  can  be  accurately  forecastedwith  a  Mean  Absolute  Percentage  Error  of  3.15%.  In  addition  to  that,  the  paperproposes  way  to  quantify  the  time  as  a  feature  to  be  used  in  the  training  phasewhich  is  shown  to  affect  the  network  performance.</p>
http://arxiv.org/abs/1903.11944|Taxonomies  in  DUI  Design  Patterns:  A  Systematic  Approach  for  Removing  Overlaps  Among  Design  Patterns  and  Creating  a  Clear  Hierarchy.  (arXiv:1903.11944v1  [cs.HC])|<p>Recently  a  library  of  design  patterns  for  designing  distributed  userinterfaces  (DUIs)  was  created  to  help  researchers  and  designers  to  create  userinterfaces  and  to  provide  an  overview  of  solutions  to  common  DUIs  designproblems  without  requiring  a  significant  amount  of  time  to  be  spent  on  readingdomain-specific  literature  and  exploring  existing  DUIs  implementations.  Thecurrent  version  of  the  DUI  design  patterns  library  need  to  be  assessed  becausea  lot  of  design  patterns  are  overlapping  each  other  and  their  relationships  arenot  clear  enough  to  effectively  find  the  most  relevant  design  pattern  forsolving  particular  design  problem,  so  the  purpose  of  this  thesis  is  to  maturethe  DUI  design  patterns  knowledge  field  by  removing  the  duplicate  designpatterns,  their  description  and  to  create  a  taxonomy  where  each  design  patternshould  be  organised  in  a  way  that  will  reduce  redundancy,  possibly  leading  togrouping  or  eventually  merging  similar  patterns  and  allow  to  navigate  torelated  patterns.  To  achieve  the  defined  goals,  the  first  target  was  toinvestigate  the  possible  overlaps  among  design  patterns  and  their  relevancywith  each  other,  in  order  to  get  these  insights  natural  language  processingtool  was  built  for  extracting  and  analysing  each  design  pattern  research  paperto  find  potential  codes.  Later  in  this  study  thematic  analysis  was  done  withdomain  experts  to  get  themes,  their  description  and  higher  level  categoriesfrom  generated  codes  to  organize  all  related  design  patterns  in  a  clearhierarchy.  The  outcomes  of  this  thesis  included  the  clarification  of  therelationships  among  design  patterns  by  creating  a  taxonomy,  clarified  thedescription  of  individual  design  pattern,  overlaps  and  duplicate  designpatterns  were  removed  and  merged  similar  design  patterns.</p>
http://arxiv.org/abs/1903.11960|Learning  Discrete  Structures  for  Graph  Neural  Networks.  (arXiv:1903.11960v1  [cs.LG])|<p>Graph  neural  networks  (GNNs)  are  a  popular  class  of  machine  learning  modelswhose  major  advantage  is  their  ability  to  incorporate  a  sparse  and  discretedependency  structure  between  data  points.  Unfortunately,  GNNs  can  only  be  usedwhen  such  a  graph-structure  is  available.  In  practice,  however,  real-worldgraphs  are  often  noisy  and  incomplete  or  might  not  be  available  at  all.  Withthis  work,  we  propose  to  jointly  learn  the  graph  structure  and  the  parametersof  graph  convolutional  networks  (GCNs)  by  approximately  solving  a  bilevelprogram  that  learns  a  discrete  probability  distribution  on  the  edges  of  thegraph.  This  allows  one  to  apply  GCNs  not  only  in  scenarios  where  the  givengraph  is  incomplete  or  corrupted  but  also  in  those  where  a  graph  is  notavailable.  We  conduct  a  series  of  experiments  that  analyze  the  behavior  of  theproposed  method  and  demonstrate  that  it  outperforms  related  methods  by  asignificant  margin.</p>
http://arxiv.org/abs/1903.11968|On  the  stability  of  periodic  binary  sequences  with  zone  restriction.  (arXiv:1903.11968v1  [cs.IT])|<p>Traditional  global  stability  measure  for  sequences  is  hard  to  determinebecause  of  large  search  space.  We  propose  the  $k$-error  linear  complexity  witha  zone  restriction  for  measuring  the  local  stability  of  sequences.  Accordingly,we  can  efficiently  determine  the  global  stability  by  studying  a  local  stabilityfor  these  sequences.  For  several  classes  of  sequences,  we  demonstrate  that  the$k$-error  linear  complexity  is  identical  to  the  $k$-error  linear  complexitywithin  a  zone,  while  the  length  of  a  zone  is  much  smaller  than  the  whole  periodwhen  the  $k$-error  linear  complexity  is  large.  These  sequences  have  periods$2^n$,  or  $2^v  r$  ($r$  odd  prime  and  $2$  is  primitive  modulo  $r$),  or  $2^vp_1^{s_1}  cdots  p_n^{s_n}$  ($p_i$  is  an  odd  prime  and  $2$  is  primitive  modulo$p_i$  and  $p_i^2$,  where  $1leq  i  leq  n$)  respectively.  In  particular,  wecompletely  determine  the  spectrum  of  $1$-error  linear  complexity  with  any  zonelength  for  an  arbitrary  $2^n$-periodic  binary  sequence.</p>
http://arxiv.org/abs/1903.11971|The  Global  Convergence  Analysis  of  the  Bat  Algorithm  Using  a  Markovian  Framework  and  Dynamical  System  Theory.  (arXiv:1903.11971v1  [math.OC])|<p>The  bat  algorithm  (BA)  has  been  shown  to  be  effective  to  solve  a  wider  rangeof  optimization  problems.  However,  there  is  not  much  theoretical  analysisconcerning  its  convergence  and  stability.  In  order  to  prove  the  convergence  ofthe  bat  algorithm,  we  have  built  a  Markov  model  for  the  algorithm  and  provedthat  the  state  sequence  of  the  bat  population  forms  a  finite  homogeneous  Markovchain,  satisfying  the  global  convergence  criteria.  Then,  we  prove  that  the  batalgorithm  can  have  global  convergence.  In  addition,  in  order  to  enhance  theconvergence  performance  of  the  algorithm,  we  have  designed  an  updated  modelusing  the  dynamical  system  theory  in  terms  of  a  dynamic  matrix,  and  theparameter  ranges  for  the  algorithm  stability  are  then  obtained.  We  then  usesome  benchmark  functions  to  demonstrate  that  BA  can  indeed  achieve  globaloptimality  efficiently  for  these  functions.</p>
http://arxiv.org/abs/1903.11980|Probabilistic  Analysis  of  Facility  Location  on  Random  Shortest  Path  Metrics.  (arXiv:1903.11980v1  [cs.DS])|<p>The  facility  location  problem  is  an  NP-hard  optimization  problem.  Therefore,approximation  algorithms  are  often  used  to  solve  large  instances.  Suchalgorithms  often  perform  much  better  than  worst-case  analysis  suggests.Therefore,  probabilistic  analysis  is  a  widely  used  tool  to  analyze  suchalgorithms.  Most  research  on  probabilistic  analysis  of  NP-hard  optimizationproblems  involving  metric  spaces,  such  as  the  facility  location  problem,  hasbeen  focused  on  Euclidean  instances,  and  also  instances  with  independent(random)  edge  lengths,  which  are  non-metric,  have  been  researched.  We  wouldlike  to  extend  this  knowledge  to  other,  more  general,  metrics.</p><p>We  investigate  the  facility  location  problem  using  random  shortest  pathmetrics.  We  analyze  some  probabilistic  properties  for  a  simple  greedy  heuristicwhich  gives  a  solution  to  the  facility  location  problem:  opening  the  $kappa$cheapest  facilities  (with  $kappa$  only  depending  on  the  facility  openingcosts).  If  the  facility  opening  costs  are  such  that  $kappa$  is  not  too  large,then  we  show  that  this  heuristic  is  asymptotically  optimal.  On  the  other  hand,for  large  values  of  $kappa$,  the  analysis  becomes  more  difficult,  and  weprovide  a  closed-form  expression  as  upper  bound  for  the  expected  approximationratio.  In  the  special  case  where  all  facility  opening  costs  are  equal  thisclosed-form  expression  reduces  to  $O(sqrt[4]{ln(n)})$  or  $O(1)$  or  even$1+o(1)$  if  the  opening  costs  are  sufficiently  small.</p>
http://arxiv.org/abs/1903.11981|Regularizing  Trajectory  Optimization  with  Denoising  Autoencoders.  (arXiv:1903.11981v1  [cs.LG])|<p>Trajectory  optimization  with  learned  dynamics  models  can  often  suffer  fromerroneous  predictions  of  out-of-distribution  trajectories.  We  propose  toregularize  trajectory  optimization  by  means  of  a  denoising  autoencoder  that  istrained  on  the  same  trajectories  as  the  dynamics  model.  We  visually  demonstratethe  effectiveness  of  the  regularization  in  gradient-based  trajectoryoptimization  for  open-loop  control  of  an  industrial  process.  We  compare  withrecent  model-based  reinforcement  learning  algorithms  on  a  set  of  popular  motorcontrol  tasks  to  demonstrate  that  the  denoising  regularization  enablesstate-of-the-art  sample-efficiency.  We  demonstrate  the  efficacy  of  the  proposedmethod  in  regularizing  both  gradient-based  and  gradient-free  trajectoryoptimization.</p>
http://arxiv.org/abs/1903.11983|Sentiment  Analysis  on  IMDB  Movie  Comments  and  Twitter  Data  by  Machine  Learning  and  Vector  Space  Techniques.  (arXiv:1903.11983v1  [cs.IR])|"<p>This  study's  goal  is  to  create  a  model  of  sentiment  analysis  on  a  2000  rowsIMDB  movie  comments  and  3200  Twitter  data  by  using  machine  learning  and  vectorspace  techniques;  positive  or  negative  preliminary  information  about  the  textis  to  provide.  In  the  study,  a  vector  space  was  created  in  the  KNIME  Analyticsplatform,  and  a  classification  study  was  performed  on  this  vector  space  byDecision  Trees,  Na""ive  Bayes  and  Support  Vector  Machines  classificationalgorithms.  The  conclusions  obtained  were  compared  in  terms  of  each  algorithms.The  classification  results  for  IMDB  movie  comments  are  obtained  as  94,00%,73,20%,  and  85,50%  by  Decision  Tree,  Naive  Bayes  and  SVM  algorithms.  Theclassification  results  for  Twitter  data  set  are  presented  as  82,76%,  75,44%  and72,50%  by  Decision  Tree,  Naive  Bayes  SVM  algorithms  as  well.  It  is  seen  thatthe  best  classification  results  presented  in  both  data  sets  are  whichcalculated  by  SVM  algorithm.</p>"
http://arxiv.org/abs/1903.11987|Universal  chosen-ciphertext  attack  for  a  family  of  image  encryption  schemes.  (arXiv:1903.11987v1  [cs.MM])|<p>During  the  past  decades,  there  is  a  great  popularity  employing  nonlineardynamics  and  permutation-substitution  architecture  for  image  encryption.  Thereare  three  primary  procedures  in  such  encryption  schemes,  the  key  schedulemodule  for  producing  encryption  factors,  permutation  for  image  scrambling  andsubstitution  for  pixel  modification.  Under  the  assumption  of  chosen-ciphertextattack,  we  evaluate  the  security  of  a  class  of  image  ciphers  which  adoptspixel-level  permutation  and  modular  addition  for  substitution.  It  ismathematically  revealed  that  the  mapping  from  differentials  of  ciphertexts  tothose  of  plaintexts  are  linear  and  has  nothing  to  do  with  the  key  schedules,permutation  techniques  and  encryption  rounds.  Moreover,  a  universalchosen-ciphertext  attack  is  proposed  and  validated.  Experimental  resultsdemonstrate  that  the  plaintexts  can  be  directly  reconstructed  without  anysecurity  key  or  encryption  elements.  Related  cryptographic  discussions  are  alsogiven.</p>
http://arxiv.org/abs/1903.11990|On  the  Stability  and  Generalization  of  Learning  with  Kernel  Activation  Functions.  (arXiv:1903.11990v1  [stat.ML])|<p>In  this  brief  we  investigate  the  generalization  properties  of  arecently-proposed  class  of  non-parametric  activation  functions,  the  kernelactivation  functions  (KAFs).  KAFs  introduce  additional  parameters  in  thelearning  process  in  order  to  adapt  nonlinearities  individually  on  a  per-neuronbasis,  exploiting  a  cheap  kernel  expansion  of  every  activation  value.  Whilethis  increase  in  flexibility  has  been  shown  to  provide  significant  improvementsin  practice,  a  theoretical  proof  for  its  generalization  capability  has  not  beenaddressed  yet  in  the  literature.  Here,  we  leverage  recent  literature  on  thestability  properties  of  non-convex  models  trained  via  stochastic  gradientdescent  (SGD).  By  indirectly  proving  two  key  smoothness  properties  of  themodels  under  consideration,  we  prove  that  neural  networks  endowed  with  KAFsgeneralize  well  when  trained  with  SGD  for  a  finite  number  of  steps.Interestingly,  our  analysis  provides  a  guideline  for  selecting  one  of  thehyper-parameters  of  the  model,  the  bandwidth  of  the  scalar  Gaussian  kernel.  Ashort  experimental  evaluation  validates  the  proof.</p>
http://arxiv.org/abs/1903.11991|PAL:  A  fast  DNN  optimization  method  based  on  curvature  information.  (arXiv:1903.11991v1  [cs.LG])|<p>We  present  a  novel  optimizer  for  deep  neural  networks  that  combines  the  ideasof  Netwon's  method  and  line  search  to  efficiently  compute  and  utilize  curvatureinformation.  Our  work  is  based  on  empirical  observation  suggesting  that  theloss  function  can  be  approximated  by  a  parabola  in  negative  gradient  direction.Due  to  this  approximation,  we  are  able  to  perform  a  variable  and  loss  functiondependent  parameter  update  by  jumping  directly  into  the  minimum  of  theapproximated  parabola.  To  evaluate  our  optimizer,  we  performed  multiplecomprehensive  hyperparameter  grid  searches  for  which  we  trained  more  than  20000networks  in  total.  We  can  show  that  PAL  outperforms  RMSPROP,  and  can  outperformgradient  descent  with  momentum  and  ADAM  on  large-scale  high-dimensional  machinelearning  problems.  Furthermore,  PAL  requires  up  to  52.2%  less  training  epochs.PyTorch  and  TensorFlow  implementations  are  provided  athttps://github.com/cogsys-tuebingen/PAL.</p>
http://arxiv.org/abs/1903.11993|Fault  and  Performance  Management  in  Multi-Cloud  Based  NFV  using  Shallow  and  Deep  Predictive  Structures.  (arXiv:1903.11993v1  [cs.NI])|<p>Deployment  of  Network  Function  Virtualization  (NFV)  over  multiple  cloudsaccentuates  its  advantages  like  the  flexibility  of  virtualization,  proximity  tocustomers  and  lower  total  cost  of  operation.  However,  NFV  over  multiple  cloudshas  not  yet  attained  the  level  of  performance  to  be  a  viable  replacement  fortraditional  networks.  One  of  the  reasons  is  the  absence  of  a  standard  basedFault,  Configuration,  Accounting,  Performance  and  Security  (FCAPS)  frameworkfor  the  virtual  network  services.  In  NFV,  faults  and  performance  issues  canhave  complex  geneses  within  virtual  resources  as  well  as  virtual  networks  andcannot  be  effectively  handled  by  traditional  rule-based  systems.  To  tackle  theabove  problem,  we  propose  a  fault  detection  and  localization  model  based  on  acombination  of  shallow  and  deep  learning  structures.  Relatively  simplerdetection  of  'fault'  and  'no-fault'  conditions  or  'manifest'  and  'impending'faults  have  been  effectively  shown  to  be  handled  by  shallow  machine  learningstructures  like  Support  Vector  Machine  (SVM).  Deeper  structure,  i.e.  thestacked  autoencoder  has  been  found  to  be  useful  for  a  more  complex  localizationfunction  where  a  large  amount  of  information  needs  to  be  worked  through,  indifferent  layers,  to  get  to  the  root  cause  of  the  problem.  We  provideevaluation  results  using  a  dataset  adapted  from  logs  of  disruption  in  anoperator's  live  network  fault  datasets  available  on  Kaggle  and  another  based  onmultivariate  kernel  density  estimation  and  Markov  sampling.</p>
http://arxiv.org/abs/1903.11994|Efficient  Virtual  Network  Function  Placement  Strategies  for  Cloud  Radio  Access  Networks.  (arXiv:1903.11994v1  [cs.NI])|<p>The  new  generation  of  5G  mobile  services  places  stringent  requirements  forcellular  network  operators  in  terms  of  latency  and  costs.  The  latest  trend  inradio  access  networks  (RANs)  is  to  pool  the  baseband  units  (BBUs)  of  multipleradio  base  stations  and  to  install  them  in  a  centralized  infrastructure,  suchas  a  cloud,  for  statistical  multiplexing  gains.  The  technology  is  known  asCloud  Radio  Access  Network  (CRAN).  Since  cloud  computing  is  gaining  significanttraction  and  virtualized  data  centers  are  becoming  popular  as  a  cost-effectiveinfrastructure  in  the  telecommunication  industry,  CRAN  is  being  heralded  as  acandidate  technology  to  meet  the  expectations  of  radio  access  networks  for  5G.In  CRANs,  low  energy  base  stations  (BSs)  are  deployed  over  a  small  geographicallocation  and  are  connected  to  a  cloud  via  finite  capacity  backhaul  links.Baseband  processing  unit  (BBU)  functions  are  implemented  on  the  virtualmachines  (VMs)  in  the  cloud  over  commodity  hardware.  Such  functions,  built-insoftware,  are  termed  as  virtual  functions  (VFs).  The  optimized  placement  of  VFsis  necessary  to  reduce  the  total  delays  and  minimize  the  overall  costs  tooperate  CRANs.  Our  study  considers  the  problem  of  optimal  VF  placement  overdistributed  virtual  resources  spread  across  multiple  clouds,  creating  acentralized  BBU  cloud.  We  propose  a  combinatorial  optimization  model  and  theuse  of  two  heuristic  approaches,  which  are,  branch-and-bound  (BnB)  andsimulated  annealing  (SA)  for  the  proposed  optimal  placement.  In  addition,  wepropose  enhancements  to  the  standard  BnB  heuristic  and  compare  the  results  withstandard  BnB  and  SA  approaches.  The  proposed  enhancements  improve  the  qualityof  the  solution  in  terms  of  latency  and  cost  as  well  as  reduce  the  executioncomplexity  significantly.</p>
http://arxiv.org/abs/1903.11996|To  boldly  go  where  no  sensor  has  gone  before:  The  movement  to  place  IoT  in  radical  new  spaces.  (arXiv:1903.11996v1  [eess.SP])|<p>In  this  article,  we  have  presented  an  overview  on  a  unified  framework  whichis  called  as  the  Internet  of  X-things  (X-IoT)  that  will  warrant  the  convergenceof  emerging  use  cases  of  the  Internet  of  Things  everywhere  around  us,  i.e.,under  the  ground  and  oceans  and  even  in  the  outer  space.  It  is  anticipated  thatsuch  a  framework  will  foster  the  design  and  development  of  smart  objectscapable  of  performing  sensing  under  all-rounded  environment  and  communicationtechnologies  capable  of  offering  ubiquitous  connectivity  of  course  with  thedesired  requirements.  Through  this  framework,  we  get  to  know  what  has  been  donesince  recently  and  how  the  technical  challenges  across  the  broad  spectrum  ofemerging  use  cases  under  the  water,  underground  and  over  the  space  areconverging  toward  future  solutions.</p>
http://arxiv.org/abs/1903.11997|A  gradual  approach  for  maximising  user  conversion  without  compromising  experience  with  high  visual  intensity  website  elements.  (arXiv:1903.11997v1  [cs.HC])|<p>The  study  develops  and  tests  a  method  that  can  gradually  find  a  sweet  spotbetween  user  experience  and  visual  intensity  of  website  elements  to  maximiseuser  conversion  with  minimal  adverse  effect.  In  the  first  phase  of  the  study,we  develop  the  method.  In  the  second  stage,  we  test  and  evaluate  the  method  viaan  empirical  study;  also,  an  experiment  was  conducted  within  web  interface  withthe  gradual  intensity  of  visual  elements.The  findings  reveal  that  negativeresponse  grows  faster  than  conversion  when  the  visual  intensity  of  the  webinterface  is  increased.  However,  a  saturation  point,  where  there  is  coexistencebetween  maximum  conversion  and  minimum  negative  response,  can  be  found.  Thefindings  imply  that  efforts  to  attract  user  attention  should  be  pursued  withincreased  caution  and  that  a  gradual  approach  presented  in  this  study  helps  infinding  a  site-specific  sweet-spot  for  a  level  of  visual  intensity  byincrementally  adjusting  the  elements  of  the  interface  and  tracking  the  changesin  user  behaviour.  Web  marketing  and  advertising  professionals  often  face  thedilemma  of  determining  the  optimal  level  of  visual  intensity  of  interfaceelement.  Excessive  use  of  marketing  component  and  attention-grabbing  visualelements  can  lead  to  an  inferior  user  experience  and  consequent  user  churn  dueto  growing  intrusiveness.  At  the  same  time,  too  little  visual  intensity  canfail  to  steer  users.  The  present  study  provides  a  gradual  approach  which  aidsin  finding  a  balance  between  user  experience  and  visual  intensity,  maximisinguser  conversion  and  thus  providing  a  practical  solution  for  the  problem.</p>
http://arxiv.org/abs/1903.12001|Experimental  Implementation  of  a  New  Non-redundant  6-DOF  Quadrotor  Manipulation  System.  (arXiv:1903.12001v1  [cs.RO])|<p>This  paper  presents  an  experimental  validation  of  a  new  quadrotor-basedaerial  manipulator.  A  quadrotor  is  equipped  with  a  2-DOF  robotic  arm  that  isdesigned  with  a  new  topology  to  enable  the  end-effector  of  the  whole  system  totrack  a  6-DOF  trajectory.  An  identification  experiment  is  carried  out  to  findout  the  system  parameters.  The  mathematical  model  of  the  whole  system  ispresented.  A  measurement  scheme  is  proposed  to  get  the  accurate  pose  of  thevehicle  considering  the  motion  of  the  manipulator  below  the  quadrotor.  Thesystem  controller  is  designed  and  implemented  based  on  PID  with  a  gravitycompensation  algorithm.  System  simulation  is  implemented  in  the  MATLAB/SIMULINKenvironment  with  real  system  parameters,  to  better  emulate  a  realistic  setup.Real-time  Experiments  are  conducted.  Both  simulation  and  experimental  resultsshow  the  feasibility  and  a  satisfactory  efficiency  of  the  proposed  system  inachieving  the  position  holding  and  transferring  an  object  to  a  specific  targetposition.</p>
http://arxiv.org/abs/1903.12003|High  Fidelity  Face  Manipulation  with  Extreme  Pose  and  Expression.  (arXiv:1903.12003v1  [cs.CV])|<p>Face  manipulation  has  shown  remarkable  advances  with  the  flourish  ofGenerative  Adversarial  Networks.  However,  due  to  the  difficulties  ofcontrolling  the  structure  and  texture  in  high-resolution,  it  is  challenging  tosimultaneously  model  pose  and  expression  during  manipulation.  In  this  paper,  wepropose  a  novel  framework  that  simplifies  face  manipulation  with  extreme  poseand  expression  into  two  correlated  stages:  a  boundary  prediction  stage  and  adisentangled  face  synthesis  stage.  In  the  first  stage,  we  propose  to  use  aboundary  image  for  joint  pose  and  expression  modeling.  An  encoder-decodernetwork  is  employed  to  predict  the  boundary  image  of  the  target  face  in  asemi-supervised  way.  Pose  and  expression  estimators  are  used  to  improve  theprediction  accuracy.  In  the  second  stage,  the  predicted  boundary  image  and  theoriginal  face  are  encoded  into  the  structure  and  texture  latent  space  by  twoencoder  networks  respectively.  A  proxy  network  and  a  feature  threshold  loss  arefurther  imposed  as  constraints  to  disentangle  the  latent  space.  In  addition,  webuild  up  a  new  high  quality  Multi-View  Face  (MVF-HQ)  database  that  contains120K  high-resolution  face  images  of  479  identities  with  pose  and  expressionvariations,  which  will  be  released  soon.  Qualitative  and  quantitativeexperiments  on  four  databases  show  that  our  method  pushes  forward  the  advanceof  extreme  face  manipulation  from  128  $	imes$  128  resolution  to  1024  $	imes$1024  resolution,  and  significantly  improves  the  face  recognition  performanceunder  large  poses.</p>
http://arxiv.org/abs/1903.12008|Handling  Noisy  Labels  for  Robustly  Learning  from  Self-Training  Data  for  Low-Resource  Sequence  Labeling.  (arXiv:1903.12008v1  [cs.CL])|<p>In  this  paper,  we  address  the  problem  of  effectively  self-training  neuralnetworks  in  a  low-resource  setting.  Self-training  is  frequently  used  toautomatically  increase  the  amount  of  training  data.  However,  in  a  low-resourcescenario,  it  is  less  effective  due  to  unreliable  annotations  created  usingself-labeling  of  unlabeled  data.  We  propose  to  combine  self-training  with  noisehandling  on  the  self-labeled  data.  Directly  estimating  noise  on  the  combinedclean  training  set  and  self-labeled  data  can  lead  to  corruption  of  the  cleandata  and  hence,  performs  worse.  Thus,  we  propose  the  Clean  and  Noisy  LabelNeural  Network  which  trains  on  clean  and  noisy  self-labeled  data  simultaneouslyby  explicitly  modelling  clean  and  noisy  labels  separately.  In  our  experimentson  Chunking  and  NER,  this  approach  performs  more  robustly  than  the  baselines.Complementary  to  this  explicit  approach,  noise  can  also  be  handled  implicitlywith  the  help  of  an  auxiliary  learning  task.  To  such  a  complementary  approach,our  method  is  more  beneficial  than  other  baseline  methods  and  together  providesthe  best  performance  overall.</p>
http://arxiv.org/abs/1903.12011|Novel  Artificial  Human  Optimization  Field  Algorithms  -  The  Beginning.  (arXiv:1903.12011v1  [cs.NE])|"<p>New  Artificial  Human  Optimization  (AHO)  Field  Algorithms  can  be  created  fromscratch  or  by  adding  the  concept  of  Artificial  Humans  into  other  existingOptimization  Algorithms.  Particle  Swarm  Optimization  (PSO)  has  been  verypopular  for  solving  complex  optimization  problems  due  to  its  simplicity.  Inthis  work,  new  Artificial  Human  Optimization  Field  Algorithms  are  created  bymodifying  existing  PSO  algorithms  with  AHO  Field  Concepts.  These  Hybrid  PSOAlgorithms  comes  under  PSO  Field  as  well  as  AHO  Field.  There  are  Hybrid  PSOresearch  articles  based  on  Human  Behavior,  Human  Cognition  and  Human  Thinkingetc.  But  there  are  no  Hybrid  PSO  articles  which  based  on  concepts  like  HumanDisease,  Human  Kindness  and  Human  Relaxation.  This  paper  proposes  new  AHO  Fieldalgorithms  based  on  these  research  gaps.  Some  existing  Hybrid  PSO  algorithmsare  given  a  new  name  in  this  work  so  that  it  will  be  easy  for  future  AHOresearchers  to  find  these  novel  Artificial  Human  Optimization  Field  Algorithms.A  total  of  6  Artificial  Human  Optimization  Field  algorithms  titled  ""HumanSafety  Particle  Swarm  Optimization  (HuSaPSO)"",  ""Human  Kindness  Particle  SwarmOptimization  (HKPSO)"",  ""Human  Relaxation  Particle  Swarm  Optimization  (HRPSO)"",""Multiple  Strategy  Human  Particle  Swarm  Optimization  (MSHPSO)"",  ""Human  ThinkingParticle  Swarm  Optimization  (HTPSO)""  and  ""Human  Disease  Particle  SwarmOptimization  (HDPSO)""  are  tested  by  applying  these  novel  algorithms  on  Ackley,Beale,  Bohachevsky,  Booth  and  Three-Hump  Camel  Benchmark  Functions.  Resultsobtained  are  compared  with  PSO  algorithm.</p>"
http://arxiv.org/abs/1903.12012|Forecasting  model  based  on  information-granulated  GA-SVR  and  ARIMA  for  producer  price  index.  (arXiv:1903.12012v1  [stat.AP])|<p>The  accuracy  of  predicting  the  Producer  Price  Index  (PPI)  plays  anindispensable  role  in  government  economic  work.  However,  it  is  difficult  toforecast  the  PPI.  In  our  research,  we  first  propose  an  unprecedented  hybridmodel  based  on  fuzzy  information  granulation  that  integrates  the  GA-SVR  andARIMA  (Autoregressive  Integrated  Moving  Average  Model)  models.  Thefuzzy-information-granulation-based  GA-SVR-ARIMA  hybrid  model  is  intended  todeal  with  the  problem  of  imprecision  in  PPI  estimation.  The  proposed  modeladopts  the  fuzzy  information-granulation  algorithm  topre-classification-process  monthly  training  samples  of  the  PPI,  and  producedthree  different  sequences  of  fuzzy  information  granules,  whose  Support  VectorRegression  (SVR)  machine  forecast  models  were  separately  established  for  theirGenetic  Algorithm  (GA)  optimization  parameters.  Finally,  the  residual  errors  ofthe  GA-SVR  model  were  rectified  through  ARIMA  modeling,  and  the  PPI  estimatewas  reached.  Research  shows  that  the  PPI  value  predicted  by  this  hybrid  modelis  more  accurate  than  that  predicted  by  other  models,  including  ARIMA,  GRNN,and  GA-SVR,  following  several  comparative  experiments.  Research  also  indicatesthe  precision  and  validation  of  the  PPI  prediction  of  the  hybrid  model  anddemonstrates  that  the  model  has  consistent  ability  to  leverage  the  forecastingadvantage  of  GA-SVR  in  non-linear  space  and  of  ARIMA  in  linear  space.</p>
http://arxiv.org/abs/1903.12017|Train,  Sort,  Explain:  Learning  to  Diagnose  Translation  Models.  (arXiv:1903.12017v1  [cs.CL])|<p>Evaluating  translation  models  is  a  trade-off  between  effort  and  detail.  Onthe  one  end  of  the  spectrum  there  are  automatic  count-based  methods  such  asBLEU,  on  the  other  end  linguistic  evaluations  by  humans,  which  arguably  aremore  informative  but  also  require  a  disproportionately  high  effort.  To  narrowthe  spectrum,  we  propose  a  general  approach  on  how  to  automatically  exposesystematic  differences  between  human  and  machine  translations  to  human  experts.Inspired  by  adversarial  settings,  we  train  a  neural  text  classifier  todistinguish  human  from  machine  translations.  A  classifier  that  performs  andgeneralizes  well  after  training  should  recognize  systematic  differences  betweenthe  two  classes,  which  we  uncover  with  neural  explainability  methods.  Ourproof-of-concept  implementation,  DiaMaT,  is  open  source.  Applied  to  a  datasettranslated  by  a  state-of-the-art  neural  Transformer  model,  DiaMaT  achieves  aclassification  accuracy  of  75%  and  exposes  meaningful  differences  betweenhumans  and  the  Transformer,  amidst  the  current  discussion  about  human  parity.</p>
http://arxiv.org/abs/1903.12018|Team  Optimal  Decentralized  State  Estimation.  (arXiv:1903.12018v1  [cs.SY])|<p>We  consider  the  problem  of  optimal  decentralized  estimation  of  a  linearstochastic  process  by  multiple  agents.  Each  agent  receives  a  noisy  observationof  the  state  of  the  process  and  delayed  observations  of  its  neighbors(according  to  a  pre-specified,  strongly  connected,  communication  graph).  Basedon  their  observations,  all  agents  generate  a  sequence  of  estimates  of  the  stateof  the  process.  The  objective  is  to  minimize  the  total  expected  weighted  meansquare  error  between  the  state  and  the  agents'  estimates  over  a  finite  horizon.In  centralized  estimation  with  weighted  mean  square  error  criteria,  the  optimalestimator  does  not  depend  on  the  weight  matrix  in  the  cost  function.  We  showthat  this  is  not  the  case  when  the  information  is  decentralized.  The  optimaldecentralized  estimates  depend  on  the  weight  matrix  in  the  cost  function.  Inparticular,  we  show  that  the  optimal  estimate  consists  of  two  parts:  a  commonestimate  which  is  the  conditional  mean  of  the  state  given  the  commoninformation  and  a  correction  term  which  is  a  linear  function  of  the  offset  ofthe  local  information  from  the  conditional  expectation  of  the  local  informationgiven  the  common  information.  The  corresponding  gain  depends  on  the  weightmatrix  as  well  as  on  the  covariance  between  the  offset  of  agents'  localinformation  from  the  conditional  mean  of  the  local  information  given  the  commoninformation.  We  show  that  the  local  and  common  estimates  can  be  computed  from  asingle  Kalman  filter  and  derive  recursive  expressions  for  computing  the  offsetcovariances  and  the  estimation  gains.</p>
http://arxiv.org/abs/1903.12019|Multimodal  Deep  Network  Embedding  with  Integrated  Structure  and  Attribute  Information.  (arXiv:1903.12019v1  [cs.LG])|<p>Network  embedding  is  the  process  of  learning  low-dimensional  representationsfor  nodes  in  a  network,  while  preserving  node  features.  Existing  studies  onlyleverage  network  structure  information  and  focus  on  preserving  structuralfeatures.  However,  nodes  in  real-world  networks  often  have  a  rich  set  ofattributes  providing  extra  semantic  information.  It  has  been  demonstrated  thatboth  structural  and  attribute  features  are  important  for  network  analysistasks.  To  preserve  both  features,  we  investigate  the  problem  of  integratingstructure  and  attribute  information  to  perform  network  embedding  and  propose  aMultimodal  Deep  Network  Embedding  (MDNE)  method.  MDNE  captures  the  non-linearnetwork  structures  and  the  complex  interactions  among  structures  andattributes,  using  a  deep  model  consisting  of  multiple  layers  of  non-linearfunctions.  Since  structures  and  attributes  are  two  different  types  ofinformation,  a  multimodal  learning  method  is  adopted  to  pre-process  them  andhelp  the  model  to  better  capture  the  correlations  between  node  structure  andattribute  information.  We  employ  both  structural  proximity  and  attributeproximity  in  the  loss  function  to  preserve  the  respective  features  and  therepresentations  are  obtained  by  minimizing  the  loss  function.  Results  ofextensive  experiments  on  four  real-world  datasets  show  that  the  proposed  methodperforms  significantly  better  than  baselines  on  a  variety  of  tasks,  whichdemonstrate  the  effectiveness  and  generality  of  our  method.</p>
http://arxiv.org/abs/1903.12020|Describing  like  humans:  on  diversity  in  image  captioning.  (arXiv:1903.12020v1  [cs.CV])|<p>Recently,  the  state-of-the-art  models  for  image  captioning  have  overtakenhuman  performance  based  on  the  most  popular  metrics,  such  as  BLEU,  METEOR,ROUGE,  and  CIDEr.  Does  this  mean  we  have  solved  the  task  of  image  captioning?The  above  metrics  only  measure  the  similarity  of  the  generated  caption  to  thehuman  annotations,  which  reflects  its  accuracy.  However,  an  image  contains  manyconcepts  and  multiple  levels  of  detail,  and  thus  there  is  a  variety  of  captionsthat  express  different  concepts  and  details  that  might  be  interesting  fordifferent  humans.  Therefore  only  evaluating  accuracy  is  not  sufficient  formeasuring  the  performance  of  captioning  models  ---  the  diversity  of  thegenerated  captions  should  also  be  considered.  In  this  paper,  we  proposed  a  newmetric  for  measuring  the  diversity  of  image  captions,  which  is  derived  fromlatent  semantic  analysis  and  kernelized  to  use  CIDEr  similarity.  We  conductextensive  experiments  to  re-evaluate  recent  captioning  models  in  the  context  ofboth  diversity  and  accuracy.  We  find  that  there  is  still  a  large  gap  betweenthe  model  and  human  performance  in  terms  of  both  accuracy  and  diversity  and  themodels  that  have  optimized  accuracy  (CIDEr)  have  low  diversity.  We  also  showthat  balancing  the  cross-entropy  loss  and  CIDEr  reward  in  reinforcementlearning  during  training  can  effectively  control  the  tradeoff  between  diversityand  accuracy  of  the  generated  captions.</p>
http://arxiv.org/abs/1903.12021|Counting  the  learnable  functions  of  structured  data.  (arXiv:1903.12021v1  [cond-mat.dis-nn])|<p>Cover's  function  counting  theorem  is  a  milestone  in  the  theory  of  artificialneural  networks.  It  provides  an  answer  to  the  fundamental  question  ofdetermining  how  many  binary  assignments  (dichotomies)  of  $p$  points  in  $n$dimensions  can  be  linearly  realized.  Regrettably,  it  has  proved  hard  to  extendthe  same  approach  to  more  advanced  problems  than  the  classification  of  points.In  particular,  an  emerging  necessity  is  to  find  methods  to  deal  with  structureddata,  and  specifically  with  non-pointlike  patterns.  A  prominent  case  is  that  ofinvariant  recognition,  whereby  identification  of  a  stimulus  is  insensitive  toirrelevant  transformations  on  the  inputs  (such  as  rotations  or  changes  inperspective  in  an  image).  An  object  is  therefore  represented  by  an  extendedperceptual  manifold,  consisting  of  inputs  that  are  classified  similarly.  Here,we  develop  a  function  counting  theory  for  structured  data  of  this  kind,  byextending  Cover's  combinatorial  technique,  and  we  derive  analytical  expressionsfor  the  average  number  of  dichotomies  of  generically  correlated  sets  ofpatterns.  As  an  application,  we  obtain  a  closed  formula  for  the  capacity  of  abinary  classifier  trained  to  distinguish  general  polytopes  of  any  dimension.These  results  may  help  extend  our  theoretical  understanding  of  generalization,feature  extraction,  and  invariant  object  recognition  by  neural  networks.</p>
http://arxiv.org/abs/1903.12033|Repeatable  and  Reproducible  Wireless  Networking  Experimentation  through  Trace-based  Simulation.  (arXiv:1903.12033v1  [cs.NI])|<p>To  properly  validate  wireless  networking  solutions  we  depend  onexperimentation.  Simulation  very  often  produces  less  accurate  results  due  tothe  use  of  models  that  are  simplifications  of  the  real  phenomena  they  try  tomodel.  Networking  experimentation  may  offer  limited  repeatability  andreproducibility.  Being  influenced  by  external  random  phenomena  such  as  noise,interference,  and  multipath,  real  experiments  are  hardly  repeatable.  Inaddition,  they  are  difficult  to  reproduce  due  to  testbed  operationalconstraints  and  availability.  Without  repeatability  and  reproducibility,  thevalidation  of  the  networking  solution  under  evaluation  is  questionable.  In  thispaper,  we  show  how  the  Trace-based  Simulation  (TS)  approach  can  be  used  toaccurately  repeat  and  reproduce  real  experiments  and,  consequently,  introduce  aparadigm  shift  when  it  comes  to  the  evaluation  of  wireless  networkingsolutions.  We  present  an  extensive  evaluation  of  the  TS  approach  using  theFed4FIRE+  w-iLab.2  testbed.  The  results  show  that  it  is  possible  to  repeat  andreproduce  real  experiments  using  ns-3  trace-based  simulations  with  moreaccuracy  than  in  pure  simulation,  with  average  accuracy  gains  above  50%.</p>
http://arxiv.org/abs/1903.12041|The  Geography  of  Pok'emon  GO:  Beneficial  and  Problematic  Effects  on  Places  and  Movement.  (arXiv:1903.12041v1  [cs.HC])|<p>The  widespread  popularity  of  Pok'emon  GO  presents  the  first  opportunity  toobserve  the  geographic  effects  of  location-based  gaming  at  scale.  This  paperreports  the  results  of  a  mixed  methods  study  of  the  geography  of  Pok'emon  GOthat  includes  a  five-country  field  survey  of  375  Pok'emon  GO  players  and  alarge  scale  geostatistical  analysis  of  game  elements.  Focusing  on  the  keygeographic  themes  of  places  and  movement,  we  find  that  the  design  of  Pok'emonGO  reinforces  existing  geographically-linked  biases  (e.g.  the  game  advantagesurban  areas  and  neighborhoods  with  smaller  minority  populations),  thatPok'emon  GO  may  have  instigated  a  relatively  rare  large-scale  shift  in  globalhuman  mobility  patterns,  and  that  Pok'emon  GO  has  geographically-linked  safetyrisks,  but  not  those  typically  emphasized  by  the  media.  Our  results  point  togeographic  design  implications  for  future  systems  in  this  space  such  as  a  meansthrough  which  the  geographic  biases  present  in  Pok'emon  GO  may  becounteracted.</p>
http://arxiv.org/abs/1903.12049|Road  User  Detection  in  Videos.  (arXiv:1903.12049v1  [cs.CV])|<p>Successive  frames  of  a  video  are  highly  redundant,  and  the  most  popularobject  detection  methods  do  not  take  advantage  of  this  fact.  Using  multipleconsecutive  frames  can  improve  detection  of  small  objects  or  difficult  examplesand  can  improve  speed  and  detection  consistency  in  a  video  sequence,  forinstance  by  interpolating  features  between  frames.  In  this  work,  a  novelapproach  is  introduced  to  perform  online  video  object  detection  using  twoconsecutive  frames  of  video  sequences  involving  road  users.  Two  new  models,RetinaNet-Double  and  RetinaNet-Flow,  are  proposed,  based  respectively  on  theconcatenation  of  a  target  frame  with  a  preceding  frame,  and  the  concatenationof  the  optical  flow  with  the  target  frame.  The  models  are  trained  and  evaluatedon  three  public  datasets.  Experiments  show  that  using  a  preceding  frameimproves  performance  over  single  frame  detectors,  but  using  explicit  opticalflow  usually  does  not.</p>
http://arxiv.org/abs/1903.12050|Finding  a  planted  clique  by  adaptive  probing.  (arXiv:1903.12050v1  [math.CO])|<p>We  consider  a  variant  of  the  planted  clique  problem  where  we  are  allowedunbounded  computational  time  but  can  only  investigate  a  small  part  of  the  graphby  adaptive  edge  queries.  We  determine  (up  to  logarithmic  factors)  the  numberof  queries  necessary  both  for  detecting  the  presence  of  a  planted  clique  andfor  finding  the  planted  clique.</p><p>Specifically,  let  $G  sim  G(n,1/2,k)$  be  a  random  graph  on  $n$  vertices  witha  planted  clique  of  size  $k$.  We  show  that  no  algorithm  that  makes  at  most  $q  =o(n^2  /  k^2  +  n)$  adaptive  queries  to  the  adjacency  matrix  of  $G$  is  likely  tofind  the  planted  clique.  On  the  other  hand,  when  $k  geq  (2+epsilon)  log_2  n$there  exists  a  simple  algorithm  (with  unbounded  computational  power)  that  findsthe  planted  clique  with  high  probability  by  making  $q  =  O(  (n^2  /  k^2)  log^2  n+  n  log  n)$  adaptive  queries.  For  detection,  the  additive  $n$  term  is  notnecessary:  the  number  of  queries  needed  to  detect  the  presence  of  a  plantedclique  is  $n^2  /  k^2$  (up  to  logarithmic  factors).</p>
http://arxiv.org/abs/1903.12058|Deep  Neural  Network  Embedding  Learning  with  High-Order  Statistics  for  Text-Independent  Speaker  Verification.  (arXiv:1903.12058v1  [eess.AS])|<p>The  x-vector  based  deep  neural  network  (DNN)  embedding  systems  havedemonstrated  effectiveness  for  text-independent  speaker  verification.  Thispaper  presents  a  multi-task  learning  architecture  for  training  the  speakerembedding  DNN,  with  the  primary  task  of  classifying  the  target  speakers  and  theauxiliary  task  of  reconstructing  the  higher-order  statistics  of  the  originalinput  utterance.  The  proposed  training  strategy  aggregates  both  the  supervisedand  unsupervised  learning  into  one  framework  to  make  the  speaker  embeddingsmore  discriminative  and  robust.  Experiments  are  carried  out  in  the  NIST  SRE16evaluation  dataset  and  the  VOiCES  dataset.  The  results  demonstrate  that  ourproposed  method  outperform  the  original  x-vector  approach  with  very  lowadditional  complexity  added.</p>
http://arxiv.org/abs/1903.12060|Penobscot  Dataset:  Fostering  Machine  Learning  Development  for  Seismic  Interpretation.  (arXiv:1903.12060v1  [physics.geo-ph])|<p>We  have  seen  in  the  past  years  the  flourishing  of  machine  and  deep  learningalgorithms  in  several  applications  such  as  image  classification  andsegmentation,  object  detection  and  recognition,  among  many  others.  This  wasonly  possible,  in  part,  because  datasets  like  ImageNet  --  with  +14  millionlabeled  images  --  were  created  and  made  publicly  available,  providingresearches  with  a  common  ground  to  compare  their  advances  and  extend  thestate-of-the-art.  Although  we  have  seen  an  increasing  interest  in  machinelearning  in  geosciences  as  well,  we  will  only  be  able  to  achieve  a  significantimpact  in  our  community  if  we  collaborate  to  build  such  a  common  basis.  This  iseven  more  difficult  when  it  comes  to  the  Oil&amp;Gas  industry,  in  whichconfidentiality  and  commercial  interests  often  hinder  the  sharing  of  datasetswith  others.  In  this  letter,  we  present  the  Penobscot  interpretation  dataset,our  contribution  to  the  development  of  machine  learning  in  geosciences,  morespecifically  in  seismic  interpretation.  The  Penobscot  3D  seismic  dataset  wasacquired  in  the  Scotian  shelf,  offshore  Nova  Scotia,  Canada.  The  data  ispublicly  available  and  comprises  pre-  and  pos-stack  data,  5  horizons  and  welllogs  of  2  wells.  However,  for  the  dataset  to  be  of  practical  use  for  our  tasks,we  had  to  reinterpret  the  seismic,  generating  7  horizons  separating  differentseismic  facies  intervals.  The  interpreted  horizons  were  used  to  generated+100,000  labeled  images  for  inlines  and  crosslines.  To  demonstrate  the  utilityof  our  dataset,  results  of  two  experiments  are  presented.</p>
http://arxiv.org/abs/1903.12061|Depth  from  a  polarisation  +  RGB  stereo  pair.  (arXiv:1903.12061v1  [cs.CV])|<p>In  this  paper,  we  propose  a  hybrid  depth  imaging  system  in  which  apolarisation  camera  is  augmented  by  a  second  image  from  a  standard  digitalcamera.  For  this  modest  increase  in  equipment  complexity  over  conventionalshape-from-polarisation,  we  obtain  a  number  of  benefits  that  enable  us  toovercome  longstanding  problems  with  the  polarisation  shape  cue.  The  stereo  cueprovides  a  depth  map  which,  although  coarse,  is  metrically  accurate.  This  isused  as  a  guide  surface  for  disambiguation  of  the  polarisation  surface  normalestimates  using  a  higher  order  graphical  model.  In  turn,  these  are  used  toestimate  diffuse  albedo.  By  extending  a  previous  shape-from-polarisation  methodto  the  perspective  case,  we  show  how  to  compute  dense,  detailed  maps  ofabsolute  depth,  while  retaining  a  linear  formulation.  We  show  that  our  hybridmethod  is  able  to  recover  dense  3D  geometry  that  is  superior  tostate-of-the-art  shape-from-polarisation  or  two  view  stereo  alone.</p>
http://arxiv.org/abs/1903.12063|Robust,  fast  and  accurate:  a  3-step  method  for  automatic  histological  image  registration.  (arXiv:1903.12063v1  [cs.CV])|<p>We  present  a  3-step  registration  pipeline  for  differently  stainedhistological  serial  sections  that  consists  of  1)  a  robust  pre-alignment,  2)  aparametric  registration  computed  on  coarse  resolution  images,  and  3)  anaccurate  nonlinear  registration.  In  all  three  steps  the  NGF  distance  measure  isminimized  with  respect  to  an  increasingly  flexible  transformation.  We  apply  themethod  in  the  ANHIR  image  registration  challenge  and  evaluate  its  performanceon  the  training  data.  The  presented  method  is  robust  (error  reduction  in  99.6%of  the  cases),  fast  (runtime  &lt;  4  seconds)  and  accurate  (median  relative  targetregistration  error  0.0019).</p>
http://arxiv.org/abs/1903.12064|Data4UrbanMobility:  Towards  Holistic  Data  Analytics  for  Mobility  Applications  in  Urban  Regions.  (arXiv:1903.12064v1  [cs.CY])|<p>With  the  increasing  availability  of  mobility-related  data,  such  asGPS-traces,  Web  queries  and  climate  conditions,  there  is  a  growing  demand  toutilize  this  data  to  better  understand  and  support  urban  mobility  needs.However,  data  available  from  the  individual  actors,  such  as  providers  ofinformation,  navigation  and  transportation  systems,  is  mostly  restricted  toisolated  mobility  modes,  whereas  holistic  data  analytics  over  integrated  datasources  is  not  sufficiently  supported.  In  this  paper  we  present  our  ongoingresearch  in  the  context  of  holistic  data  analytics  to  support  urban  mobilityapplications  in  the  Data4UrbanMobility  (D4UM)  project.  First,  we  discusschallenges  in  urban  mobility  analytics  and  present  the  D4UM  platform  we  arecurrently  developing  to  facilitate  holistic  urban  data  analytics  overintegrated  heterogeneous  data  sources  along  with  the  available  data  sources.Second,  we  present  the  MiC  app  -  a  tool  we  developed  to  complement  availabledatasets  with  intermodal  mobility  data  (i.e.  data  about  journeys  that  involvemore  than  one  mode  of  mobility)  using  a  citizen  science  approach.  Finally,  wepresent  selected  use  cases  and  discuss  our  future  work.</p>
http://arxiv.org/abs/1903.12065|Optimal  Random  Sampling  from  Distributed  Streams  Revisited.  (arXiv:1903.12065v1  [cs.DC])|<p>We  give  an  improved  algorithm  for  drawing  a  random  sample  from  a  large  datastream  when  the  input  elements  are  distributed  across  multiple  sites  whichcommunicate  via  a  central  coordinator.  At  any  point  in  time  the  set  of  elementsheld  by  the  coordinator  represent  a  uniform  random  sample  from  the  set  of  allthe  elements  observed  so  far.  When  compared  with  prior  work,  our  algorithmsasymptotically  improve  the  total  number  of  messages  sent  in  the  system  as  wellas  the  computation  required  of  the  coordinator.  We  also  present  a  matchinglower  bound,  showing  that  our  protocol  sends  the  optimal  number  of  messages  upto  a  constant  factor  with  large  probability.  As  a  byproduct,  we  obtain  animproved  algorithm  for  finding  the  heavy  hitters  across  multiple  distributedsites.</p>
http://arxiv.org/abs/1903.12069|The  Virtual  Doctor:  An  Interactive  Artificial  Intelligence  based  on  Deep  Learning  for  Non-Invasive  Prediction  of  Diabetes.  (arXiv:1903.12069v1  [cs.CY])|<p>Artificial  intelligence  (AI)  will  pave  the  way  to  a  new  era  in  medicine.However,  currently  available  AI  systems  do  not  interact  with  a  patient,  e.g.,for  anamnesis,  and  thus  are  only  used  by  the  physicians  for  predictions  indiagnosis  or  prognosis.  However,  these  systems  are  widely  used,  e.g.,  indiabetes  or  cancer  prediction.  In  the  current  study,  we  developed  an  AI  that  isable  to  interact  with  a  patient  (virtual  doctor)  by  using  a  speech  recognitionand  speech  synthesis  system  and  thus  can  autonomously  interact  with  thepatient,  which  is  particularly  important  for,  e.g.,  rural  areas,  where  theavailability  of  primary  medical  care  is  strongly  limited  by  low  populationdensities.  As  a  proof-of-concept,  the  system  is  able  to  predict  type  2  diabetesmellitus  (T2DM)  based  on  non-invasive  sensors  and  deep  neural  networks.Moreover,  the  system  provides  an  easy-to-interpret  probability  estimation  forT2DM  for  a  given  patient.  Besides  the  development  of  the  AI,  we  furtheranalyzed  the  acceptance  of  young  people  for  AI  in  healthcare  to  estimate  theimpact  of  such  system  in  the  future.</p>
http://arxiv.org/abs/1903.12070|Comprehensive  Analysis  of  Dynamic  Message  Sign  Impact  on  Driver  Behavior:  A  Random  Forest  Approach.  (arXiv:1903.12070v1  [cs.CY])|<p>This  study  investigates  the  potential  effects  of  different  Dynamic  MessageSigns  (DMSs)  on  driver  behavior  using  a  full-scale  high-fidelity  drivingsimulator.  Different  DMSs  are  categorized  by  their  content,  structure,  and  typeof  messages.  A  random  forest  algorithm  is  used  for  three  separate  behavioralanalyses;  a  route  diversion  analysis,  a  route  choice  analysis  and  a  complianceanalysis;  to  identify  the  potential  and  relative  influences  of  different  DMSson  these  aspects  of  driver  behavior.  A  total  of  390  simulation  runs  areconducted  using  a  sample  of  65  participants  from  diverse  socioeconomicbackgrounds.  Results  obtained  suggest  that  DMSs  displaying  lane  closure  anddelay  information  with  advisory  messages  are  most  influential  with  regards  todiversion  while  color-coded  DMSs  and  DMSs  with  avoid  route  advice  are  the  topcontributors  impacting  route  choice  decisions  and  DMS  compliance.  In  thisfirst-of-a-kind  study,  based  on  the  responses  to  the  pre  and  post  simulationsurveys  as  well  as  results  obtained  from  the  analysis  ofdriving-simulation-session  data,  the  authors  found  that  color-blind-friendly,color-coded  DMSs  are  more  effective  than  alphanumeric  DMSs  -  especially  inscenarios  that  demand  high  compliance  from  drivers.  The  increased  effectivenessmay  be  attributed  to  reduced  comprehension  time  and  ease  with  which  such  DMSsare  understood  by  a  greater  percentage  of  road  users.</p>
http://arxiv.org/abs/1903.12071|Big  Data  Analytics  and  AI  in  Mental  Healthcare.  (arXiv:1903.12071v1  [cs.CY])|<p>Mental  health  conditions  cause  a  great  deal  of  distress  or  impairment;depression  alone  will  affect  11%  of  the  world's  population.  The  application  ofArtificial  Intelligence  (AI)  and  big-data  technologies  to  mental  health  hasgreat  potential  for  personalizing  treatment  selection,  prognosticating,monitoring  for  relapse,  detecting  and  helping  to  prevent  mental  healthconditions  before  they  reach  clinical-level  symptomatology,  and  even  deliveringsome  treatments.  However,  unlike  similar  applications  in  other  fields  ofmedicine,  there  are  several  unique  challenges  in  mental  health  applicationswhich  currently  pose  barriers  towards  the  implementation  of  these  technologies.Specifically,  there  are  very  few  widely  used  or  validated  biomarkers  in  mentalhealth,  leading  to  a  heavy  reliance  on  patient  and  clinician  derivedquestionnaire  data  as  well  as  interpretation  of  new  signals  such  as  digitalphenotyping.  In  addition,  diagnosis  also  lacks  the  same  objective  'goldstandard'  as  in  other  conditions  such  as  oncology,  where  clinicians  andresearchers  can  often  rely  on  pathological  analysis  for  confirmation  ofdiagnosis.  In  this  chapter  we  discuss  the  major  opportunities,  limitations  andtechniques  used  for  improving  mental  healthcare  through  AI  and  big-data.  Weexplore  both  the  computational,  clinical  and  ethical  considerations  and  bestpractices  as  well  as  lay  out  the  major  researcher  directions  for  the  nearfuture.</p>
http://arxiv.org/abs/1903.12073|Scope  of  Research  on  Particle  Swarm  Optimization  Based  Data  Clustering.  (arXiv:1903.12073v1  [cs.NE])|<p>Optimization  is  nothing  but  a  mathematical  technique  which  finds  maxima  orminima  of  any  function  of  concern  in  some  realistic  region.  Differentoptimization  techniques  are  proposed  which  are  competing  for  the  best  solution.Particle  Swarm  Optimization  (PSO)  is  a  new,  advanced,  and  most  powerfuloptimization  methodology  that  performs  empirically  well  on  several  optimizationproblems.  It  is  the  extensively  used  Swarm  Intelligence  (SI)  inspiredoptimization  algorithm  used  for  finding  the  global  optimal  solution  in  amultifaceted  search  region.  Data  clustering  is  one  of  the  challenging  realworld  applications  that  invite  the  eminent  research  works  in  variety  of  fields.Applicability  of  different  PSO  variants  to  data  clustering  is  studied  in  theliterature,  and  the  analyzed  research  work  shows  that,  PSO  variants  give  poorresults  for  multidimensional  data.  This  paper  describes  the  differentchallenges  associated  with  multidimensional  data  clustering  and  scope  ofresearch  on  optimizing  the  clustering  problems  using  PSO.  We  also  propose  astrategy  to  use  hybrid  PSO  variant  for  clustering  multidimensional  numerical,text  and  image  data.</p>
http://arxiv.org/abs/1903.12074|Interpretation  of  machine  learning  predictions  for  patient  outcomes  in  electronic  health  records.  (arXiv:1903.12074v1  [cs.CY])|<p>Electronic  health  records  are  an  increasingly  important  resource  forunderstanding  the  interactions  between  patient  health,  environment,  andclinical  decisions.  In  this  paper  we  report  an  empirical  study  of  predictivemodeling  of  several  patient  outcomes  using  three  state-of-the-art  machinelearning  methods.  Our  primary  goal  is  to  validate  the  models  by  interpretingthe  importance  of  predictors  in  the  final  models.  Central  to  interpretation  isthe  use  of  feature  importance  scores,  which  vary  depending  on  the  underlyingmethodology.  In  order  to  assess  feature  importance,  we  compared  univariatestatistical  tests,  information-theoretic  measures,  permutation  testing,  andnormalized  coefficients  from  multivariate  logistic  regression  models.  Ingeneral  we  found  poor  correlation  between  methods  in  their  assessment  offeature  importance,  even  when  their  performance  is  comparable  and  relativelygood.  However,  permutation  tests  applied  to  random  forest  and  gradient  boostingmodels  showed  the  most  agreement,  and  the  importance  scores  matched  theclinical  interpretation  most  frequently.</p>
http://arxiv.org/abs/1903.12075|An  Empirical  Exploration  on  the  Supervision  of  PhD  Students  Closely  Collaborating  with  Industry.  (arXiv:1903.12075v1  [cs.CY])|<p>With  an  increase  of  PhD  students  working  in  industry,  there  is  a  need  tounderstand  what  factors  are  influencing  supervision  for  industrial  students.This  paper  aims  at  exploring  the  challenges  and  good  approaches  to  supervisionof  industrial  PhD  students.  Data  was  collected  through  semi-structuredinterviews  of  six  PhD  students  and  supervisors  with  experience  in  PhD  studiesat  several  organizations  in  the  embedded  software  industry  in  Sweden.  The  datawas  anonymized  and  it  was  analyzed  by  means  of  thematic  analysis.  The  resultsindicate  that  there  are  many  challenges  and  opportunities  to  improve  thesupervision  of  industrial  PhD  students.</p>
http://arxiv.org/abs/1903.12076|Toward  a  fitness  landscape  model  of  firms'  IT-enabled  dynamic  capabilities.  (arXiv:1903.12076v1  [cs.CY])|<p>This  chapter  presents,  extends  and  integrates  a  complexity  scienceperspective  and  applies  this  to  IT-enabled  dynamic  capabilities  (ITDCs)  offirms.  By  doing  so,  this  chapter  leverages  statistical  survey  data  and  usesthem  as  parameters  for  a  simulation  using  the  NK-model.  This  NK-model  createsstochastically  generated  fitness  landscapes  that  are  parameterized  using  afinite  number  of  (N)  elements,  or  capabilities,  and  (K)  complex  interactionsbetween  those  capabilities,  and  studies  the  performance  (fitness)  of  systems.We  simulate  firm  efforts  to  adaptively  explore  and  walk  through  a  fitnesslandscape  of  possible  strategies  of  inter-related  capabilities  to  reach  towardhigher  levels  of  fitness  of  ITDCs.  Also,  our  fitness  landscape  model  providesrealistic  scenarios  with  a  nexus  of  possible  business  strategies  that  can  beemployed  considering  the  current  status,  interdependency,  and  alignment  amongcapabilities  in  the  organization.  Our  work  suggests  that  firms  achieve  thehighest  fitness  values  when  the  interdependency  among  the  individualcapabilities  is  relatively  small.</p>
http://arxiv.org/abs/1903.12079|Deterrence  and  Prevention-based  Model  to  Mitigate  Information  Security  Insider  Threats  in  Organisations.  (arXiv:1903.12079v1  [cs.CY])|<p>Previous  studies  show  that  information  security  breaches  and  privacyviolations  are  important  issues  for  organisations  and  people.  It  isacknowledged  that  decreasing  the  risk  in  this  domain  requires  consideration  ofthe  technological  aspects  of  information  security  alongside  human  aspects.Employees  intentionally  or  unintentionally  account  for  a  significant  portion  ofthe  threats  to  information  assets  in  organisations.  This  research  presents  anovel  conceptual  framework  to  mitigate  the  risk  of  insiders  using  deterrenceand  prevention  approaches.  Deterrence  factors  discourage  employees  fromengaging  in  information  security  misbehaviour  in  organisations,  and  situationalcrime  prevention  factors  encourage  them  to  prevent  information  securitymisconduct.  Our  findings  show  that  perceived  sanctions  certainty  and  severitysignificantly  influence  individuals'  attitudes  and  deter  them  from  informationsecurity  misconduct.  In  addition,  the  output  revealed  that  increasing  theeffort,  risk  and  reducing  the  reward  (benefits  of  crime)  influence  theemployees'  attitudes  towards  prevent  information  security  misbehaviour.However,  removing  excuses  and  reducing  provocations  do  not  significantlyinfluence  individuals'  attitudes  towards  prevent  information  securitymisconduct.  Finally,  the  output  of  the  data  analysis  also  showed  thatsubjective  norms,  perceived  behavioural  control  and  attitude  influenceindividuals'  intentions,  and,  ultimately,  their  behaviour  towards  avoidinginformation  security  misbehaviour.</p>
http://arxiv.org/abs/1903.12080|Detecting  Activities  of  Daily  Living  and  Routine  Behaviours  in  Dementia  Patients  Living  Alone  Using  Smart  Meter  Load  Disaggregation.  (arXiv:1903.12080v1  [cs.CY])|<p>The  emergence  of  an  ageing  population  is  a  significant  public  health  concern.This  has  led  to  an  increase  in  the  number  of  people  living  with  progressiveneurodegenerative  disorders  like  dementia.  Consequently,  the  strain  this  isplaces  on  health  and  social  care  services  means  providing  24-hour  monitoring  isnot  sustainable.  Technological  intervention  is  being  considered,  however  nosolution  exists  to  non-intrusively  monitor  the  independent  living  needs  ofpatients  with  dementia.  As  a  result  many  patients  hit  crisis  point  beforeintervention  and  support  is  provided.  In  parallel,  patient  care  relies  onfeedback  from  informal  carers  about  significant  behavioural  changes.  Yet,  notall  people  have  a  social  support  network  and  early  intervention  in  dementiacare  is  often  missed.  The  smart  meter  rollout  has  the  potential  to  change  this.Using  machine  learning  and  signal  processing  techniques,  a  home  energy  supplycan  be  disaggregated  to  detect  which  home  appliances  are  turned  on  and  off.This  will  allow  Activities  of  Daily  Living  (ADLs)  to  be  assessed,  such  aseating  and  drinking,  and  observed  changes  in  routine  to  be  detected  for  earlyintervention.  The  primary  aim  is  to  help  reduce  deterioration  and  enablepatients  to  stay  in  their  homes  for  longer.  A  Support  Vector  Machine  (SVM)  andRandom  Decision  Forest  classifier  are  modelled  using  data  from  three  testhomes.  The  trained  models  are  then  used  to  monitor  two  patients  with  dementiaduring  a  six-month  clinical  trial  undertaken  in  partnership  with  Mersey  CareNHS  Foundation  Trust.  In  the  case  of  load  disaggregation  for  appliancedetection,  the  SVM  achieved  (AUC=0.86074,  Sen=0.756  and  Spec=0.92838).  Whilethe  Decision  Forest  achieved  (AUC=0.9429,  Sen=0.9634  and  Spec=0.9634).  ADLs  arealso  analysed  to  identify  the  behavioural  patterns  of  the  occupant  whiledetecting  alterations  in  routine.</p>
http://arxiv.org/abs/1903.12084|Definition  of  Internet  of  Things  (IoT)  Cyber  Risk  Discussion  on  a  Transformation  Roadmap  for  Standardisation  of  Regulations  Risk  Maturity  Strategy  Design  and  Impact  Assessment.  (arXiv:1903.12084v1  [cs.CR])|<p>A  comparative  empirical  analysis  is  performed  to  define  a  high-levelpotential  target  state  followed  by  a  high-level  transformation  roadmap,describing  how  company  can  achieve  their  target  state,  based  on  their  currentstate.  The  transformation  roadmap  is  used  to  adapt  the  Goal-Oriented  Approachand  the  IoT  Micro  Mart  model.</p>
http://arxiv.org/abs/1903.12085|More  Parallelism  in  Dijkstra's  Single-Source  Shortest  Path  Algorithm.  (arXiv:1903.12085v1  [cs.DC])|<p>Dijkstra's  algorithm  for  the  Single-Source  Shortest  Path  (SSSP)  problem  isnotoriously  hard  to  parallelize  in  $o(n)$  depth,  $n$  being  the  number  ofvertices  in  the  input  graph,  without  increasing  the  required  parallel  workunreasonably.  Crauser  et  al.  (1998)  presented  observations  that  allow  toidentify  more  than  a  single  vertex  at  a  time  as  correct  and  correspondinglymore  edges  to  be  relaxed  simultaneously.  Their  algorithm  runs  in  parallelphases,  and  for  certain  random  graphs  they  showed  that  the  number  of  phases  is$O(n^{1/3})$  with  high  probability.  A  work-efficient  CRCW  PRAM  with  this  depthwas  given,  but  no  implementation  on  a  real,  parallel  system.</p><p>In  this  paper  we  strengthen  the  criteria  of  Crauser  et  al.,  and  discusstradeoffs  between  work  and  number  of  phases  in  their  implementation.  We  presentsimulation  results  with  a  range  of  common  input  graphs  for  the  depth  that  anideal  parallel  algorithm  that  can  apply  the  criteria  at  no  cost  and  parallelizerelaxations  without  conflicts  can  achieve.  These  results  show  that  the  numberof  phases  is  indeed  a  small  root  of  $n$,  but  still  off  from  the  shortest  pathlength  lower  bound  that  can  also  be  computed.</p><p>We  give  a  shared-memory  parallel  implementation  of  the  most  work-efficientversion  of  a  Dijkstra's  algorithm  running  in  parallel  phases,  which  we  compareto  an  own  implementation  of  the  well-known  $Delta$-stepping  algorithm.  We  canshow  that  the  work-efficient  SSSP  algorithm  applying  the  criteria  of  Crauser  etal.  is  competitive  to  and  often  better  than  $Delta$-stepping  on  our  choseninput  graphs.  Despite  not  providing  an  $o(n)$  guarantee  on  the  number  ofrequired  phases,  criteria  allowing  concurrent  relaxation  of  many  correctvertices  may  be  a  viable  approach  to  practically  fast,  parallel  SSSPimplementations.</p>
http://arxiv.org/abs/1903.12086|Towards  a  Theory  of  Systems  Engineering  Processes:  A  Principal-Agent  Model  of  a  One-Shot,  Shallow  Process.  (arXiv:1903.12086v1  [cs.MA])|<p>Systems  engineering  processes  coordinate  the  effort  of  different  individualsto  generate  a  product  satisfying  certain  requirements.  As  the  involvedengineers  are  self-interested  agents,  the  goals  at  different  levels  of  thesystems  engineering  hierarchy  may  deviate  from  the  system-level  goals  which  maycause  budget  and  schedule  overruns.  Therefore,  there  is  a  need  of  a  systemsengineering  theory  that  accounts  for  the  human  behavior  in  systems  design.  Tothis  end,  the  objective  of  this  paper  is  to  develop  and  analyze  aprincipal-agent  model  of  a  one-shot  (single  iteration),  shallow  (one  level  ofhierarchy)  systems  engineering  process.  We  assume  that  the  systems  engineermaximizes  the  expected  utility  of  the  system,  while  the  subsystem  engineersseek  to  maximize  their  expected  utilities.  Furthermore,  the  systems  engineer  isunable  to  monitor  the  effort  of  the  subsystem  engineer  and  may  not  have  acomplete  information  about  their  types  or  the  complexity  of  the  design  task.However,  the  systems  engineer  can  incentivize  the  subsystem  engineers  byproposing  specific  contracts.  To  obtain  an  optimal  incentive,  we  pose  and  solvenumerically  a  bi-level  optimization  problem.  Through  extensive  simulations,  westudy  the  optimal  incentives  arising  from  different  system-level  valuefunctions  under  various  combinations  of  effort  costs,  problem-solving  skills,and  task  complexities.</p>
http://arxiv.org/abs/1903.12087|A  Real-Time  Wideband  Neural  Vocoder  at  1.6  kb/s  Using  LPCNet.  (arXiv:1903.12087v1  [eess.AS])|<p>Neural  speech  synthesis  algorithms  are  a  promising  new  approach  for  codingspeech  at  very  low  bitrate.  They  have  so  far  demonstrated  quality  that  farexceeds  traditional  vocoders,  at  the  cost  of  very  high  complexity.  In  thiswork,  we  present  a  low-bitrate  neural  vocoder  based  on  the  LPCNet  model.  Theuse  of  linear  prediction  and  sparse  recurrent  networks  makes  it  possible  toachieve  real-time  operation  on  general-purpose  hardware.  We  demonstrate  thatLPCNet  operating  at  1.6  kb/s  achieves  significantly  higher  quality  than  MELPand  that  uncompressed  LPCNet  can  exceed  the  quality  of  a  waveform  codecoperating  at  low  bitrate.  This  opens  the  way  for  new  codec  designs  based  onneural  synthesis  models.</p>
http://arxiv.org/abs/1903.12088|GANs-NQM:  A  Generative  Adversarial  Networks  based  No  Reference  Quality  Assessment  Metric  for  RGB-D  Synthesized  Views.  (arXiv:1903.12088v1  [cs.MM])|<p>In  this  paper,  we  proposed  a  no-reference  (NR)  quality  metric  for  RGB  plusimage-depth  (RGB-D)  synthesis  images  based  on  Generative  Adversarial  Networks(GANs),  namely  GANs-NQM.  Due  to  the  failure  of  the  inpainting  on  dis-occludedregions  in  RGB-D  synthesis  process,  to  capture  the  non-uniformly  distributedlocal  distortions  and  to  learn  their  impact  on  perceptual  quality  arechallenging  tasks  for  objective  quality  metrics.  In  our  study,  based  on  thecharacteristics  of  GANs,  we  proposed  i)  a  novel  training  strategy  of  GANs  forRGB-D  synthesis  images  using  existing  large-scale  computer  vision  datasetsrather  than  RGB-D  dataset;  ii)  a  referenceless  quality  metric  based  on  thetrained  discriminator  by  learning  a  `Bag  of  Distortion  Word'  (BDW)  codebook  anda  local  distortion  regions  selector;  iii)  a  hole  filling  inpainter,  i.e.,  thegenerator  of  the  trained  GANs,  for  RGB-D  dis-occluded  regions  as  a  sideoutcome.  According  to  the  experimental  results  on  IRCCyN/IVC  DIBR  database,  theproposed  model  outperforms  the  state-of-the-art  quality  metrics,  in  addition,is  more  applicable  in  real  scenarios.  The  corresponding  context  inpainter  alsoshows  appealing  results  over  other  inpainting  algorithms.</p>
http://arxiv.org/abs/1903.12090|Learning  to  Weight  for  Text  Classification.  (arXiv:1903.12090v1  [cs.LG])|<p>In  information  retrieval  (IR)  and  related  tasks,  term  weighting  approachestypically  consider  the  frequency  of  the  term  in  the  document  and  in  thecollection  in  order  to  compute  a  score  reflecting  the  importance  of  the  termfor  the  document.  In  tasks  characterized  by  the  presence  of  training  data  (suchas  text  classification)  it  seems  logical  that  the  term  weighting  functionshould  take  into  account  the  distribution  (as  estimated  from  training  data)  ofthe  term  across  the  classes  of  interest.  Although  `supervised  term  weighting'approaches  that  use  this  intuition  have  been  described  before,  they  have  failedto  show  consistent  improvements.  In  this  article  we  analyse  the  possiblereasons  for  this  failure,  and  call  consolidated  assumptions  into  question.Following  this  criticism  we  propose  a  novel  supervised  term  weighting  approachthat,  instead  of  relying  on  any  predefined  formula,  learns  a  term  weightingfunction  optimised  on  the  training  set  of  interest;  we  dub  this  approachemph{Learning  to  Weight}  (LTW).  The  experiments  that  we  run  on  severalwell-known  benchmarks,  and  using  different  learning  methods,  show  that  ourmethod  outperforms  previous  term  weighting  approaches  in  text  classification.</p>
http://arxiv.org/abs/1903.12091|Nonlinear  Model  Predictive  Control  for  Distributed  Motion  Planning  in  Road  Intersections  Using  PANOC.  (arXiv:1903.12091v1  [cs.SY])|<p>The  coordination  of  highly  automated  vehicles  (or  agents)  in  roadintersections  is  an  inherently  nonconvex  and  challenging  problem.  In  thispaper,  we  propose  a  distributed  motion  planning  scheme  under  reasonablevehicle-to-vehicle  communication  requirements.  Each  agent  solves  a  nonlinearmodel  predictive  control  problem  in  real  time  and  transmits  their  plannedtrajectory  to  other  agents,  which  may  have  conflicting  objectives.  The  problemformulation  is  augmented  with  conditional  constraints  that  enable  the  agents  todecide  whether  to  wait  at  a  stopping  line,  if  crossing  is  not  possible.  Theinvolved  nonconvex  problems  are  solved  very  efficiently  using  the  proximalaveraged  Newton  method  for  optimal  control  (PANOC).  We  demonstrate  theefficiency  of  the  proposed  approach  in  a  realistic  intersection  crossingscenario.</p>
http://arxiv.org/abs/1903.12092|Deep  Neural  Network  Embeddings  with  Gating  Mechanisms  for  Text-Independent  Speaker  Verification.  (arXiv:1903.12092v1  [eess.AS])|<p>In  this  paper,  gating  mechanisms  are  applied  in  deep  neural  network  (DNN)training  for  x-vector-based  text-independent  speaker  verification.  First,  agated  convolution  neural  network  (GCNN)  is  employed  for  modeling  theframe-level  embedding  layers.  Compared  with  the  time-delay  DNN  (TDNN),  the  GCNNcan  obtain  more  expressive  frame-level  representations  through  carefullydesigned  memory  cell  and  gating  mechanisms.  Moreover,  we  propose  a  novelgated-attention  statistics  pooling  strategy  in  which  the  attention  scores  areshared  with  the  output  gate.  The  gated-attention  statistics  pooling  combinesboth  gating  and  attention  mechanisms  into  one  framework;  therefore,  we  cancapture  more  useful  information  in  the  temporal  pooling  layer.  Experiments  arecarried  out  using  the  NIST  SRE16  and  SRE18  evaluation  datasets.  The  resultsdemonstrate  the  effectiveness  of  the  GCNN  and  show  that  the  proposedgated-attention  statistics  pooling  can  further  improve  the  performance.</p>
http://arxiv.org/abs/1903.12094|Barking  up  the  Right  Tree:  Improving  Cross-Corpus  Speech  Emotion  Recognition  with  Adversarial  Discriminative  Domain  Generalization  (ADDoG).  (arXiv:1903.12094v1  [cs.LG])|"<p>Automatic  speech  emotion  recognition  provides  computers  with  critical  contextto  enable  user  understanding.  While  methods  trained  and  tested  within  the  samedataset  have  been  shown  successful,  they  often  fail  when  applied  to  unseendatasets.  To  address  this,  recent  work  has  focused  on  adversarial  methods  tofind  more  generalized  representations  of  emotional  speech.  However,  many  ofthese  methods  have  issues  converging,  and  only  involve  datasets  collected  inlaboratory  conditions.  In  this  paper,  we  introduce  Adversarial  DiscriminativeDomain  Generalization  (ADDoG),  which  follows  an  easier  to  train  ""meet  in  themiddle""  approach.  The  model  iteratively  moves  representations  learned  for  eachdataset  closer  to  one  another,  improving  cross-dataset  generalization.  We  alsointroduce  Multiclass  ADDoG,  or  MADDoG,  which  is  able  to  extend  the  proposedmethod  to  more  than  two  datasets,  simultaneously.  Our  results  show  consistentconvergence  for  the  introduced  methods,  with  significantly  improved  resultswhen  not  using  labels  from  the  target  dataset.  We  also  show  how,  in  most  cases,ADDoG  and  MADDoG  can  be  used  to  improve  upon  baseline  state-of-the-art  methodswhen  target  dataset  labels  are  added  and  in-the-wild  data  are  considered.  Eventhough  our  experiments  focus  on  cross-corpus  speech  emotion,  these  methodscould  be  used  to  remove  unwanted  factors  of  variation  in  other  settings.</p>"
http://arxiv.org/abs/1903.12099|Recommendation  Systems  for  Tourism  Based  on  Social  Networks:  A  Survey.  (arXiv:1903.12099v1  [cs.IR])|<p>Nowadays,  recommender  systems  are  present  in  many  daily  activities  such  asonline  shopping,  browsing  social  networks,  etc.  Given  the  rising  demand  forreinvigoration  of  the  tourist  industry  through  information  technology,recommenders  have  been  included  into  tourism  websites  such  as  Expedia,  Bookingor  Tripadvisor,  among  others.  Furthermore,  the  amount  of  scientific  papersrelated  to  recommender  systems  for  tourism  is  on  solid  and  continuous  growthsince  2004.  Much  of  this  growth  is  due  to  social  networks  that,  besides  tooffer  researchers  the  possibility  of  using  a  great  mass  of  available  andconstantly  updated  data,  they  also  enable  the  recommendation  systems  to  becomemore  personalised,  effective  and  natural.  This  paper  reviews  and  analyses  manyresearch  publications  focusing  on  tourism  recommender  systems  that  use  socialnetworks  in  their  projects.  We  detail  their  main  characteristics,  like  whichsocial  networks  are  exploited,  which  data  is  extracted,  the  appliedrecommendation  techniques,  the  methods  of  evaluation,  etc.  Through  acomprehensive  literature  review,  we  aim  to  collaborate  with  the  futurerecommender  systems,  by  giving  some  clear  classifications  and  descriptions  ofthe  current  tourism  recommender  systems.</p>
http://arxiv.org/abs/1903.12101|Extending  Signature-based  Intrusion  Detection  Systems  WithBayesian  Abductive  Reasoning.  (arXiv:1903.12101v1  [cs.CR])|<p>Evolving  cybersecurity  threats  are  a  persistent  challenge  forsystemadministrators  and  security  experts  as  new  malwares  are  continu-allyreleased.  Attackers  may  look  for  vulnerabilities  in  commercialproducts  orexecute  sophisticated  reconnaissance  campaigns  tounderstand  a  targets  networkand  gather  information  on  securityproducts  like  firewalls  and  intrusiondetection  /  prevention  systems(network  or  host-based).  Many  new  attacks  tend  tobe  modificationsof  existing  ones.  In  such  a  scenario,  rule-based  systems  failto  detectthe  attack,  even  though  there  are  minor  differences  in  conditions/attributes  between  rules  to  identify  the  new  and  existing  attack.  Todetectthese  differences  the  IDS  must  be  able  to  isolate  the  subset  ofconditions  thatare  true  and  predict  the  likely  conditions  (differentfrom  the  original)  thatmust  be  observed.  In  this  paper,  we  proposeaprobabilistic  abductivereasoningapproach  that  augments  an  exist-ing  rule-based  IDS  (snort  [29])  todetect  these  evolved  attacks  by  (a)Predicting  rule  conditions  that  are  likelyto  occur  (based  on  existingrules)  and  (b)  able  to  generate  new  snort  rules  whenprovided  withseed  rule  (i.e.  a  starting  rule)  to  reduce  the  burden  on  expertstoconstantly  update  them.  We  demonstrate  the  effectiveness  of  theapproach  bygenerating  new  rules  from  the  snort  2012  rules  set  andtesting  it  on  the  MACCDC2012  dataset  [6].</p>
http://arxiv.org/abs/1903.12107|Quality  Assessment  of  Free-viewpoint  Videos  by  Quantifying  the  Elastic  Changes  of  Multi-Scale  Motion  Trajectories.  (arXiv:1903.12107v1  [cs.MM])|<p>Virtual  viewpoints  synthesis  is  an  essential  process  for  many  immersiveapplications  including  Free-viewpoint  TV  (FTV).  A  widely  used  technique  forviewpoints  synthesis  is  Depth-Image-Based-Rendering  (DIBR)  technique.  However,such  techniques  may  introduce  challenging  non-uniform  spatial-temporalstructure-related  distortions.  Most  of  the  existing  state-of-the-art  qualitymetrics  fail  to  handle  these  distortions,  especially  the  temporal  structureinconsistencies  observed  during  the  switch  of  different  viewpoints.  To  tacklethis  problem,  an  elastic  metric  and  multi-scale  trajectory  based  video  qualitymetric  (EM-VQM)  is  proposed  in  this  paper.  Dense  motion  trajectory  is  firstused  as  a  proxy  for  selecting  temporal  sensitive  regions,  where  local  geometricdistortions  might  significantly  diminish  the  perceived  quality.  Afterwards,  theamount  of  temporal  structure  inconsistencies  and  unsmooth  viewpointstransitions  are  quantified  by  calculating  1)  the  amount  of  motion  trajectorydeformations  with  elastic  metric  and,  2)  the  spatial-temporal  structuraldissimilarity.  According  to  the  comprehensive  experimental  results  on  two  FTVvideo  datasets,  the  proposed  metric  outperforms  the  state-of-the-art  metricsdesigned  for  free-viewpoint  videos  significantly  and  achieves  a  gain  of  12.86%and  16.75%  in  terms  of  median  Pearson  linear  correlation  coefficient  values  onthe  two  datasets  compared  to  the  best  one,  respectively.</p>
http://arxiv.org/abs/1903.12110|Building  Automated  Survey  Coders  via  Interactive  Machine  Learning.  (arXiv:1903.12110v1  [cs.IR])|"<p>Software  systems  trained  via  machine  learning  to  automatically  classifyopen-ended  answers  (a.k.a.  verbatims)  are  by  now  a  reality.  Still,  theiradoption  in  the  survey  coding  industry  has  been  less  widespread  than  it  mighthave  been.  Among  the  factors  that  have  hindered  a  more  massive  takeup  of  thistechnology  are  the  effort  involved  in  manually  coding  a  sufficient  amount  oftraining  data,  the  fact  that  small  studies  do  not  seem  to  justify  this  effort,and  the  fact  that  the  process  needs  to  be  repeated  anew  when  brand  new  codingtasks  arise.  In  this  paper  we  will  argue  for  an  approach  to  building  verbatimclassifiers  that  we  will  call  ""Interactive  Learning"",  and  that  addresses  allthe  above  problems.  We  will  show  that,  for  the  same  amount  of  training  effort,interactive  learning  delivers  much  better  coding  accuracy  than  standard""non-interactive""  learning.  This  is  especially  true  when  the  amount  of  data  weare  willing  to  manually  code  is  small,  which  makes  this  approach  attractivealso  for  small-scale  studies.  Interactive  learning  also  lends  itself  to  reusingpreviously  trained  classifiers  for  dealing  with  new  (albeit  related)  codingtasks.  Interactive  learning  also  integrates  better  in  the  daily  workflow  of  thesurvey  specialist,  and  delivers  a  better  user  experience  overall.</p>"
http://arxiv.org/abs/1903.12113|A  Counterexample-guided  Approach  to  Finding  Numerical  Invariants.  (arXiv:1903.12113v1  [cs.SE])|<p>Numerical  invariants,  e.g.,  relationships  among  numerical  variables  in  aprogram,  represent  a  useful  class  of  properties  to  analyze  programs.  Generalpolynomial  invariants  represent  more  complex  numerical  relations,  but  they  areoften  required  in  many  scientific  and  engineering  applications.  We  presentNumInv,  a  tool  that  implements  a  counterexample-guided  invariant  generation(CEGIR)  technique  to  automatically  discover  numerical  invariants,  which  arepolynomial  equality  and  inequality  relations  among  numerical  variables.  ThisCEGIR  technique  infers  candidate  invariants  from  program  traces  and  then  checksthem  against  the  program  source  code  using  the  KLEE  test-input  generation  tool.If  the  invariants  are  incorrect  KLEE  returns  counterexample  traces,  which  helpthe  dynamic  inference  obtain  better  results.  Existing  CEGIR  approaches  oftenrequire  sound  invariants,  however  NumInv  sacrifices  soundness  and  producesresults  that  KLEE  cannot  refute  within  certain  time  bounds.  This  design  and  theuse  of  KLEE  as  a  verifier  allow  NumInv  to  discover  useful  and  importantnumerical  invariants  for  many  challenging  programs.</p><p>Preliminary  results  show  that  NumInv  generates  required  invariants  forunderstanding  and  verifying  correctness  of  programs  involving  complexarithmetic.  We  also  show  that  NumInv  discovers  polynomial  invariants  thatcapture  precise  complexity  bounds  of  programs  used  to  benchmark  existing  staticcomplexity  analysis  techniques.  Finally,  we  show  that  NumInv  performscompetitively  comparing  to  state  of  the  art  numerical  invariant  analysis  tools.</p>
http://arxiv.org/abs/1903.12117|Many  Task  Learning  with  Task  Routing.  (arXiv:1903.12117v1  [cs.CV])|<p>Typical  multi-task  learning  (MTL)  methods  rely  on  architectural  adjustmentsand  a  large  trainable  parameter  set  to  jointly  optimize  over  several  tasks.However,  when  the  number  of  tasks  increases  so  do  the  complexity  of  thearchitectural  adjustments  and  resource  requirements.  In  this  paper,  weintroduce  a  method  which  applies  a  conditional  feature-wise  transformation  overthe  convolutional  activations  that  enables  a  model  to  successfully  perform  alarge  number  of  tasks.  To  distinguish  from  regular  MTL,  we  introduce  Many  TaskLearning  (MaTL)  as  a  special  case  of  MTL  where  more  than  20  tasks  are  performedby  a  single  model.  Our  method  dubbed  Task  Routing  (TR)  is  encapsulated  in  alayer  we  call  the  Task  Routing  Layer  (TRL),  which  applied  in  an  MaTL  scenariosuccessfully  fits  hundreds  of  classification  tasks  in  one  model.  We  evaluateour  method  on  5  datasets  against  strong  baselines  and  state-of-the-artapproaches.</p>
http://arxiv.org/abs/1903.12118|From  Motions  to  Emotions:  Can  the  Fundamental  Emotions  be  Expressed  in  a  Robot  Swarm?.  (arXiv:1903.12118v1  [cs.RO])|<p>This  paper  explores  the  expressive  capabilities  of  a  swarm  of  miniaturemobile  robots  within  the  context  of  inter-robot  interactions  and  their  mappingto  the  so-called  fundamental  emotions.  In  particular,  we  investigate  how  motionand  shape  descriptors  that  are  psychologically  associated  with  differentemotions  can  be  incorporated  into  different  swarm  behaviors  for  the  purpose  ofartistic  expositions.  Based  on  these  characterizations  from  social  psychology,a  set  of  swarm  behaviors  is  created,  where  each  behavior  corresponds  to  afundamental  emotion.  The  effectiveness  of  these  behaviors  was  evaluated  in  asurvey  in  which  the  participants  were  asked  to  associate  different  swarmbehaviors  with  the  fundamental  emotions.  The  results  of  the  survey  show  thatmost  of  the  research  participants  assigned  to  each  video  the  emotion  intendedto  be  portrayed  by  design.  These  results  confirm  that  abstract  descriptorsassociated  with  the  different  fundamental  emotions  in  social  psychology  provideuseful  motion  characterizations  that  can  be  effectively  transformed  intoexpressive  behaviors  for  a  swarm  of  simple  ground  mobile  robots.</p>
http://arxiv.org/abs/1903.12125|Nearest-Neighbor  Neural  Networks  for  Geostatistics.  (arXiv:1903.12125v1  [stat.ML])|<p>Kriging  is  the  predominant  method  used  for  spatial  prediction,  but  relies  onthe  assumption  that  predictions  are  linear  combinations  of  the  observations.Kriging  often  also  relies  on  additional  assumptions  such  as  normality  andstationarity.  We  propose  a  more  flexible  spatial  prediction  method  based  on  theNearest-Neighbor  Neural  Network  (4N)  process  that  embeds  deep  learning  into  ageostatistical  model.  We  show  that  the  4N  process  is  a  valid  stochastic  processand  propose  a  series  of  new  ways  to  construct  features  to  be  used  as  inputs  tothe  deep  learning  model  based  on  neighboring  information.  Our  model  frameworkoutperforms  some  existing  state-of-art  geostatistical  modelling  methods  forsimulated  non-Gaussian  data  and  is  applied  to  a  massive  forestry  dataset.</p>
http://arxiv.org/abs/1903.12127|Using  Latent  Class  Analysis  to  Identify  ARDS  Sub-phenotypes  for  Enhanced  Machine  Learning  Predictive  Performance.  (arXiv:1903.12127v1  [cs.LG])|<p>In  this  work,  we  utilize  Machine  Learning  for  early  recognition  of  patientsat  high  risk  of  acute  respiratory  distress  syndrome  (ARDS),  which  is  criticalfor  successful  prevention  strategies  for  this  devastating  syndrome.  Thedifficulty  in  early  ARDS  recognition  stems  from  its  complex  and  heterogenousnature.  In  this  study,  we  integrate  knowledge  of  the  heterogeneity  of  ARDSpatients  into  predictive  model  building.  Using  MIMIC-III  data,  we  first  applylatent  class  analysis  (LCA)  to  identify  homogeneous  sub-groups  in  the  ARDSpopulation,  and  then  build  predictive  models  on  the  partitioned  data.  Theresults  indicate  that  significantly  improved  performances  of  prediction  can  beobtained  for  two  of  the  three  identified  sub-phenotypes  of  ARDS.  Experimentssuggests  that  identifying  sub-phenotypes  is  beneficial  for  building  predictivemodel  for  ARDS.</p>
http://arxiv.org/abs/1903.12133|A  Multimodal  Emotion  Sensing  Platform  for  Building  Emotion-Aware  Applications.  (arXiv:1903.12133v1  [cs.HC])|<p>Humans  use  a  host  of  signals  to  infer  the  emotional  state  of  others.  Ingeneral,  computer  systems  that  leverage  signals  from  multiple  modalities  willbe  more  robust  and  accurate  in  the  same  task.  We  present  a  multimodal  affectand  context  sensing  platform.  The  system  is  composed  of  video,  audio  andapplication  analysis  pipelines  that  leverage  ubiquitous  sensors  (camera  andmicrophone)  to  log  and  broadcast  emotion  data  in  real-time.  The  platform  isdesigned  to  enable  easy  prototyping  of  novel  computer  interfaces  that  sense,respond  and  adapt  to  human  emotion.  This  paper  describes  the  different  audio,visual  and  application  processing  components  and  explains  how  the  data  isstored  and/or  broadcast  for  other  applications  to  consume.  We  hope  that  thisplatform  helps  advance  the  state-of-the-art  in  affective  computing  by  enablingdevelopment  of  novel  human-computer  interfaces.</p>
http://arxiv.org/abs/1903.12135|Sparse  Reconstruction  from  Hadamard  Matrices:  A  Lower  Bound.  (arXiv:1903.12135v1  [cs.IT])|<p>We  give  a  short  argument  that  yields  a  new  lower  bound  on  the  number  ofsubsampled  rows  from  a  bounded,  orthonormal  matrix  necessary  to  form  a  matrixwith  the  restricted  isometry  property.  We  show  that  for  a  $N  	imes  N$  Hadamardmatrix,  one  cannot  recover  all  $k$-sparse  vectors  unless  the  number  ofsubsampled  rows  is  $Omega(k  log^2  N)$.</p>
http://arxiv.org/abs/1903.12136|Distilling  Task-Specific  Knowledge  from  BERT  into  Simple  Neural  Networks.  (arXiv:1903.12136v1  [cs.CL])|<p>In  the  natural  language  processing  literature,  neural  networks  are  becomingincreasingly  deeper  and  complex.  The  recent  poster  child  of  this  trend  is  thedeep  language  representation  model,  which  includes  BERT,  ELMo,  and  GPT.  Thesedevelopments  have  led  to  the  conviction  that  previous-generation,  shallowerneural  networks  for  language  understanding  are  obsolete.  In  this  paper,however,  we  demonstrate  that  rudimentary,  lightweight  neural  networks  can  stillbe  made  competitive  without  architecture  changes,  external  training  data,  oradditional  input  features.  We  propose  to  distill  knowledge  from  BERT,  astate-of-the-art  language  representation  model,  into  a  single-layer  BiLSTM,  aswell  as  its  siamese  counterpart  for  sentence-pair  tasks.  Across  multipledatasets  in  paraphrasing,  natural  language  inference,  and  sentimentclassification,  we  achieve  comparable  results  with  ELMo,  while  using  roughly100  times  fewer  parameters  and  15  times  less  inference  time.</p>
http://arxiv.org/abs/1903.12139|Automatic  Defect  Segmentation  on  Leather  with  Deep  Learning.  (arXiv:1903.12139v1  [cs.CV])|<p>Leather  is  a  natural  and  durable  material  created  through  a  process  oftanning  of  hides  and  skins  of  animals.  The  price  of  the  leather  is  subjectiveas  it  is  highly  sensitive  to  its  quality  and  surface  defects  condition.  In  theliterature,  there  are  very  few  works  investigating  on  the  defects  detection  forleather  using  automatic  image  processing  techniques.  The  manual  defectinspection  process  is  essential  in  an  leather  production  industry  to  controlthe  quality  of  the  finished  products.  However,  it  is  tedious,  as  it  is  labourintensive,  time  consuming,  causes  eye  fatigue  and  often  prone  to  human  error.In  this  paper,  a  fully  automatic  defect  detection  and  marking  system  on  a  calfleather  is  proposed.  The  proposed  system  consists  of  a  piece  of  leather,  LEDlight,  high  resolution  camera  and  a  robot  arm.  Succinctly,  a  machine  visionmethod  is  presented  to  identify  the  position  of  the  defects  on  the  leatherusing  a  deep  learning  architecture.  Then,  a  series  of  processes  are  conductedto  predict  the  defect  instances,  including  elicitation  of  the  leather  imageswith  a  robot  arm,  train  and  test  the  images  using  a  deep  learning  architectureand  determination  of  the  boundary  of  the  defects  using  mathematical  derivationof  the  geometry.  Note  that,  all  the  processes  do  not  involve  humanintervention,  except  for  the  defect  ground  truths  construction  stage.  Theproposed  algorithm  is  capable  to  exhibit  91.5%  segmentation  accuracy  on  thetrain  data  and  70.35%  on  the  test  data.  We  also  report  confusion  matrix,F1-score,  precision  and  specificity,  sensitivity  performance  metrics  to  furtherverify  the  effectiveness  of  the  proposed  approach.</p>
http://arxiv.org/abs/1903.12141|Improving  MAE  against  CCE  under  Label  Noise.  (arXiv:1903.12141v1  [cs.LG])|<p>Label  noise  is  inherent  in  many  deep  learning  tasks  when  the  training  setbecomes  large.  A  typical  approach  to  tackle  noisy  labels  is  using  robust  lossfunctions.  Categorical  cross  entropy  (CCE)  is  a  successful  loss  function  inmany  applications.  However,  CCE  is  also  notorious  for  fitting  samples  withcorrupted  labels  easily.  In  contrast,  mean  absolute  error  (MAE)  isnoise-tolerant  theoretically,  but  it  generally  works  much  worse  than  CCE  inpractice.  In  this  work,  we  have  three  main  points.  First,  to  explain  why  MAEgenerally  performs  much  worse  than  CCE,  we  introduce  a  new  understanding  ofthem  fundamentally  by  exposing  their  intrinsic  sample  weighting  schemes  fromthe  perspective  of  every  sample's  gradient  magnitude  with  respect  to  logitvector.  Consequently,  we  find  that  MAE's  differentiation  degree  over  trainingexamples  is  too  small  so  that  informative  ones  cannot  contribute  enough  againstthe  non-informative  during  training.  Therefore,  MAE  generally  underfitstraining  data  when  noise  rate  is  high.  Second,  based  on  our  finding,  we  proposean  improved  MAE  (IMAE),  which  inherits  MAE's  good  noise-robustness.  Moreover,the  differentiation  degree  over  training  data  points  is  controllable  so  thatIMAE  addresses  the  underfitting  problem  of  MAE.  Third,  the  effectiveness  ofIMAE  against  CCE  and  MAE  is  evaluated  empirically  with  extensive  experiments,which  focus  on  image  classification  under  synthetic  corrupted  labels  and  videoretrieval  under  real  noisy  labels.</p>
http://arxiv.org/abs/1903.12146|Improved  Lower  Bounds  for  the  Restricted  Isometry  Property  of  Subsampled  Fourier  Matrices.  (arXiv:1903.12146v1  [cs.IT])|"<p>Let  $A$  be  an  $N  	imes  N$  Fourier  matrix  over$mathbb{F}_p^{log{N}/log{p}}$  for  some  prime  $p$.  We  improve  upon  knownlower  bounds  for  the  number  of  rows  of  $A$  that  must  be  sampled  so  that  theresulting  matrix  $M$  satisfies  the  restricted  isometry  property  for  $k$-sparsevectors.  This  property  states  that  $ |Mv |_2^2$  is  approximately  $ |v |_2^2$for  all  $k$-sparse  vectors  $v$.  In  particular,  if  $k  =  Omega(  log^2{N})$,  weshow  that  $Omega(klog{k}log{N}/log{p})$  rows  must  be  sampled  to  satisfy  therestricted  isometry  property  with  constant  probability.</p>"
http://arxiv.org/abs/1903.12150|Dynamic  Streaming  Spectral  Sparsification  in  Nearly  Linear  Time  and  Space.  (arXiv:1903.12150v1  [cs.DS])|<p>In  this  paper  we  consider  the  problem  of  computing  spectral  approximations  tographs  in  the  single  pass  dynamic  streaming  model.  We  provide  a  linearsketching  based  solution  that  given  a  stream  of  edge  insertions  and  deletionsto  a  $n$-node  undirected  graph,  uses  $	ilde  O(n)$  space,  processes  each  updatein  $	ilde  O(1)$  time,  and  with  high  probability  recovers  a  spectral  sparsifierin  $	ilde  O(n)$  time.  Prior  to  our  work,  state  of  the  art  results  either  usednear  optimal  $	ilde  O(n)$  space  complexity,  but  brute-force  $Omega(n^2)$recovery  time  [Kapralov  et  al.'14],  or  with  subquadratic  runtime,  butpolynomially  suboptimal  space  complexity  [Ahn  et  al.'14,  Kapralov  et  al.'19].</p><p>Our  main  technical  contribution  is  a  novel  method  for  `bucketing'  vertices  ofthe  input  graph  into  clusters  that  allows  fast  recovery  of  edges  ofsufficiently  large  effective  resistance.  Our  algorithm  first  buckets  verticesof  the  graph  by  performing  ball-carving  using  (an  approximation  to)  itseffective  resistance  metric,  and  then  recovers  the  high  effective  resistanceedges  from  a  sketched  version  of  an  electrical  flow  between  vertices  in  abucket,  taking  nearly  linear  time  in  the  number  of  vertices  overall.  Thisprocess  is  performed  at  different  geometric  scales  to  recover  a  sample  of  edgeswith  probabilities  proportional  to  effective  resistances  and  obtain  an  actualsparsifier  of  the  input  graph.</p><p>This  work  provides  both  the  first  efficient  $ell_2$-sparse  recoveryalgorithm  for  graphs  and  new  primitives  for  manipulating  the  effectiveresistance  embedding  of  a  graph,  both  of  which  we  hope  have  furtherapplications.</p>
http://arxiv.org/abs/1903.12152|3D  Whole  Brain  Segmentation  using  Spatially  Localized  Atlas  Network  Tiles.  (arXiv:1903.12152v1  [cs.CV])|<p>Detailed  whole  brain  segmentation  is  an  essential  quantitative  technique,which  provides  a  non-invasive  way  of  measuring  brain  regions  from  a  structuralmagnetic  resonance  imaging  (MRI).  Recently,  deep  convolution  neural  network(CNN)  has  been  applied  to  whole  brain  segmentation.  However,  restricted  bycurrent  GPU  memory,  2D  based  methods,  downsampling  based  3D  CNN  methods,  andpatch-based  high-resolution  3D  CNN  methods  have  been  the  de  facto  standardsolutions.  3D  patch-based  high  resolution  methods  typically  yield  superiorperformance  among  CNN  approaches  on  detailed  whole  brain  segmentation  (&gt;100labels),  however,  whose  performance  are  still  commonly  inferior  compared  withmulti-atlas  segmentation  methods  (MAS)  due  to  the  following  challenges:  (1)  asingle  network  is  typically  used  to  learn  both  spatial  and  contextualinformation  for  the  patches,  (2)  limited  manually  traced  whole  brain  volumesare  available  (typically  less  than  50)  for  training  a  network.  In  this  work,  wepropose  the  spatially  localized  atlas  network  tiles  (SLANT)  method  todistribute  multiple  independent  3D  fully  convolutional  networks  (FCN)  forhigh-resolution  whole  brain  segmentation.  To  address  the  first  challenge,multiple  spatially  distributed  networks  were  used  in  the  SLANT  method,  in  whicheach  network  learned  contextual  information  for  a  fixed  spatial  location.  Toaddress  the  second  challenge,  auxiliary  labels  on  5111  initially  unlabeledscans  were  created  by  multi-atlas  segmentation  for  training.  Since  the  methodintegrated  multiple  traditional  medical  image  processing  methods  with  deeplearning,  we  developed  a  containerized  pipeline  to  deploy  the  end-to-endsolution.  From  the  results,  the  proposed  method  achieved  superior  performancecompared  with  multi-atlas  segmentation  methods,  while  reducing  thecomputational  time  from  &gt;30  hours  to  15  minutes(https://github.com/MASILab/SLANTbrainSeg).</p>
http://arxiv.org/abs/1903.12157|Resilient  Combination  of  Complementary  CNN  and  RNN  Features  for  Text  Classification  through  Attention  and  Ensembling.  (arXiv:1903.12157v1  [cs.CL])|<p>State-of-the-art  methods  for  text  classification  include  several  distinctsteps  of  pre-processing,  feature  extraction  and  post-processing.  In  this  work,we  focus  on  end-to-end  neural  architectures  and  show  that  the  best  performancein  text  classification  is  obtained  by  combining  information  from  differentneural  modules.  Concretely,  we  combine  convolution,  recurrent  and  attentionmodules  with  ensemble  methods  and  show  that  they  are  complementary.  Weintroduce  ECGA,  an  end-to-end  go-to  architecture  for  novel  text  classificationtasks.  We  prove  that  it  is  efficient  and  robust,  as  it  attains  or  surpasses  thestate-of-the-art  on  varied  datasets,  including  both  low  and  high  data  regimes.</p>
http://arxiv.org/abs/1903.12161|Fast  video  object  segmentation  with  Spatio-Temporal  GANs.  (arXiv:1903.12161v1  [cs.CV])|<p>Learning  descriptive  spatio-temporal  object  models  from  data  is  paramount  forthe  task  of  semi-supervised  video  object  segmentation.  Most  existing  approachesmainly  rely  on  models  that  estimate  the  segmentation  mask  based  on  a  referencemask  at  the  first  frame  (aided  sometimes  by  optical  flow  or  the  previous  mask).These  models,  however,  are  prone  to  fail  under  rapid  appearance  changes  orocclusions  due  to  their  limitations  in  modelling  the  temporal  component.  On  theother  hand,  very  recently,  other  approaches  learned  long-term  features  using  aconvolutional  LSTM  to  leverage  the  information  from  all  previous  video  frames.Even  though  these  models  achieve  better  temporal  representations,  they  stillhave  to  be  fine-tuned  for  every  new  video  sequence.  In  this  paper,  we  presentan  intermediate  solution  and  devise  a  novel  GAN  architecture,  FaSTGAN,  to  learnspatio-temporal  object  models  over  finite  temporal  windows.  To  achieve  this,  weconcentrate  all  the  heavy  computational  load  to  the  training  phase  with  twocritics  that  enforce  spatial  and  temporal  mask  consistency  over  the  last  Kframes.  Then  at  test  time,  we  only  use  a  relatively  light  regressor,  whichreduces  the  inference  time  considerably.  As  a  result,  our  approach  combines  ahigh  resiliency  to  sudden  geometric  and  photometric  object  changes  withefficiency  at  test  time  (no  need  for  fine-tuning  nor  post-processing).  Wedemonstrate  that  the  accuracy  of  our  method  is  on  par  with  state-of-the-arttechniques  on  the  challenging  YouTube-VOS  and  DAVIS  datasets,  while  running  at32  fps,  about  4x  faster  than  the  closest  competitor.</p>
http://arxiv.org/abs/1903.12164|Cache-Version  Selection  and  Content  Placement  for  Adaptive  Video  Streaming  in  Wireless  Edge  Networks.  (arXiv:1903.12164v1  [cs.NI])|<p>Wireless  edge  networks  are  promising  to  provide  better  video  streamingservices  to  mobile  users  by  provisioning  computing  and  storage  resources  at  theedge  of  wireless  network.  However,  due  to  the  diversity  of  user  interests,  userdevices,  video  versions  or  resolutions,  cache  sizes,  network  conditions,  etc.,it  is  challenging  to  decide  where  to  place  the  video  contents,  and  which  cacheand  video  version  a  mobile  user  device  should  select.  In  this  paper,  we  studythe  joint  optimization  of  cache-version  selection  and  content  placement  foradaptive  video  streaming  in  wireless  edge  networks.  We  propose  a  set  ofpractical  distributed  algorithms  that  operate  at  each  user  device  and  eachnetwork  cache  to  maximize  the  overall  network  utility.  In  addition  to  provingthat  our  algorithms  indeed  achieve  the  optimal  performance,  we  implement  ouralgorithms  as  well  as  several  baseline  algorithms  on  ndnSIM,  an  ns-3  basedNamed  Data  Networking  simulator.  Simulation  evaluations  demonstrate  that  ouralgorithms  significantly  outperform  conventional  heuristic  solutions  foradaptive  video  streaming.</p>
http://arxiv.org/abs/1903.12165|Faster  Spectral  Sparsification  in  Dynamic  Streams.  (arXiv:1903.12165v1  [cs.DS])|<p>Graph  sketching  has  emerged  as  a  powerful  technique  for  processing  massivegraphs  that  change  over  time  (i.e.,  are  presented  as  a  dynamic  stream  of  edgeupdates)  over  the  past  few  years,  starting  with  the  work  of  Ahn,  Guha  andMcGregor  (SODA'12)  on  graph  connectivity  via  sketching.  In  this  paper  weconsider  the  problem  of  designing  spectral  approximations  to  graphs,  orspectral  sparsifiers,  using  a  small  number  of  linear  measurements,  with  theadditional  constraint  that  the  sketches  admit  an  efficient  recovery  scheme.</p><p>Prior  to  our  work,  sketching  algorithms  were  known  with  near  optimal  $	ildeO(n)$  space  complexity,  but  $Omega(n^2)$  time  decoding  (brute-force  over  allpotential  edges  of  the  input  graph),  or  with  subquadratic  time,  but  ratherlarge  $Omega(n^{5/3})$  space  complexity  (due  to  their  reliance  on  a  ratherweak  relation  between  connectivity  and  effective  resistances).  In  this  paper  wefirst  show  how  a  simple  relation  between  effective  resistances  and  edgeconnectivity  leads  to  an  improved  $widetilde  O(n^{3/2})$  space  and  timealgorithm,  which  we  show  is  a  natural  barrier  for  connectivity  basedapproaches.  Our  main  result  then  gives  the  first  algorithm  that  achievessubquadratic  recovery  time,  i.e.  avoids  brute-force  decoding,  and  at  the  sametime  nontrivially  uses  the  effective  resistance  metric,  achieving$n^{1.4+o(1)}$  space  and  recovery  time.</p><p>Our  main  technical  contribution  is  a  novel  method  for  `bucketing'  vertices  ofthe  input  graph  into  clusters  that  allows  fast  recovery  of  edges  of  higheffective  resistance:  the  buckets  are  formed  by  performing  ball-carving  on  theinput  graph  using  (an  approximation  to)  its  effective  resistance  metric.  Wefeel  that  this  technique  is  likely  to  be  of  independent  interest.</p>
http://arxiv.org/abs/1903.12174|TensorMask:  A  Foundation  for  Dense  Object  Segmentation.  (arXiv:1903.12174v1  [cs.CV])|<p>Sliding-window  object  detectors  that  generate  bounding-box  object  predictionsover  a  dense,  regular  grid  have  advanced  rapidly  and  proven  popular.  Incontrast,  modern  instance  segmentation  approaches  are  dominated  by  methods  thatfirst  detect  object  bounding  boxes,  and  then  crop  and  segment  these  regions,  aspopularized  by  Mask  R-CNN.  In  this  work,  we  investigate  the  paradigm  of  densesliding-window  instance  segmentation,  which  is  surprisingly  under-explored.  Ourcore  observation  is  that  this  task  is  fundamentally  different  than  other  denseprediction  tasks  such  as  semantic  segmentation  or  bounding-box  objectdetection,  as  the  output  at  every  spatial  location  is  itself  a  geometricstructure  with  its  own  spatial  dimensions.  To  formalize  this,  we  treat  denseinstance  segmentation  as  a  prediction  task  over  4D  tensors  and  present  ageneral  framework  called  TensorMask  that  explicitly  captures  this  geometry  andenables  novel  operators  on  4D  tensors.  We  demonstrate  that  the  tensor  viewleads  to  large  gains  over  baselines  that  ignore  this  structure,  and  leads  toresults  comparable  to  Mask  R-CNN.  These  promising  results  suggest  thatTensorMask  can  serve  as  a  foundation  for  novel  advances  in  dense  maskprediction  and  a  more  complete  understanding  of  the  task.  Code  will  be  madeavailable.</p>
http://arxiv.org/abs/1405.4806|Undecidability  of  model-checking  branching-time  properties  of  stateless  probabilistic  pushdown  process.  (arXiv:1405.4806v9  [cs.LO]  UPDATED)|<p>In  this  paper,  we  settle  a  problem  in  probabilistic  verification  ofinfinite--state  process  (specifically,  {it  probabilistic  pushdown  process}).We  show  that  model  checking  {it  stateless  probabilistic  pushdown  process}(pBPA)  against  {it  probabilistic  computational  tree  logic}  (PCTL)  isundecidable.</p>
http://arxiv.org/abs/1610.02336|Approximation  Algorithms  for  Multi-Multiway  Cut  and  Multicut  Problems  on  Directed  Graphs.  (arXiv:1610.02336v4  [cs.DS]  UPDATED)|<p>In  this  paper,  we  present  two  approximation  algorithms  for  the  directedmulti-multiway  cut  and  directed  multicut  problems.  The  so  called  region  growingparadigm  cite{1}  is  modified  and  used  for  these  two  cut  problems  on  directedgraphs.  By  using  this  paradigm,  we  give  for  each  problem  an  approximationalgorithm  such  that  both  algorithms  have  the  approximate  factor  $O(k)$  the  sameas  the  previous  works  done  on  these  problems.  However,  the  previous  works  needto  solve  $k$  linear  programming,  whereas  our  algorithms  require  only  one  linearprogramming.  Therefore,  our  algorithms  improve  the  running  time  of  the  previousalgorithms.</p>
http://arxiv.org/abs/1612.07562|On  the  function  approximation  error  for  risk-sensitive  reinforcement  learning.  (arXiv:1612.07562v14  [cs.LG]  UPDATED)|"<p>In  this  paper  we  obtain  several  informative  error  bounds  on  functionapproximation  for  the  policy  evaluation  algorithm  proposed  by  Basu  et  al.  whenthe  aim  is  to  find  the  risk-sensitive  cost  represented  using  exponentialutility.  The  main  idea  is  to  use  classical  Bapat's  inequality  and  to  usePerron-Frobenius  eigenvectors  (exists  if  we  assume  irreducible  Markov  chain)  toget  the  new  bounds.  The  novelty  of  our  approach  is  that  we  use  theirreduciblity  of  Markov  chain  to  get  the  new  bounds  whereas  the  earlier  work  byBasu  et  al.  used  spectral  variation  bound  which  is  true  for  any  matrix.  We  alsogive  examples  where  all  our  bounds  achieve  the  ""actual  error""  whereas  theearlier  bound  given  by  Basu  et  al.  is  much  weaker  in  comparison.  We  show  thatthis  happens  due  to  the  absence  of  difference  term  in  the  earlier  bound  whichis  always  present  in  all  our  bounds  when  the  state  space  is  large.Additionally,  we  discuss  how  all  our  bounds  compare  with  each  other.  As  acorollary  of  our  main  result  we  provide  a  bound  between  largest  eigenvalues  oftwo  irreducibile  matrices  in  terms  of  the  matrix  entries.</p>"
http://arxiv.org/abs/1703.05834|Alignment  of  the  Virtual  Scene  to  the  Tracking  Space  of  a  Mixed  Reality  Head-Mounted  Display.  (arXiv:1703.05834v4  [cs.HC]  UPDATED)|<p>With  the  mounting  global  interest  for  optical  see-through  head-mounteddisplays  (OST-HMDs)  across  medical,  industrial  and  entertainment  settings,  manysystems  with  different  capabilities  are  rapidly  entering  the  market.  Despitesuch  variety,  they  all  require  display  calibration  to  create  a  proper  mixedreality  environment.  With  the  aid  of  tracking  systems,  it  is  possible  toregister  rendered  graphics  with  tracked  objects  in  the  real  world.  We  propose  acalibration  procedure  to  properly  align  the  coordinate  system  of  a  3D  virtualscene  that  the  user  sees  with  that  of  the  tracker.  Our  method  takes  a  blackboxapproach  towards  the  HMD  calibration,  where  the  tracker's  data  is  its  input  andthe  3D  coordinates  of  a  virtual  object  in  the  observer's  eye  is  the  output;  theobjective  is  thus  to  find  the  3D  projection  that  aligns  the  virtual  contentwith  its  real  counterpart.  In  addition,  a  faster  and  more  intuitive  version  ofthis  calibration  is  introduced  in  which  the  user  simultaneously  aligns  multiplepoints  of  a  single  virtual  3D  object  with  its  real  counterpart;  this  reducesthe  number  of  required  repetitions  in  the  alignment  from  20  to  only  4,  whichleads  to  a  much  easier  calibration  task  for  the  user.  In  this  paper,  bothinternal  (HMD  camera)  and  external  tracking  systems  are  studied.  We  performexperiments  with  Microsoft  HoloLens,  taking  advantage  of  its  self  localizationand  spatial  mapping  capabilities  to  eliminate  the  requirement  for  line  of  sightfrom  the  HMD  to  the  object  or  external  tracker.  The  experimental  resultsindicate  an  accuracy  of  up  to  4  mm  in  the  average  reprojection  error  based  ontwo  separate  evaluation  methods.  We  further  perform  experiments  with  theinternal  tracking  on  the  Epson  Moverio  BT-300  to  demonstrate  that  the  methodcan  provide  similar  results  with  other  HMDs.</p>
http://arxiv.org/abs/1705.03557|DeepTingle.  (arXiv:1705.03557v2  [cs.CL]  UPDATED)|"<p>DeepTingle  is  a  text  prediction  and  classification  system  trained  on  thecollected  works  of  the  renowned  fantastic  gay  erotica  author  Chuck  Tingle.Whereas  the  writing  assistance  tools  you  use  everyday  (in  the  form  ofpredictive  text,  translation,  grammar  checking  and  so  on)  are  trained  ongeneric,  purportedly  ""neutral""  datasets,  DeepTingle  is  trained  on  a  veryspecific,  internally  consistent  but  externally  arguably  eccentric  dataset.  Thisallows  us  to  foreground  and  confront  the  norms  embedded  in  data-drivencreativity  and  productivity  assistance  tools.  As  such  tools  effectivelyfunction  as  extensions  of  our  cognition  into  technology,  it  is  important  toidentify  the  norms  they  embed  within  themselves  and,  by  extension,  us.DeepTingle  is  realized  as  a  web  application  based  on  LSTM  networks  and  theGloVe  word  embedding,  implemented  in  JavaScript  with  Keras-JS.</p>"
http://arxiv.org/abs/1705.06058|Pitfalls  and  Best  Practices  in  Algorithm  Configuration.  (arXiv:1705.06058v3  [cs.AI]  UPDATED)|<p>Good  parameter  settings  are  crucial  to  achieve  high  performance  in  many  areasof  artificial  intelligence  (AI),  such  as  propositional  satisfiability  solving,AI  planning,  scheduling,  and  machine  learning  (in  particular  deep  learning).Automated  algorithm  configuration  methods  have  recently  received  much  attentionin  the  AI  community  since  they  replace  tedious,  irreproducible  and  error-pronemanual  parameter  tuning  and  can  lead  to  new  state-of-the-art  performance.However,  practical  applications  of  algorithm  configuration  are  prone  to  several(often  subtle)  pitfalls  in  the  experimental  design  that  can  render  theprocedure  ineffective.  We  identify  several  common  issues  and  propose  bestpractices  for  avoiding  them.  As  one  possibility  for  automatically  handling  asmany  of  these  as  possible,  we  also  propose  a  tool  called  GenericWrapper4AC.</p>
http://arxiv.org/abs/1707.05881|On  Thin  Air  Reads:  Towards  an  Event  Structures  Model  of  Relaxed  Memory.  (arXiv:1707.05881v5  [cs.PL]  UPDATED)|<p>To  model  relaxed  memory,  we  propose  confusion-free  event  structures  over  analphabet  with  a  justification  relation.  Executions  are  modeled  by  justifiedconfigurations,  where  every  read  event  has  a  justifying  write  event.Justification  alone  is  too  weak  a  criterion,  since  it  allows  cycles  of  the  kindthat  result  in  so-called  thin-air  reads.  Acyclic  justification  forbids  suchcycles,  but  also  invalidates  event  reorderings  that  result  from  compileroptimizations  and  dynamic  instruction  scheduling.  We  propose  the  notion  ofwell-justification,  based  on  a  game-like  model,  which  strikes  a  middle  ground.</p><p>We  show  that  well-justified  configurations  satisfy  the  DRF  theorem:  in  anydata-race  free  program,  all  well-justified  configurations  are  sequentiallyconsistent.  We  also  show  that  rely-guarantee  reasoning  is  sound  forwell-justified  configurations,  but  not  for  justified  configurations.  Forexample,  well-justified  configurations  are  type-safe.</p><p>Well-justification  allows  many,  but  not  all  reorderings  performed  by  relaxedmemory.  In  particular,  it  fails  to  validate  the  commutation  of  independentreads.  We  discuss  variations  that  may  address  these  shortcomings.</p>
http://arxiv.org/abs/1708.01688|Abstract  Hidden  Markov  Models:  a  monadic  account  of  quantitative  information  flow.  (arXiv:1708.01688v3  [cs.LO]  UPDATED)|<p>Hidden  Markov  Models,  HMM's,  are  mathematical  models  of  Markov  processes  withstate  that  is  hidden,  but  from  which  information  can  leak.  They  are  typicallyrepresented  as  3-way  joint-probability  distributions.</p><p>We  use  HMM's  as  denotations  of  probabilistic  hidden-state  sequentialprograms:  for  that,  we  recast  them  as  `abstract'  HMM's,  computations  in  theGiry  monad  $mathbb{D}$,  and  we  equip  them  with  a  partial  order  of  increasingsecurity.  However  to  encode  the  monadic  type  with  hiding  over  some  state$mathcal{X}$  we  use  $mathbb{D}mathcal{X}	o  mathbb{D}^2mathcal{X}$  ratherthan  the  conventional  $mathcal{X}{	o}mathbb{D}mathcal{X}$  that  suffices  forMarkov  models  whose  state  is  not  hidden.  We  illustrate  the$mathbb{D}mathcal{X}	o  mathbb{D}^2mathcal{X}$  construction  with  a  smallHaskell  prototype.</p><p>We  then  present  uncertainty  measures  as  a  generalisation  of  the  extantdiversity  of  probabilistic  entropies,  with  characteristic  analytic  propertiesfor  them,  and  show  how  the  new  entropies  interact  with  the  order  of  increasingsecurity.  Furthermore,  we  give  a  `backwards'  uncertainty-transformer  semanticsfor  HMM's  that  is  dual  to  the  `forwards'  abstract  HMM's  -  it  is  an  analogue  ofthe  duality  between  forwards,  relational  semantics  and  backwards,predicate-transformer  semantics  for  imperative  programs  with  demonic  choice.</p><p>Finally,  we  argue  that,  from  this  new  denotational-semantic  viewpoint,  onecan  see  that  the  Dalenius  desideratum  for  statistical  databases  is  actually  anissue  in  compositionality.  We  propose  a  means  for  taking  it  into  account.</p>
http://arxiv.org/abs/1709.07605|mts:  A  light  framework  for  parallelizing  tree  search  codes.  (arXiv:1709.07605v2  [cs.DC]  UPDATED)|<p>We  describe  mts,  a  generic  framework  for  parallelizing  certain  types  of  treesearch  programs  including  reverse  search,  backtracking,  branch  and  bound  andsatisfiability  testing.  It  abstracts  and  generalizes  the  ideas  used  inparallelizing  lrs,  a  reverse  search  code  for  vertex  enumeration.  mts  supportssharing  information  between  processes  which  is  important  for  applications  suchas  satisfiability  testing  and  branch-and-bound.  No  parallelization  isimplemented  in  the  legacy  single  processor  programs  minimizing  the  changesneeded  and  simplying  debugging.  mts  is  written  in  C,  uses  MPI  forparallelization  and  can  be  used  on  a  network  of  computers.  We  give  fourexamples  of  reverse  search  codes  parallelized  by  using  mts:  topological  sorts,spanning  trees,  triangulations  and  Galton-Watson  trees.  We  also  give  aparallelization  of  two  codes  for  satisfiability  testing.  We  give  experimentalresults  comparing  the  parallel  codes  with  other  codes  for  the  same  problems.</p>
http://arxiv.org/abs/1710.10718|An  Approximation  Algorithm  for  Optimal  Clique  Cover  Delivery  in  Coded  Caching.  (arXiv:1710.10718v3  [cs.IT]  UPDATED)|"<p>Coded  caching  can  significantly  reduce  the  communication  bandwidthrequirement  for  satisfying  users'  demands  by  utilizing  the  multicasting  gainamong  multiple  users.  Most  existing  works  assume  that  the  users  follow  theprescriptions  for  content  placement  made  by  the  system.  However,  users  mayprefer  to  decide  what  files  to  cache.  To  address  this  issue,  we  consider  anetwork  consisting  of  a  file  server  connected  through  a  shared  link  to  $K$users,  each  equipped  with  a  cache  which  has  been  already  filled  arbitrarily.Given  an  arbitrary  content  placement,  the  goal  is  to  find  a  delivery  strategyfor  the  server  that  minimizes  the  load  of  the  shared  link.  In  this  paper,  wefocus  on  a  specific  class  of  coded  multicasting  delivery  schemes  known  as  the""clique  cover  delivery  scheme"".  We  first  formulate  the  optimal  clique  coverdelivery  problem  as  a  combinatorial  optimization  problem.  Using  a  connectionwith  the  weighted  set  cover  problem,  we  propose  an  approximation  algorithm  andshow  that  it  provides  an  approximation  ratio  of  $(1  +  log  K)$,  while  theapproximation  ratio  for  the  existing  coded  delivery  schemes  is  linear  in  $K$.Numerical  simulations  show  that  our  proposed  algorithm  provides  a  considerablebandwidth  reduction  over  the  existing  coded  delivery  schemes  for  almost  allcontent  placement  schemes.</p>"
http://arxiv.org/abs/1711.00113|Proving  Soundness  of  Extensional  Normal-Form  Bisimilarities.  (arXiv:1711.00113v4  [cs.LO]  UPDATED)|<p>Normal-form  bisimilarity  is  a  simple,  easy-to-use  behavioral  equivalence  thatrelates  terms  in  $lambda$-calculi  by  decomposing  their  normal  forms  intobisimilar  subterms.  Moreover,  it  typically  allows  for  powerful  up-totechniques,  such  as  bisimulation  up  to  context,  which  simplify  bisimulationproofs  even  further.  However,  proving  soundness  of  these  relations  becomescomplicated  in  the  presence  of  $eta$-expansion  and  usually  relies  on  ad  hocproof  methods  which  depend  on  the  language.  In  this  paper  we  propose  a  moresystematic  proof  method  to  show  that  an  extensional  normal-form  bisimilarityalong  with  its  corresponding  up  to  context  technique  are  sound.  We  illustrateour  technique  with  three  calculi:  the  call-by-value  $lambda$-calculus,  thecall-by-value  $lambda$-calculus  with  the  delimited-control  operators  shift  andreset,  and  the  call-by-value  $lambda$-calculus  with  the  abortive  controloperators  call/cc  and  abort.  In  the  first  two  cases,  there  was  previously  nosound  up  to  context  technique  validating  the  $eta$-law,  whereas  no  theory  ofnormal-form  bisimulations  for  a  calculus  with  call/cc  and  abort  has  beenpresented  before.  Our  results  have  been  fully  formalized  in  the  Coq  proofassistant.</p>
http://arxiv.org/abs/1712.05500|Ergodicity  of  some  classes  of  cellular  automata  subject  to  noise.  (arXiv:1712.05500v3  [math.PR]  UPDATED)|<p>Cellular  automata  (CA)  are  dynamical  systems  on  symbolic  configurations  onthe  lattice.  They  are  also  used  as  models  of  massively  parallel  computers.  Asdynamical  systems,  one  would  like  to  understand  the  effect  of  small  randomperturbations  on  the  dynamics  of  CA.  As  models  of  computation,  they  can  be  usedto  study  the  reliability  of  computation  against  noise.</p><p>We  consider  various  families  of  CA  (nilpotent,  permutive,  gliders,  CA  with  aspreading  symbol,  surjective,  algebraic)  and  prove  that  they  are  highlyunstable  against  noise,  meaning  that  they  forget  their  initial  conditions  underslightest  positive  noise.  This  is  manifested  as  the  ergodicity  of  the  resultingprobabilistic  CA.  The  proofs  involve  a  collection  of  different  techniques(couplings,  entropy,  Fourier  analysis),  depending  on  the  dynamical  propertiesof  the  underlying  deterministic  CA  and  the  type  of  noise.</p>
http://arxiv.org/abs/1801.04613|Software  Defined  Networks  based  Smart  Grid  Communication:  A  Comprehensive  Survey.  (arXiv:1801.04613v4  [cs.NI]  UPDATED)|<p>The  current  power  grid  is  no  longer  a  feasible  solution  due  toever-increasing  user  demand  of  electricity,  old  infrastructure,  and  reliabilityissues  and  thus  require  transformation  to  a  better  grid  a.k.a.,  smart  grid(SG).  The  key  features  that  distinguish  SG  from  the  conventional  electricalpower  grid  are  its  capability  to  perform  two-way  communication,  demand  sidemanagement,  and  real  time  pricing.  Despite  all  these  advantages  that  SG  willbring,  there  are  certain  issues  which  are  specific  to  SG  communication  system.For  instance,  network  management  of  current  SG  systems  is  complex,  timeconsuming,  and  done  manually.  Moreover,  SG  communication  (SGC)  system  is  builton  different  vendor  specific  devices  and  protocols.  Therefore,  the  current  SGsystems  are  not  protocol  independent,  thus  leading  to  interoperability  issue.Software  defined  network  (SDN)  has  been  proposed  to  monitor  and  manage  thecommunication  networks  globally.  This  article  serves  as  a  comprehensive  surveyon  SDN-based  SGC.  In  this  article,  we  first  discuss  taxonomy  of  advantages  ofSDNbased  SGC.We  then  discuss  SDN-based  SGC  architectures,  along  with  casestudies.  Our  article  provides  an  in-depth  discussion  on  routing  schemes  forSDN-based  SGC.  We  also  provide  detailed  survey  of  security  and  privacy  schemesapplied  to  SDN-based  SGC.  We  furthermore  present  challenges,  open  issues,  andfuture  research  directions  related  to  SDN-based  SGC.</p>
http://arxiv.org/abs/1801.07528|Computer-Assisted  Proving  of  Combinatorial  Conjectures  Over  Finite  Domains:  A  Case  Study  of  a  Chess  Conjecture.  (arXiv:1801.07528v4  [cs.LO]  UPDATED)|<p>There  are  several  approaches  for  using  computers  in  deriving  mathematicalproofs.  For  their  illustration,  we  provide  an  in-depth  study  of  using  computersupport  for  proving  one  complex  combinatorial  conjecture  --  correctness  of  astrategy  for  the  chess  KRK  endgame.  The  final,  machine  verifiable,  resultpresented  in  this  paper  is  that  there  is  a  winning  strategy  for  white  in  theKRK  endgame  generalized  to  $n  	imes  n$  board  (for  natural  $n$  greater  than$3$).  We  demonstrate  that  different  approaches  for  computer-based  theoremproving  work  best  together  and  in  synergy  and  that  the  technology  currentlyavailable  is  powerful  enough  for  providing  significant  help  to  humans  derivingcomplex  proofs.</p>
http://arxiv.org/abs/1802.04634|Lattice  Functions  for  the  Analysis  of  Analog-to-Digital  Conversion.  (arXiv:1802.04634v2  [eess.SP]  UPDATED)|<p>Analog-to-digital  (A/D)  converters  are  the  common  interface  between  analogsignals  and  the  domain  of  digital  discrete-time  signal  processing.  In  essence,this  domain  simultaneously  incorporates  quantization  both  in  amplitude  andtime,  i.e.  amplitude  quantization  and  uniform  time  sampling.  Thus,  we  view  A/Dconversion  as  a  sampling  process  in  both  the  time  and  amplitude  domains  basedon  the  observation  that  the  underlying  continuous-time  signals  representingdigital  sequences  can  be  sampled  in  a  lattice---i.e.  at  points  restricted  tolie  on  a  uniform  grid  both  in  time  and  amplitude.  We  refer  to  them  as  latticefunctions.  This  is  in  contrast  with  the  traditional  approach  based  on  theclassical  sampling  theorem  and  quantization  error  analysis.  The  latter  has  beenmainly  addressed  with  the  help  of  probabilistic  models,  or  deterministic  oneseither  confined  to  very  particular  scenarios  or  considering  worst-caseassumptions.  In  this  paper,  we  provide  a  deterministic  theoretical  analysis  andframework  for  the  functions  involved  in  digital  discrete-time  processing.  Weshow  that  lattice  functions  possess  a  rich  analytic  structure  in  the  context  ofintegral-valued  entire  functions  of  exponential  type.  We  derive  set  andspectral  properties  of  this  class  of  functions.  This  allows  us  to  prove  in  adeterministic  way  and  for  general  bandlimited  functions  a  fundamental  lowerbound  on  the  maximum  frequency  component  introduced  by  quantization  that  isindependent  of  the  resolution  of  the  quantizer.</p>
http://arxiv.org/abs/1802.04672|Delta-Ramp  Encoder  for  Amplitude  Sampling  and  its  Interpretation  as  Time  Encoding.  (arXiv:1802.04672v2  [eess.SP]  UPDATED)|<p>The  theoretical  basis  for  conventional  acquisition  of  bandlimited  signalstypically  relies  on  uniform  time  sampling  and  assumes  infinite-precisionamplitude  values.  In  this  paper,  we  explore  signal  representation  and  recoverybased  on  uniform  amplitude  sampling  with  assumed  infinite  precision  timinginformation.  The  approach  is  based  on  the  delta-ramp  encoder  which  consists  ofapplying  a  one-level  level-crossing  detector  to  the  result  of  adding  anappropriate  sawtooth-like  waveform  to  the  input  signal.  The  output  samples  arethe  time  instants  of  these  level  crossings,  thus  representing  a  time-encodedversion  of  the  input  signal.  For  theoretical  purposes,  this  system  can  beequivalently  analyzed  by  reversibly  transforming  through  ramp  addition  anonmonotonic  input  signal  into  a  monotonic  one  which  is  then  uniformly  sampledin  amplitude.  The  monotonic  function  is  then  represented  by  the  times  at  whichthe  signal  crosses  a  predefined  and  equally-spaced  set  of  amplitude  values.  Werefer  to  this  technique  as  amplitude  sampling.  The  time  sequence  generated  canbe  interpreted  alternatively  as  nonuniform  time  sampling  of  the  original  sourcesignal.  We  derive  duality  and  frequency-domain  properties  for  the  functionsinvolved  in  the  transformation.  Iterative  algorithms  are  proposed  andimplemented  for  recovery  of  the  original  source  signal.  As  indicated  in  thesimulations,  the  proposed  iterative  amplitude-sampling  algorithm  achieves  afaster  convergence  rate  than  frame-based  reconstruction  for  nonuniformsampling.  The  performance  can  also  be  improved  by  appropriate  choice  of  theparameters  while  maintaining  the  same  sampling  density.</p>
http://arxiv.org/abs/1803.08988|Evaluating  Sentence-Level  Relevance  Feedback  for  High-Recall  Information  Retrieval.  (arXiv:1803.08988v2  [cs.IR]  UPDATED)|"<p>This  study  uses  a  novel  simulation  framework  to  evaluate  whether  the  time  andeffort  necessary  to  achieve  high  recall  using  active  learning  is  reduced  bypresenting  the  reviewer  with  isolated  sentences,  as  opposed  to  full  documents,for  relevance  feedback.  Under  the  weak  assumption  that  more  time  and  effort  isrequired  to  review  an  entire  document  than  a  single  sentence,  simulationresults  indicate  that  the  use  of  isolated  sentences  for  relevance  feedback  canyield  comparable  accuracy  and  higher  efficiency,  relative  to  thestate-of-the-art  Baseline  Model  Implementation  (BMI)  of  the  AutoTAR  ContinuousActive  Learning  (""CAL"")  method  employed  in  the  TREC  2015  and  2016  Total  RecallTrack.</p>"
http://arxiv.org/abs/1805.04772|VAMS:  Verifiable  Auditing  of  Access  to  Confidential  Data.  (arXiv:1805.04772v4  [cs.CR]  UPDATED)|<p>The  sharing  of  personal  data  has  the  potential  to  bring  sub-stantial  benefitsboth  to  individuals  and  society,  but  only  if  people  have  confidence  that  theirdata  will  not  be  used  in-appropriately.  As  more  sensitive  data  is  consideredfor  sharing  (e.g.,  communication  records  and  medical  records)  and  used  to  makeimportant  decisions,  there  is  a  growing  need  for  transparency  in  the  way  thatthe  data  is  processed,  while  protecting  the  privacy  of  individuals  and  theintegrity  of  their  data.  We  propose  a  system,  VAMS,  which  allows  individuals  tocheck  accesses  to  their  personal  data,  and  enables  auditors  to  detectviolations  of  policy.  Furthermore,  our  system  protects  the  privacy  ofindividuals  and  organizations,  while  allowing  published  statistics  to  bepublicly  verified.  We  demonstrate  the  practicality  of  our  system  with  twoprototypes,  based  on  Hyperledger  Fabric  and  Trillian.</p>
http://arxiv.org/abs/1805.10505|Cookie  Synchronization:  Everything  You  Always  Wanted  to  Know  But  Were  Afraid  to  Ask.  (arXiv:1805.10505v2  [cs.IR]  UPDATED)|<p>User  data  is  the  primary  input  of  digital  advertising,  fueling  the  freeInternet  as  we  know  it.  As  a  result,  web  companies  invest  a  lot  in  elaboratetracking  mechanisms  to  acquire  user  data  that  can  sell  to  data  markets  andadvertisers.  However,  with  same-origin  policy,  and  cookies  as  a  primaryidentification  mechanism  on  the  web,  each  tracker  knows  the  same  user  with  adifferent  ID.  To  mitigate  this,  Cookie  Synchronization  (CSync)  came  to  therescue,  facilitating  an  information  sharing  channel  between  third  parties  thatmay  or  not  have  direct  access  to  the  website  the  user  visits.  In  thebackground,  with  CSync,  they  merge  user  data  they  own,  but  also  reconstruct  auser's  browsing  history,  bypassing  the  same  origin  policy.  In  this  paper,  weperform  a  first  to  our  knowledge  in-depth  study  of  CSync  in  the  wild,  using  ayear-long  weblog  from  850  real  mobile  users.  Through  our  study,  we  aim  tounderstand  the  characteristics  of  the  CSync  protocol  and  the  impact  it  has  onweb  users'  privacy.  For  this,  we  design  and  implement  CONRAD,  a  holisticmechanism  to  detect  CSync  events  at  real  time,  and  the  privacy  loss  on  the  userside,  even  when  the  synced  IDs  are  obfuscated.  Using  CONRAD,  we  find  that  97%of  the  regular  web  users  are  exposed  to  CSync:  most  of  them  within  the  firstweek  of  their  browsing,  and  the  median  userID  gets  leaked,  on  average,  to  3.5different  domains.  Finally,  we  see  that  CSync  increases  the  number  of  domainsthat  track  the  user  by  a  factor  of  6.75.</p>
http://arxiv.org/abs/1807.00056|Fundamental  Limits  of  Decentralized  Data  Shuffling.  (arXiv:1807.00056v3  [cs.IT]  UPDATED)|<p>Data  shuffling  of  training  data  among  different  computing  nodes  (workers)  hasbeen  identified  as  a  core  element  to  improve  the  statistical  performance  ofmodern  large  scale  machine  learning  algorithms.  Data  shuffling  is  oftenconsidered  as  one  of  the  most  significant  bottlenecks  in  such  systems  due  tothe  heavy  communication  load.  Under  a  master-worker  architecture  (where  amaster  has  access  to  the  entire  dataset  and  only  communication  between  themaster  and  the  workers  is  allowed)  coding  has  been  recently  proved  toconsiderably  reduce  the  communication  load.  This  work  considers  a  differentcommunication  paradigm  referred  to  as  decentralized  data  shuffling,  whereworkers  are  allowed  to  communicate  with  one  another  via  a  shared  link.  Thedecentralized  data  shuffling  problem  has  two  phases:  workers  communicate  witheach  other  during  the  data  shuffling  phase,  and  then  workers  update  theirstored  content  during  the  storage  phase.  For  the  case  of  uncoded  storage  (i.e.,each  worker  directly  stores  a  subset  of  bits  of  the  dataset),  this  paperproposes  converse  and  achievable  bounds  that  are  to  within  a  factor  of  3/2  ofone  another.  The  proposed  schemes  are  also  exactly  optimal  under  the  constraintof  uncoded  storage  for  either  large  memory  size  or  at  most  four  workers  in  thesystem.  As  a  by-product,  a  novel  distributed  clique-covering  scheme  is  proposedfor  distributed  broadcast  with  side  information-a  setting  that  includes  asspecial  cases  decentralized  data  shuffling,  distributed  index  coding,  anddevice-to-device  coded  caching.</p>
http://arxiv.org/abs/1807.01417|The  Implementation  of  the  Colored  Abstract  Simplicial  Complex  and  its  Application  to  Mesh  Generation.  (arXiv:1807.01417v2  [cs.MS]  UPDATED)|<p>We  introduce  CASC:  a  new,  modern,  and  header-only  C++  library  which  providesa  data  structure  to  represent  arbitrary  dimension  abstract  simplicial  complexes(ASC)  with  user-defined  classes  stored  directly  on  the  simplices  at  eachdimension.  This  is  accomplished  by  using  the  latest  C++  language  featuresincluding  variadic  template  parameters  introduced  in  C++11  and  automaticfunction  return  type  deduction  from  C++14.  Effectively  CASC  decouples  therepresentation  of  the  topology  from  the  interactions  of  user  data.  We  presentthe  innovations  and  design  principles  of  the  data  structure  and  relatedalgorithms.  This  includes  a  metadata  aware  decimation  algorithm  which  isgeneral  for  collapsing  simplices  of  any  dimension.  We  also  present  an  exampleapplication  of  this  library  to  represent  an  orientable  surface  mesh.</p>
http://arxiv.org/abs/1807.07648|"On  Chebotar""ev's  nonvanishing  minors  theorem  and  the  Bir'o-Meshulam-Tao  discrete  uncertainty  principle.  (arXiv:1807.07648v2  [math.CA]  UPDATED)"|"<p>Chebotar""ev's  theorem  says  that  every  minor  of  a  discrete  Fourier  matrix  ofprime  order  is  nonzero.  We  prove  a  generalization  of  this  result  that  includesanalogues  for  discrete  cosine  and  discrete  sine  matrices  as  special  cases.  Wethen  establish  a  generalization  of  the  Bir'o-Meshulam-Tao  uncertaintyprinciple  to  functions  with  symmetries  that  arise  from  certain  group  actions,with  some  of  the  simplest  examples  being  even  and  odd  functions.  We  show  thatour  result  is  best  possible  and  in  some  cases  is  stronger  than  that  ofBir'o-Meshulam-Tao.  Some  of  these  results  hold  in  certain  circumstances  fornon-prime  fields;  Gauss  sums  play  a  central  role  in  such  investigations.</p>"
http://arxiv.org/abs/1807.07978|Prior  Convictions:  Black-Box  Adversarial  Attacks  with  Bandits  and  Priors.  (arXiv:1807.07978v3  [stat.ML]  UPDATED)|<p>We  study  the  problem  of  generating  adversarial  examples  in  a  black-boxsetting  in  which  only  loss-oracle  access  to  a  model  is  available.  We  introducea  framework  that  conceptually  unifies  much  of  the  existing  work  on  black-boxattacks,  and  we  demonstrate  that  the  current  state-of-the-art  methods  areoptimal  in  a  natural  sense.  Despite  this  optimality,  we  show  how  to  improveblack-box  attacks  by  bringing  a  new  element  into  the  problem:  gradient  priors.We  give  a  bandit  optimization-based  algorithm  that  allows  us  to  seamlesslyintegrate  any  such  priors,  and  we  explicitly  identify  and  incorporate  twoexamples.  The  resulting  methods  use  two  to  four  times  fewer  queries  and  failtwo  to  five  times  less  often  than  the  current  state-of-the-art.</p>
http://arxiv.org/abs/1807.10936|Unsupervised  Learning  of  a  Hierarchical  Spiking  Neural  Network  for  Optical  Flow  Estimation:  From  Events  to  Global  Motion  Perception.  (arXiv:1807.10936v2  [cs.CV]  UPDATED)|<p>The  combination  of  spiking  neural  networks  and  event-based  vision  sensorsholds  the  potential  of  highly  efficient  and  high-bandwidth  optical  flowestimation.  This  paper  presents  the  first  hierarchical  spiking  architecture  inwhich  motion  (direction  and  speed)  selectivity  emerges  in  an  unsupervisedfashion  from  the  raw  stimuli  generated  with  an  event-based  camera.  A  noveladaptive  neuron  model  and  stable  spike-timing-dependent  plasticity  formulationare  at  the  core  of  this  neural  network  governing  its  spike-based  processing  andlearning,  respectively.  After  convergence,  the  neural  architecture  exhibits  themain  properties  of  biological  visual  motion  systems,  namely  feature  extractionand  local  and  global  motion  perception.  Convolutional  layers  with  inputsynapses  characterized  by  single  and  multiple  transmission  delays  are  employedfor  feature  and  local  motion  perception,  respectively;  while  global  motionselectivity  emerges  in  a  final  fully-connected  layer.  The  proposed  solution  isvalidated  using  synthetic  and  real  event  sequences.  Along  with  this  paper,  weprovide  the  cuSNN  library,  a  framework  that  enables  GPU-accelerated  simulationsof  large-scale  spiking  neural  networks.  Source  code  and  samples  are  availableat  https://github.com/tudelft/cuSNN.</p>
http://arxiv.org/abs/1808.00441|Matrix  completion  and  extrapolation  via  kernel  regression.  (arXiv:1808.00441v2  [stat.ML]  UPDATED)|<p>Matrix  completion  and  extrapolation  (MCEX)  are  dealt  with  here  overreproducing  kernel  Hilbert  spaces  (RKHSs)  in  order  to  account  for  priorinformation  present  in  the  available  data.  Aiming  at  a  faster  andlow-complexity  solver,  the  task  is  formulated  as  a  kernel  ridge  regression.  Theresultant  MCEX  algorithm  can  also  afford  online  implementation,  while  the  classof  kernel  functions  also  encompasses  several  existing  approaches  to  MC  withprior  information.  Numerical  tests  on  synthetic  and  real  datasets  show  that  thenovel  approach  performs  faster  than  widespread  methods  such  as  alternatingleast  squares  (ALS)  or  stochastic  gradient  descent  (SGD),  and  that  the  recoveryerror  is  reduced,  especially  when  dealing  with  noisy  data.</p>
http://arxiv.org/abs/1808.02871|Random  directions  stochastic  approximation  with  deterministic  perturbations.  (arXiv:1808.02871v2  [math.OC]  UPDATED)|<p>We  introduce  deterministic  perturbation  schemes  for  the  recently  proposedrandom  directions  stochastic  approximation  (RDSA)  [17],  and  propose  newfirst-order  and  second-order  algorithms.  In  the  latter  case,  these  are  thefirst  second-order  algorithms  to  incorporate  deterministic  perturbations.  Weshow  that  the  gradient  and/or  Hessian  estimates  in  the  resulting  algorithmswith  deterministic  perturbations  are  asymptotically  unbiased,  so  that  thealgorithms  are  provably  convergent.  Furthermore,  we  derive  convergence  rates  toestablish  the  superiority  of  the  first-order  and  second-order  algorithms,  forthe  special  case  of  a  convex  and  quadratic  optimization  problem,  respectively.Numerical  experiments  are  used  to  validate  the  theoretical  results.</p>
http://arxiv.org/abs/1808.06570|Detecting  cognitive  impairments  by  agreeing  on  interpretations  of  linguistic  features.  (arXiv:1808.06570v3  [cs.CL]  UPDATED)|<p>Linguistic  features  have  shown  promising  applications  for  detecting  variouscognitive  impairments.  To  improve  detection  accuracies,  increasing  the  amountof  data  or  the  number  of  linguistic  features  have  been  two  applicableapproaches.  However,  acquiring  additional  clinical  data  can  be  expensive,  andhand-crafting  features  is  burdensome.  In  this  paper,  we  take  a  third  approach,proposing  Consensus  Networks  (CNs),  a  framework  to  classify  after  reachingagreements  between  modalities.  We  divide  linguistic  features  intonon-overlapping  subsets  according  to  their  modalities,  and  let  neural  networkslearn  low-dimensional  representations  that  agree  with  each  other.  Theserepresentations  are  passed  into  a  classifier  network.  All  neural  networks  areoptimized  iteratively.</p><p>In  this  paper,  we  also  present  two  methods  that  improve  the  performance  ofCNs.  We  then  present  ablation  studies  to  illustrate  the  effectiveness  ofmodality  division.  To  understand  further  what  happens  in  CNs,  we  visualize  therepresentations  during  training.  Overall,  using  all  of  the  413  linguisticfeatures,  our  models  significantly  outperform  traditional  classifiers,  whichare  used  by  the  state-of-the-art  papers.</p>
http://arxiv.org/abs/1808.08765|Identifiability  of  Complete  Dictionary  Learning.  (arXiv:1808.08765v2  [stat.ML]  UPDATED)|<p>Sparse  component  analysis  (SCA),  also  known  as  complete  dictionary  learning,is  the  following  problem:  Given  an  input  matrix  $M$  and  an  integer  $r$,  find  adictionary  $D$  with  $r$  columns  and  a  matrix  $B$  with  $k$-sparse  columns  (thatis,  each  column  of  $B$  has  at  most  $k$  non-zero  entries)  such  that  $M  approxDB$.  A  key  issue  in  SCA  is  identifiability,  that  is,  characterizing  theconditions  under  which  $D$  and  $B$  are  essentially  unique  (that  is,  they  areunique  up  to  permutation  and  scaling  of  the  columns  of  $D$  and  rows  of  $B$).Although  SCA  has  been  vastly  investigated  in  the  last  two  decades,  only  a  fewworks  have  tackled  this  issue  in  the  deterministic  scenario,  and  no  workprovides  reasonable  bounds  in  the  minimum  number  of  samples  (that  is,  columnsof  $M$)  that  leads  to  identifiability.  In  this  work,  we  provide  new  results  inthe  deterministic  scenario  when  the  data  has  a  low-rank  structure,  that  is,when  $D$  is  (under)complete.  While  previous  bounds  feature  a  combinatorial  term$r  choose  k$,  we  exhibit  a  sufficient  condition  involving$mathcal{O}(r^3/(r-k)^2)$  samples  that  yields  an  essentially  uniquedecomposition,  as  long  as  these  data  points  are  well  spread  among  the  subspacesspanned  by  $r-1$  columns  of  $D$.  We  also  exhibit  a  necessary  lower  bound  on  thenumber  of  samples  that  contradicts  previous  results  in  the  literature  when  $k$equals  $r-1$.  Our  bounds  provide  a  drastic  improvement  compared  to  the  state  ofthe  art,  and  imply  for  example  that  for  a  fixed  proportion  of  zeros  (constantand  independent  of  $r$,  e.g.,  10%  of  zero  entries  in  $B$),  one  only  requires$mathcal{O}(r)$  data  points  to  guarantee  identifiability.</p>
http://arxiv.org/abs/1809.00156|Optimization  of  Quantum  Discord  to  Connection  with  Non-commutivity.  (arXiv:1809.00156v9  [quant-ph]  UPDATED)|<p>In  a  pair  of  correlated  quantum  systems,  a  measurement  in  one  corresponds  toa  change  in  the  state  of  the  other.  In  the  process,  information  is  lost.Measurement  along  which  set  of  projectors  would  accompany  minimum  loss  ininformation  content  is  the  optimization  problem  of  quantum  discord.  Thisoptimization  problem  needs  to  be  addressed  in  any  computation  of  discord  and  isalso  an  important  aspect  of  our  understanding  of  any  quantum  to  classicaltransition.  This  asks  us  to  explore  the  standard  zero  discord  condition  to  moveto  a  stronger  measure  that  addresses  the  correlated  observables  of  the  m  x  nmatrix  instead.  In  such  a  context  one  could  show  that  discord  minimizes  at  thediagonal  basis  of  the  reduced  density  matrices  and  present  an  analyticalexpression  of  quantum  discord  that  demonstrates  how  it  arises  out  ofnon-commutivity.</p>
http://arxiv.org/abs/1809.01794|Information-Theoretic  Privacy  For  Distributed  Average  Consensus:  Bounded  Integral  Inputs.  (arXiv:1809.01794v2  [cs.SY]  UPDATED)|<p>We  propose  an  asynchronous  distributed  average  consensus  algorithm  thatguarantees  information-theoretic  privacy  of  honest  agents'  inputs  againstcolluding  passive  adversarial  agents,  as  long  as  the  set  of  colluding  passiveadversarial  agents  is  not  a  vertex  cut  in  the  underlying  communication  network.This  implies  that  a  network  with  $(t+1)$-connectivity  guaranteesinformation-theoretic  privacy  of  honest  agents'  inputs  against  any  $t$colluding  agents.  The  proposed  protocol  is  formed  by  composing  a  distributedprivacy  mechanism  we  provide  with  any  (non-private)  distributed  averageconsensus  algorithm.  The  agent'  inputs  are  bounded  integers,  where  the  boundsare  apriori  known  to  all  the  agents.</p>
http://arxiv.org/abs/1809.08514|Fundamental  Limits  of  Invisible  Flow  Fingerprinting.  (arXiv:1809.08514v3  [cs.NI]  UPDATED)|<p>Network  flow  fingerprinting  can  be  used  to  de-anonymize  communications  onanonymity  systems  such  as  Tor  by  linking  the  ingress  and  egress  segments  ofanonymized  connections.  Assume  Alice  and  Bob  have  access  to  the  input  and  theoutput  links  of  an  anonymous  network,  respectively,  and  they  wish  tocollaboratively  reveal  the  connections  between  the  input  and  the  output  linkswithout  being  detected  by  Willie  who  protects  the  network.  Alice  generates  acodebook  of  fingerprints,  where  each  fingerprint  corresponds  to  a  uniquesequence  of  inter-packet  delays  and  shares  it  only  with  Bob.  For  each  inputflow,  she  selects  a  fingerprint  from  the  codebook  and  embeds  it  in  the  flow,i.e.,  changes  the  packet  timings  of  the  flow  to  follow  the  packet  timingssuggested  by  the  fingerprint,  and  Bob  extracts  the  fingerprints  from  the  outputflows.  We  model  the  network  as  parallel  $M/M/1$  queues  where  each  queue  isshared  by  a  flow  from  Alice  to  Bob  and  other  flows  independent  of  the  flow  fromAlice  to  Bob.  The  timings  of  the  flows  are  governed  by  independent  Poissonpoint  processes.  Assuming  all  input  flows  have  equal  rates  and  that  Bobobserves  only  flows  with  fingerprints,  we  first  present  two  scenarios:  1)  Alicefingerprints  all  the  flows;  2)  Alice  fingerprints  a  subset  of  the  flows,unknown  to  Willie.  Then,  we  extend  the  construction  and  analysis  to  the  casewhere  flow  rates  are  arbitrary  as  well  as  the  case  where  not  all  the  flows  thatBob  observes  have  a  fingerprint.  For  each  scenario,  we  derive  the  number  offlows  that  Alice  can  fingerprint  and  Bob  can  trace  by  fingerprinting.</p>
http://arxiv.org/abs/1809.10243|Segmentation  of  Skin  Lesions  and  their  Attributes  Using  Multi-Scale  Convolutional  Neural  Networks  and  Domain  Specific  Augmentations.  (arXiv:1809.10243v2  [cs.CV]  UPDATED)|<p>Computer-aided  diagnosis  systems  for  classification  of  different  type  of  skinlesions  have  been  an  active  field  of  research  in  recent  decades.  It  has  beenshown  that  introducing  lesions  and  their  attributes  masks  into  lesionclassification  pipeline  can  greatly  improve  the  performance.  In  this  paper,  wepropose  a  framework  by  incorporating  transfer  learning  for  segmenting  lesionsand  their  attributes  based  on  the  convolutional  neural  networks.  The  proposedframework  is  inspired  by  the  well-known  UNet  architecture.  It  utilizes  avariety  of  pre-trained  networks  in  the  encoding  path  and  generates  theprediction  map  by  combining  multi-scale  information  in  decoding  path  using  apyramid  pooling  manner.  To  circumvent  the  lack  of  training  data  and  increasethe  proposed  model  generalization,  an  extensive  set  of  novel  augmentationroutines  have  been  applied  during  the  training  of  the  network.  Moreover,  foreach  task  of  lesion  and  attribute  segmentation,  a  specific  loss  function  hasbeen  designed  to  obviate  the  training  phase  difficulties.  Finally,  theprediction  for  each  task  is  generated  by  ensembling  the  outputs  from  differentmodels.  The  proposed  approach  achieves  promising  results  on  thecross-validation  experiments  on  the  ISIC2018-  Task1  and  Task2  data  sets.</p>
http://arxiv.org/abs/1810.01898|A  Multi-Face  Challenging  Dataset  for  Robust  Face  Recognition.  (arXiv:1810.01898v2  [cs.CV]  UPDATED)|<p>Face  recognition  in  images  is  an  active  area  of  interest  among  the  computervision  researchers.  However,  recognizing  human  face  in  an  unconstrainedenvironment,  is  a  relatively  less-explored  area  of  research.  Multiple  facerecognition  in  unconstrained  environment  is  a  challenging  task,  due  to  thevariation  of  view-point,  scale,  pose,  illumination  and  expression  of  the  faceimages.  Partial  occlusion  of  faces  makes  the  recognition  task  even  morechallenging.  The  contribution  of  this  paper  is  two-folds:  introducing  achallenging  multiface  dataset  (i.e.,  IIITS  MFace  Dataset)  for  face  recognitionin  unconstrained  environment  and  evaluating  the  performance  of  state-of-the-arthand-designed  and  deep  learning  based  face  descriptors  on  the  dataset.  Theproposed  IIITS  MFace  dataset  contains  faces  with  challenges  like  posevariation,  occlusion,  mask,  spectacle,  expressions,  change  of  illumination,etc.  We  experiment  with  several  state-of-the-art  face  descriptors,  includingrecent  deep  learning  based  face  descriptors  like  VGGFace,  and  compare  with  theexisting  benchmark  face  datasets.  Results  of  the  experiments  clearly  show  thatthe  difficulty  level  of  the  proposed  dataset  is  much  higher  compared  to  thebenchmark  datasets.</p>
http://arxiv.org/abs/1810.05947|Robust  Model  Predictive  Control  of  Irrigation  Systems  with  Active  Uncertainty  Learning  and  Data  Analytics.  (arXiv:1810.05947v2  [cs.SY]  UPDATED)|<p>We  develop  a  novel  data-driven  robust  model  predictive  control  (DDRMPC)approach  for  automatic  control  of  irrigation  systems.  The  fundamental  idea  isto  integrate  both  mechanistic  models,  which  describe  dynamics  in  soil  moisturevariations,  and  data-driven  models,  which  characterize  uncertainty  in  forecasterrors  of  evapotranspiration  and  precipitation,  into  a  holistic  systems  controlframework.  To  better  capture  the  support  of  uncertainty  distribution,  we  take  anew  learning-based  approach  by  constructing  uncertainty  sets  from  historicaldata.  For  evapotranspiration  forecast  error,  the  support  vectorclustering-based  uncertainty  set  is  adopted,  which  can  be  conveniently  builtfrom  historical  data.  As  for  precipitation  forecast  errors,  we  analyze  thedependence  of  their  distribution  on  forecast  values,  and  further  design  atailored  uncertainty  set  based  on  the  properties  of  this  type  of  uncertainty.In  this  way,  the  overall  uncertainty  distribution  can  be  elaborately  described,which  finally  contributes  to  rational  and  efficient  control  decisions.  Toassure  the  quality  of  data-driven  uncertainty  sets,  a  training-calibrationscheme  is  used  to  provide  theoretical  performance  guarantees.  A  generalizedaffine  decision  rule  is  adopted  to  obtain  tractable  approximations  of  optimalcontrol  problems,  thereby  ensuring  the  practicability  of  DDRMPC.  Case  studiesusing  real  data  show  that,  DDRMPC  can  reliably  maintain  soil  moisture  above  thesafety  level  and  avoid  crop  devastation.  The  proposed  DDRMPC  approach  leads  toa  40%  reduction  of  total  water  consumption  compared  to  the  fine-tunedopen-loop  control  strategy.  In  comparison  with  the  carefully  tuned  rule-basedcontrol  and  certainty  equivalent  model  predictive  control,  the  proposed  DDRMPCapproach  can  significantly  reduce  the  total  water  consumption  and  improve  thecontrol  performance.</p>
http://arxiv.org/abs/1811.03433|Explainable  cardiac  pathology  classification  on  cine  MRI  with  motion  characterization  by  semi-supervised  learning  of  apparent  flow.  (arXiv:1811.03433v2  [cs.CV]  UPDATED)|<p>We  propose  a  method  to  classify  cardiac  pathology  based  on  a  novel  approachto  extract  image  derived  features  to  characterize  the  shape  and  motion  of  theheart.  An  original  semi-supervised  learning  procedure,  which  makes  efficientuse  of  a  large  amount  of  non-segmented  images  and  a  small  amount  of  imagessegmented  manually  by  experts,  is  developed  to  generate  pixel-wise  apparentflow  between  two  time  points  of  a  2D+t  cine  MRI  image  sequence.  Combining  theapparent  flow  maps  and  cardiac  segmentation  masks,  we  obtain  a  local  apparentflow  corresponding  to  the  2D  motion  of  myocardium  and  ventricular  cavities.This  leads  to  the  generation  of  time  series  of  the  radius  and  thickness  ofmyocardial  segments  to  represent  cardiac  motion.  These  time  series  of  motionfeatures  are  reliable  and  explainable  characteristics  of  pathological  cardiacmotion.  Furthermore,  they  are  combined  with  shape-related  features  to  classifycardiac  pathologies.  Using  only  nine  feature  values  as  input,  we  propose  anexplainable,  simple  and  flexible  model  for  pathology  classification.  On  ACDCtraining  set  and  testing  set,  the  model  achieves  95%  and  94%  respectively  asclassification  accuracy.  Its  performance  is  hence  comparable  to  that  of  thestate-of-the-art.  Comparison  with  various  other  models  is  performed  to  outlinesome  advantages  of  our  model.</p>
http://arxiv.org/abs/1811.08383|TSM:  Temporal  Shift  Module  for  Efficient  Video  Understanding.  (arXiv:1811.08383v2  [cs.CV]  UPDATED)|<p>The  explosive  growth  in  video  streaming  gives  rise  to  challenges  onefficiently  extracting  the  spatial-temporal  information  to  perform  videounderstanding  at  low  computation  cost.  Conventional  2D  CNNs  are  computationallycheap  but  cannot  capture  temporal  relationships;  3D  CNN  based  methods  canachieve  good  performance  but  are  computationally  intensive,  making  it  expensiveto  deploy.  In  this  paper,  we  propose  a  generic  and  effective  Temporal  ShiftModule  (TSM)  that  enjoys  both  high  efficiency  and  high  performance.Specifically,  it  can  achieve  the  performance  of  3D  CNN  but  maintain  2D  CNN'scomplexity.  TSM  shifts  part  of  the  channels  along  the  temporal  dimension,  thusfacilitate  information  exchanged  among  neighboring  frames.  It  can  be  insertedinto  2D  CNNs  to  achieve  temporal  modeling  at  zero  computation  and  zeroparameters.  We  also  extended  TSM  to  online  video  recognition  setting,  whichenables  real-time  low-latency  online  video  recognition.  On  theSomething-Something-V1  dataset  which  focuses  on  temporal  modeling,  we  achievedbetter  results  than  I3D  family  and  ECO  family  using  6X  and  2.7X  fewer  FLOPsrespectively.  Measured  on  P100  GPU,  our  single  model  achieved  1.8%  higheraccuracy  at  9.5X  lower  latency  and  12.7X  higher  throughput  compared  to  I3D.  Thecode  is  available  here:  https://github.com/MIT-HAN-LAB/temporal-shift-module.</p>
http://arxiv.org/abs/1811.12019|Large-Scale  Distributed  Second-Order  Optimization  Using  Kronecker-Factored  Approximate  Curvature  for  Deep  Convolutional  Neural  Networks.  (arXiv:1811.12019v4  [cs.LG]  UPDATED)|<p>Large-scale  distributed  training  of  deep  neural  networks  suffer  from  thegeneralization  gap  caused  by  the  increase  in  the  effective  mini-batch  size.Previous  approaches  try  to  solve  this  problem  by  varying  the  learning  rate  andbatch  size  over  epochs  and  layers,  or  some  ad  hoc  modification  of  the  batchnormalization.  We  propose  an  alternative  approach  using  a  second-orderoptimization  method  that  shows  similar  generalization  capability  to  first-ordermethods,  but  converges  faster  and  can  handle  larger  mini-batches.  To  test  ourmethod  on  a  benchmark  where  highly  optimized  first-order  methods  are  availableas  references,  we  train  ResNet-50  on  ImageNet.  We  converged  to  75%  Top-1validation  accuracy  in  35  epochs  for  mini-batch  sizes  under  16,384,  andachieved  75%  even  with  a  mini-batch  size  of  131,072,  which  took  only  978iterations.</p>
http://arxiv.org/abs/1812.00020|TextureNet:  Consistent  Local  Parametrizations  for  Learning  from  High-Resolution  Signals  on  Meshes.  (arXiv:1812.00020v2  [cs.CV]  UPDATED)|<p>We  introduce,  TextureNet,  a  neural  network  architecture  designed  to  extractfeatures  from  high-resolution  signals  associated  with  3D  surface  meshes  (e.g.,color  texture  maps).  The  key  idea  is  to  utilize  a  4-rotational  symmetric(4-RoSy)  field  to  define  a  domain  for  convolution  on  a  surface.  Though  4-RoSyfields  have  several  properties  favorable  for  convolution  on  surfaces  (lowdistortion,  few  singularities,  consistent  parameterization,  etc.),  orientationsare  ambiguous  up  to  4-fold  rotation  at  any  sample  point.  So,  we  introduce  a  newconvolutional  operator  invariant  to  the  4-RoSy  ambiguity  and  use  it  in  anetwork  to  extract  features  from  high-resolution  signals  on  geodesicneighborhoods  of  a  surface.  In  comparison  to  alternatives,  such  as  PointNetbased  methods  which  lack  a  notion  of  orientation,  the  coherent  structure  givenby  these  neighborhoods  results  in  significantly  stronger  features.  As  anexample  application,  we  demonstrate  the  benefits  of  our  architecture  for  3Dsemantic  segmentation  of  textured  3D  meshes.  The  results  show  that  our  methodoutperforms  all  existing  methods  on  the  basis  of  mean  IoU  by  a  significantmargin  in  both  geometry-only  (6.4%)  and  RGB+Geometry  (6.9-8.2%)  settings.</p>
http://arxiv.org/abs/1812.00876|Deep  Learning  based  Pedestrian  Detection  at  Distance  in  Smart  Cities.  (arXiv:1812.00876v3  [cs.CV]  UPDATED)|<p>Generative  adversarial  networks  (GANs)  have  been  promising  for  many  computervision  problems  due  to  their  powerful  capabilities  to  enhance  the  data  fortraining  and  test.  In  this  paper,  we  leveraged  GANs  and  proposed  a  newarchitecture  with  a  cascaded  Single  Shot  Detector  (SSD)  for  pedestriandetection  at  distance,  which  is  yet  a  challenge  due  to  the  varied  sizes  ofpedestrians  in  videos  at  distance.  To  overcome  the  low-resolution  issues  inpedestrian  detection  at  distance,  DCGAN  is  employed  to  improve  the  resolutionfirst  to  reconstruct  more  discriminative  features  for  a  SSD  to  detect  objectsin  images  or  videos.  A  crucial  advantage  of  our  method  is  that  it  learns  amulti-scale  metric  to  distinguish  multiple  objects  at  different  distances  underone  image,  while  DCGAN  serves  as  an  encoder-decoder  platform  to  generate  partsof  an  image  that  contain  better  discriminative  information.  To  measure  theeffectiveness  of  our  proposed  method,  experiments  were  carried  out  on  theCanadian  Institute  for  Advanced  Research  (CIFAR)  dataset,  and  it  wasdemonstrated  that  the  proposed  new  architecture  achieved  a  much  betterdetection  rate,  particularly  on  vehicles  and  pedestrians  at  distance,  making  ithighly  suitable  for  smart  cities  applications  that  need  to  discover  key  objectsor  pedestrians  at  distance.</p>
http://arxiv.org/abs/1812.02222|Predicting  pregnancy  using  large-scale  data  from  a  women's  health  tracking  mobile  application.  (arXiv:1812.02222v2  [stat.AP]  UPDATED)|<p>Predicting  pregnancy  has  been  a  fundamental  problem  in  women's  health  formore  than  50  years.  Previous  datasets  have  been  collected  via  carefully  curatedmedical  studies,  but  the  recent  growth  of  women's  health  tracking  mobile  appsoffers  potential  for  reaching  a  much  broader  population.  However,  thefeasibility  of  predicting  pregnancy  from  mobile  health  tracking  data  isunclear.  Here  we  develop  four  models  --  a  logistic  regression  model,  and  3  LSTMmodels  --  to  predict  a  woman's  probability  of  becoming  pregnant  using  data  froma  women's  health  tracking  app,  Clue  by  BioWink  GmbH.  Evaluating  our  models  on  adataset  of  79  million  logs  from  65,276  women  with  ground  truth  pregnancy  testdata,  we  show  that  our  predicted  pregnancy  probabilities  meaningfully  stratifywomen:  women  in  the  top  10%  of  predicted  probabilities  have  a  89%  chance  ofbecoming  pregnant  over  6  menstrual  cycles,  as  compared  to  a  27%  chance  forwomen  in  the  bottom  10%.  We  develop  a  technique  for  extracting  interpretabletime  trends  from  our  deep  learning  models,  and  show  these  trends  are  consistentwith  previous  fertility  research.  Our  findings  illustrate  the  potential  thatwomen's  health  tracking  data  offers  for  predicting  pregnancy  on  a  broaderpopulation;  we  conclude  by  discussing  the  steps  needed  to  fulfill  thispotential.</p>
http://arxiv.org/abs/1812.02347|Counterfactual  Critic  Multi-Agent  Training  for  Scene  Graph  Generation.  (arXiv:1812.02347v2  [cs.CV]  UPDATED)|"<p>Scene  graphs  --  objects  as  nodes  and  visual  relationships  as  edges  --describe  the  whereabouts  and  interactions  of  the  things  and  stuff  in  an  imagefor  comprehensive  scene  understanding.  To  generate  coherent  scene  graphs,almost  all  existing  methods  exploit  the  fruitful  visual  context  by  modelingmessage  passing  among  objects,  fitting  the  dynamic  nature  of  reasoning  withvisual  context,  eg,  ""person""  on  ""bike""  can  help  determine  the  relationship""ride"",  which  in  turn  contributes  to  the  category  confidence  of  the  twoobjects.  However,  we  argue  that  the  scene  dynamics  is  not  properly  learned  byusing  the  prevailing  cross-entropy  based  supervised  learning  paradigm,  which  isnot  sensitive  to  graph  inconsistency:  errors  at  the  hub  or  non-hub  nodes  areunfortunately  penalized  equally.  To  this  end,  we  propose  a  Counterfactualcritic  Multi-Agent  Training  (CMAT)  approach  to  resolve  the  mismatch.  CMAT  is  amulti-agent  policy  gradient  method  that  frames  objects  as  cooperative  agents,and  then  directly  maximizes  a  graph-level  metric  as  the  reward.  In  particular,to  assign  the  reward  properly  to  each  agent,  CMAT  uses  a  counterfactualbaseline  that  disentangles  the  agent-specific  reward  by  fixing  the  dynamics  ofother  agents.  Extensive  validations  on  the  challenging  Visual  Genome  benchmarkshow  that  CMAT  achieves  a  state-of-the-art  by  significant  performance  gainsunder  various  settings  and  metrics.</p>"
http://arxiv.org/abs/1812.03831|On  the  Enumeration  Complexity  of  Unions  of  Conjunctive  Queries.  (arXiv:1812.03831v3  [cs.DB]  UPDATED)|<p>We  study  the  enumeration  complexity  of  Unions  of  Conjunctive  Queries  (UCQs).We  aim  to  identify  the  UCQs  that  are  tractable  in  the  sense  that  the  answertuples  can  be  enumerated  with  a  linear  preprocessing  phase  and  a  constant  delaybetween  every  successive  tuples.  It  has  been  established  that,  in  the  absenceof  self  joins  and  under  conventional  complexity  assumptions,  the  CQs  that  admitsuch  an  evaluation  are  precisely  the  free-connex  ones.  A  union  of  tractable  CQsis  always  tractable.  We  generalize  the  notion  of  free-connexity  from  CQs  toUCQs,  thus  showing  that  some  unions  containing  intractable  CQs  are,  in  fact,tractable.  Interestingly,  some  unions  consisting  of  only  intractable  CQs  aretractable  too.  The  question  of  a  finding  a  full  characterization  of  thetractability  of  UCQs  remains  open.  Nevertheless,  we  prove  that  for  severalclasses  of  queries,  free-connexity  fully  captures  the  tractable  UCQs.</p>
http://arxiv.org/abs/1812.08252|Towards  an  Evolvable  Cancer  Treatment  Simulator.  (arXiv:1812.08252v2  [cs.NE]  UPDATED)|<p>The  use  of  high-fidelity  computational  simulations  promises  to  enablehigh-throughput  hypothesis  testing  and  optimisation  of  cancer  therapies.However,  increasing  realism  comes  at  the  cost  of  increasing  computationalrequirements.  This  article  explores  the  use  of  surrogate-assisted  evolutionaryalgorithms  to  optimise  the  targeted  delivery  of  a  therapeutic  compound  tocancerous  tumour  cells  with  the  multicellular  simulator,  PhysiCell.  The  use  ofboth  Gaussian  process  models  and  multi-layer  perceptron  neural  networksurrogate  models  are  investigated.  We  find  that  evolutionary  algorithms  areable  to  effectively  explore  the  parameter  space  of  biophysical  propertieswithin  the  agent-based  simulations,  minimising  the  resulting  number  ofcancerous  cells  after  a  period  of  simulated  treatment.  Both  model-assistedalgorithms  are  found  to  outperform  a  standard  evolutionary  algorithm,demonstrating  their  ability  to  perform  a  more  effective  search  within  the  verysmall  evaluation  budget.  This  represents  the  first  use  of  efficientevolutionary  algorithms  within  a  high-throughput  multicellular  computingapproach  to  find  therapeutic  design  optima  that  maximise  tumour  regression.</p>
http://arxiv.org/abs/1812.10085|A  Data-driven  Adversarial  Examples  Recognition  Framework  via  Adversarial  Feature  Genome.  (arXiv:1812.10085v2  [cs.CV]  UPDATED)|<p>Convolutional  neural  networks  (CNNs)  are  easily  spoofed  by  adversarialexamples  which  lead  to  wrong  classification  results.  Most  of  the  defensemethods  focus  only  on  how  to  improve  the  robustness  of  CNNs  or  to  detectadversarial  examples.  They  are  incapable  of  detecting  and  correctly  classifyingadversarial  examples  simultaneously.  We  find  that  adversarial  examples  andoriginal  images  have  diverse  representations  in  the  feature  space,  and  thisdifference  grows  as  layers  go  deeper,  which  we  call  Adversarial  FeatureSeparability  (AFS).  Inspired  by  AFS,  we  propose  a  defense  framework  based  onAdversarial  Feature  Genome  (AFG),  which  can  detect  and  correctly  classifyadversarial  examples  into  original  classes  simultaneously.  AFG  is  an  innovativeencoding  for  both  image  and  adversarial  example.  It  consists  of  group  featuresand  a  mixed  label.  With  group  features  which  are  visual  representations  ofadversarial  and  original  images  via  group  visualization  method,  one  can  detectadversarial  examples  because  of  ASF  of  group  features.  With  a  mixed  label,  onecan  trace  back  to  the  original  label  of  an  adversarial  example.  Then,  theclassification  of  adversarial  example  is  modeled  as  a  multi-labelclassification  trained  on  the  AFG  dataset,  which  can  get  the  original  class  ofadversarial  example.  Experiments  show  that  the  proposed  framework  not  onlyeffectively  detects  adversarial  examples  from  different  attack  algorithms,  butalso  correctly  classifies  adversarial  examples.  Our  framework  potentially  givesa  new  perspective,  i.e.,  a  data-driven  way,  to  improve  the  robustness  of  a  CNNmodel.</p>
http://arxiv.org/abs/1812.11448|Removing  Malicious  Nodes  from  Networks.  (arXiv:1812.11448v5  [cs.SI]  UPDATED)|<p>A  fundamental  challenge  in  networked  systems  is  detection  and  removal  ofsuspected  malicious  nodes.  In  reality,  detection  is  always  imperfect,  and  thedecision  about  which  potentially  malicious  nodes  to  remove  must  trade  off  falsepositives  (erroneously  removing  benign  nodes)  and  false  negatives  (mistakenlyfailing  to  remove  malicious  nodes).  However,  in  network  settings  thisconventional  tradeoff  must  now  account  for  node  connectivity.  In  particular,malicious  nodes  may  exert  malicious  influence,  so  that  mistakenly  leaving  someof  these  in  the  network  may  cause  damage  to  spread.  On  the  other  hand,  removingbenign  nodes  causes  direct  harm  to  these,  and  indirect  harm  to  their  benignneighbors  who  would  wish  to  communicate  with  them.</p><p>We  formalize  the  problem  of  removing  potentially  malicious  nodes  from  anetwork  under  uncertainty  through  an  objective  that  takes  connectivity  intoaccount.  We  show  that  optimally  solving  the  resulting  problem  is  NP-Hard.  Wethen  propose  a  tractable  solution  approach  based  on  a  convex  relaxation  of  theobjective.  Finally,  we  experimentally  demonstrate  that  our  approachsignificantly  outperforms  both  a  simple  baseline  that  ignores  networkstructure,  as  well  as  a  state-of-the-art  approach  for  a  related  problem,  onboth  synthetic  and  real-world  datasets.</p>
http://arxiv.org/abs/1901.00434|The  capacity  of  feedforward  neural  networks.  (arXiv:1901.00434v2  [cs.LG]  UPDATED)|"<p>A  long  standing  open  problem  in  the  theory  of  neural  networks  is  thedevelopment  of  quantitative  methods  to  estimate  and  compare  the  capabilities  ofdifferent  architectures.  Here  we  define  the  capacity  of  an  architecture  by  thebinary  logarithm  of  the  number  of  functions  it  can  compute,  as  the  synapticweights  are  varied.  The  capacity  provides  an  upper  bound  on  the  number  of  bitsthat  can  be  extracted  from  the  training  data  and  stored  in  the  architectureduring  learning.  We  study  the  capacity  of  layered,  fully-connected,architectures  of  linear  threshold  neurons  with  $L$  layers  of  size  $n_1,n_2,ldots,  n_L$  and  show  that  in  essence  the  capacity  is  given  by  a  cubicpolynomial  in  the  layer  sizes:  $C(n_1,ldots,  n_L)=sum_{k=1}^{L-1}min(n_1,ldots,n_k)n_kn_{k+1}$,  where  layers  that  are  smaller  than  allprevious  layers  act  as  bottlenecks.  In  proving  the  main  result,  we  also  developnew  techniques  (multiplexing,  enrichment,  and  stacking)  as  well  as  new  boundson  the  capacity  of  finite  sets.  We  use  the  main  result  to  identifyarchitectures  with  maximal  or  minimal  capacity  under  a  number  of  naturalconstraints.  This  leads  to  the  notion  of  structural  regularization  for  deeparchitectures.  While  in  general,  everything  else  being  equal,  shallow  networkscompute  more  functions  than  deep  networks,  the  functions  computed  by  deepnetworks  are  more  regular  and  ""interesting"".</p>"
http://arxiv.org/abs/1901.01660|Deeper  and  Wider  Siamese  Networks  for  Real-Time  Visual  Tracking.  (arXiv:1901.01660v3  [cs.CV]  UPDATED)|<p>Siamese  networks  have  drawn  great  attention  in  visual  tracking  because  oftheir  balanced  accuracy  and  speed.  However,  the  backbone  networks  used  inSiamese  trackers  are  relatively  shallow,  such  as  AlexNet  [18],  which  does  notfully  take  advantage  of  the  capability  of  modern  deep  neural  networks.  In  thispaper,  we  investigate  how  to  leverage  deeper  and  wider  convolutional  neuralnetworks  to  enhance  tracking  robustness  and  accuracy.  We  observe  that  directreplacement  of  backbones  with  existing  powerful  architectures,  such  as  ResNet[14]  and  Inception  [33],  does  not  bring  improvements.  The  main  reasons  are  that1)large  increases  in  the  receptive  field  of  neurons  lead  to  reduced  featurediscriminability  and  localization  precision;  and  2)  the  network  padding  forconvolutions  induces  a  positional  bias  in  learning.  To  address  these  issues,  wepropose  new  residual  modules  to  eliminate  the  negative  impact  of  padding,  andfurther  design  new  architectures  using  these  modules  with  controlled  receptivefield  size  and  network  stride.  The  designed  architectures  are  lightweight  andguarantee  real-time  tracking  speed  when  applied  to  SiamFC  [2]  and  SiamRPN  [20].Experiments  show  that  solely  due  to  the  proposed  network  architectures,  ourSiamFC+  and  SiamRPN+  obtain  up  to  9.8%/5.7%  (AUC),  23.3%/8.8%  (EAO)  and24.4%/25.0%  (EAO)  relative  improvements  over  the  original  versions  [2,  20]  onthe  OTB-15,  VOT-16  and  VOT-17  datasets,  respectively.</p>
http://arxiv.org/abs/1901.01894|Resilient  Design  of  5G  Mobile-Edge  Computing  Over  Intermittent  mmWave  Links.  (arXiv:1901.01894v3  [cs.IT]  UPDATED)|<p>Two  enablers  of  the  5th  Generation  (5G)  of  mobile  communication  systems  arethe  high  data  rates  achievable  with  millimeter-wave  radio  signals  and  thecloudification  of  the  network's  mobile  edge,  made  possible  also  by  Multi-accessEdge  Computing  (MEC).  In  5G  networks,  user  devices  may  exploit  the  highcapacity  of  their  mobile  connection  and  the  computing  capabilities  of  the  edgecloud  to  offload  computational  tasks  to  MEC  servers,  which  run  applications  ondevices'  behalf.  This  paper  investigates  new  methods  to  perform  power-  andlatency-constrained  offloading.  First,  aiming  to  minimize  user  devices'transmit  power,  the  opportunity  to  exploit  concurrent  communication  linksbetween  the  device  and  the  edge  cloud  is  studied.  The  optimal  number  ofchannels  for  simultaneous  transmission  is  characterized  in  a  deterministic  anda  probabilistic  scenario.  Subsequently,  blocking  events  that  obstructmillimeter-wave  channels  making  them  `intermittent'  are  considered.  Resourceoverprovisioning  and  error-correcting  codes  against  asymmetric  block  erasuresare  proposed  to  jointly  contrast  blocking  and  exploit  multi-linkcommunications'  diversity.  The  asymmetric  block-erasure  channel  ischaracterized  by  a  study  of  its  outage  probability.  The  analysis  is  performedin  a  framework  that  yields  closed-form  expressions.  These,  together  withcorroborating  numerical  results,  are  intended  to  provide  reference  points  andbounds  to  optimal  performance  in  practical  applications.</p>
http://arxiv.org/abs/1901.02360|Sum-of-square-of-rational-function  based  representations  of  positive  semidefinite  polynomial  matrices.  (arXiv:1901.02360v2  [math.OC]  UPDATED)|"<p>The  paper  proves  sum-of-square-of-rational-function  based  representations(shortly,  sosrf-based  representations)  of  polynomial  matrices  that  are  positivesemidefinite  on  some  special  sets:  $mathbb{R}^n;$  $mathbb{R}$  and  itsintervals  $[a,b]$,  $[0,infty)$;  and  the  strips  $[a,b]  	imes  mathbb{R}subset  mathbb{R}^2.$  A  method  for  numerically  computing  such  representationsis  also  presented.  The  methodology  is  divided  into  two  stages:</p><p>(S1)  diagonalizing  the  initial  polynomial  matrix  based  on  the  Schm""{u}dgen'sprocedure  cite{Schmudgen09};</p><p>(S2)  for  each  diagonal  element  of  the  resulting  matrix,  find  its  low  ranksosrf-representation  satisfying  the  Artin's  theorem  solving  the  Hilbert's  17thproblem.</p><p>Some  numerical  tests  and  illustrations  with  	extsf{OCTAVE}  are  alsopresented  for  each  type  of  polynomial  matrices.</p>"
http://arxiv.org/abs/1901.04240|Semi-supervised  Learning  with  Graphs:  Covariance  Based  Superpixels  For  Hyperspectral  Image  Classification.  (arXiv:1901.04240v3  [cs.CV]  UPDATED)|<p>In  this  paper,  we  present  a  graph-based  semi-supervised  framework  forhyperspectral  image  classification.  We  first  introduce  a  novel  superpixelalgorithm  based  on  the  spectral  covariance  matrix  representation  of  pixels  toprovide  a  better  representation  of  our  data.  We  then  construct  a  superpixelgraph,  based  on  carefully  considered  feature  vectors,  before  performingclassification.  We  demonstrate,  through  a  set  of  experimental  results  using  twobenchmarking  datasets,  that  our  approach  outperforms  three  state-of-the-artclassification  frameworks,  especially  when  an  extremely  small  amount  oflabelled  data  is  used.</p>
http://arxiv.org/abs/1901.06523|Frequency  Principle:  Fourier  Analysis  Sheds  Light  on  Deep  Neural  Networks.  (arXiv:1901.06523v3  [cs.LG]  UPDATED)|<p>We  study  the  training  process  of  Deep  Neural  Networks  (DNNs)  from  the  Fourieranalysis  perspective.  Our  starting  point  is  a  Frequency  Principle  (F-Principle)---  DNNs  initialized  with  small  parameters  often  fit  target  functions  from  lowto  high  frequencies  ---  which  was  first  proposed  by  Xu  et  al.  (2018)  andRahaman  et  al.  (2018)  on  synthetic  datasets.  In  this  work,  we  first  show  theuniversality  of  the  F-Principle  by  demonstrating  this  phenomenon  onhigh-dimensional  benchmark  datasets,  such  as  MNIST  and  CIFAR10.  Then,  based  onexperiments,  we  show  that  the  F-Principle  provides  insight  into  both  thesuccess  and  failure  of  DNNs  in  different  types  of  problems.  Based  on  theF-Principle,  we  further  propose  that  DNN  can  be  adopted  to  accelerate  theconvergence  of  low  frequencies  for  scientific  computing  problems,  in  which  mostof  the  conventional  methods  (e.g.,  Jacobi  method)  exhibit  the  oppositeconvergence  behavior  ---  faster  convergence  for  higher  frequencies.  Finally,  weprove  a  theorem  for  DNNs  of  one  hidden  layer  as  a  first  step  towards  amathematical  explanation  of  the  F-Principle.  Our  work  indicates  that  theF-Principle  with  Fourier  analysis  is  a  promising  approach  to  the  study  of  DNNsbecause  it  seems  ubiquitous,  applicable,  and  explainable.</p>
http://arxiv.org/abs/1901.06731|Four  Deviations  Suffice  for  Rank  1  Matrices.  (arXiv:1901.06731v2  [math.CO]  UPDATED)|"<p>We  prove  a  matrix  discrepancy  bound  that  strengthens  the  famousKadison-Singer  result  of  Marcus,  Spielman,  and  Srivastava.  Consider  anyindependent  scalar  random  variables  $xi_1,  ldots,  xi_n$  with  finite  support,e.g.</p><p>${  pm  1  }$  or  ${  0,1  }$-valued  random  variables,  or  some  combinationthereof.  Let  $u_1,  dots,  u_n  in  mathbb{C}^m$  and  $$  sigma^2  =  left |sum_{i=1}^n  	ext{Var}[  xi_i  ]  (u_i  u_i^{*})^2  ight |.  $$  Then  there  existsa  choice  of  outcomes  $varepsilon_1,ldots,varepsilon_n$  in  the  support  of$xi_1,  ldots,  xi_n$  s.t.  $$  left   |sum_{i=1}^n  mathbb{E}  [  xi_i]  u_iu_i^*  -  sum_{i=1}^n  varepsilon_i  u_i  u_i^*  ight   |  leq  4  sigma.  $$  Asimple  consequence  of  our  result  is  an  improvement  of  a  Lyapunov-type  theoremof  Akemann  and  Weaver.</p>"
http://arxiv.org/abs/1901.07298|Online  Estimation  of  Multiple  Dynamic  Graphs  in  Pattern  Sequences.  (arXiv:1901.07298v2  [stat.ML]  UPDATED)|<p>Sequences  of  correlated  binary  patterns  can  represent  many  time-series  dataincluding  text,  movies,  and  biological  signals.  These  patterns  may  be  describedby  weighted  combinations  of  a  few  dominant  structures  that  underpin  specificinteractions  among  the  binary  elements.  To  extract  the  dominant  correlationstructures  and  their  contributions  to  generating  data  in  a  time-dependentmanner,  we  model  the  dynamics  of  binary  patterns  using  the  state-space  model  ofan  Ising-type  network  that  is  composed  of  multiple  undirected  graphs.  Weprovide  a  sequential  Bayes  algorithm  to  estimate  the  dynamics  of  weights  on  thegraphs  while  gaining  the  graph  structures  online.  This  model  can  uncoveroverlapping  graphs  underlying  the  data  better  than  a  traditional  orthogonaldecomposition  method,  and  outperforms  an  original  time-dependent  Ising  model.We  assess  the  performance  of  the  method  by  simulated  data,  and  demonstrate  thatspontaneous  activity  of  cultured  hippocampal  neurons  is  represented  by  dynamicsof  multiple  graphs.</p>
http://arxiv.org/abs/1901.08991|Diffusion  Variational  Autoencoders.  (arXiv:1901.08991v2  [cs.LG]  UPDATED)|<p>A  standard  Variational  Autoencoder,  with  a  Euclidean  latent  space,  isstructurally  incapable  of  capturing  topological  properties  of  certain  datasets.To  remove  topological  obstructions,  we  introduce  Diffusion  VariationalAutoencoders  with  arbitrary  manifolds  as  a  latent  space.  A  DiffusionVariational  Autoencoder  uses  transition  kernels  of  Brownian  motion  on  themanifold.  In  particular,  it  uses  properties  of  the  Brownian  motion  to  implementthe  reparametrization  trick  and  fast  approximations  to  the  KL  divergence.  Weshow  that  the  Diffusion  Variational  Autoencoder  is  capable  of  capturingtopological  properties  of  synthetic  datasets.  Additionally,  we  train  MNIST  onspheres,  tori,  projective  spaces,  SO(3),  and  a  torus  embedded  in  R3.  Although  anatural  dataset  like  MNIST  does  not  have  latent  variables  with  a  clear-cuttopological  structure,  training  it  on  a  manifold  can  still  highlighttopological  and  geometrical  properties.</p>
http://arxiv.org/abs/1901.09221|Progressive  Image  Deraining  Networks:  A  Better  and  Simpler  Baseline.  (arXiv:1901.09221v2  [cs.CV]  UPDATED)|<p>Along  with  the  deraining  performance  improvement  of  deep  networks,  theirstructures  and  learning  become  more  and  more  complicated  and  diverse,  making  itdifficult  to  analyze  the  contribution  of  various  network  modules  whendeveloping  new  deraining  networks.  To  handle  this  issue,  this  paper  provides  abetter  and  simpler  baseline  deraining  network  by  considering  networkarchitecture,  input  and  output,  and  loss  functions.  Specifically,  by  repeatedlyunfolding  a  shallow  ResNet,  progressive  ResNet  (PRN)  is  proposed  to  takeadvantage  of  recursive  computation.  A  recurrent  layer  is  further  introduced  toexploit  the  dependencies  of  deep  features  across  stages,  forming  ourprogressive  recurrent  network  (PReNet).  Furthermore,  intra-stage  recursivecomputation  of  ResNet  can  be  adopted  in  PRN  and  PReNet  to  notably  reducenetwork  parameters  with  graceful  degradation  in  deraining  performance.  Fornetwork  input  and  output,  we  take  both  stage-wise  result  and  original  rainyimage  as  input  to  each  ResNet  and  finally  output  the  prediction  of  {residualimage}.  As  for  loss  functions,  single  MSE  or  negative  SSIM  losses  aresufficient  to  train  PRN  and  PReNet.  Experiments  show  that  PRN  and  PReNetperform  favorably  on  both  synthetic  and  real  rainy  images.  Considering  itssimplicity,  efficiency  and  effectiveness,  our  models  are  expected  to  serve  as  asuitable  baseline  in  future  deraining  research.  The  source  codes  are  availableat  https://github.com/csdwren/PReNet.</p>
http://arxiv.org/abs/1901.10593|Decentralized  Online  Learning:  Take  Benefits  from  Others'  Data  without  Sharing  Your  Own  to  Track  Global  Trend.  (arXiv:1901.10593v3  [cs.LG]  UPDATED)|<p>Decentralized  Online  Learning  (online  learning  in  decentralized  networks)attracts  more  and  more  attention,  since  it  is  believed  that  DecentralizedOnline  Learning  can  help  the  data  providers  cooperatively  better  solve  theironline  problems  without  sharing  their  private  data  to  a  third  party  or  otherproviders.  Typically,  the  cooperation  is  achieved  by  letting  the  data  providersexchange  their  models  between  neighbors,  e.g.,  recommendation  model.  However,the  best  regret  bound  for  a  decentralized  online  learning  algorithm  is$Ocal{nsqrt{T}}$,  where  $n$  is  the  number  of  nodes  (or  users)  and  $T$  is  thenumber  of  iterations.  This  is  clearly  insignificant  since  this  bound  can  beachieved  emph{without}  any  communication  in  the  networks.  This  reminds  us  toask  a  fundamental  question:  emph{Can  people  really  get  benefit  from  thedecentralized  online  learning  by  exchanging  information?}  In  this  paper,  westudied  when  and  why  the  communication  can  help  the  decentralized  onlinelearning  to  reduce  the  regret.  Specifically,  each  loss  function  ischaracterized  by  two  components:  the  adversarial  component  and  the  stochasticcomponent.  Under  this  characterization,  we  show  that  decentralized  onlinegradient  (DOG)  enjoys  a  regret  bound  $Ocal{nsqrt{T}G  +  sqrt{nT}sigma}$,where  $G$  measures  the  magnitude  of  the  adversarial  component  in  the  privatedata  (or  equivalently  the  local  loss  function)  and  $sigma$  measures  therandomness  within  the  private  data.  This  regret  suggests  that  people  can  getbenefits  from  the  randomness  in  the  private  data  by  exchanging  privateinformation.  Another  important  contribution  of  this  paper  is  to  consider  thedynamic  regret  --  a  more  practical  regret  to  track  users'  interest  dynamics.Empirical  studies  are  also  conducted  to  validate  our  analysis.</p>
http://arxiv.org/abs/1901.10915|Benchmarking  Classic  and  Learned  Navigation  in  Complex  3D  Environments.  (arXiv:1901.10915v2  [cs.CV]  UPDATED)|<p>Navigation  research  is  attracting  renewed  interest  with  the  advent  oflearning-based  methods.  However,  this  new  line  of  work  is  largely  disconnectedfrom  well-established  classic  navigation  approaches.  In  this  paper,  we  take  astep  towards  coordinating  these  two  directions  of  research.  We  set  up  classicand  learning-based  navigation  systems  in  common  simulated  environments  andthoroughly  evaluate  them  in  indoor  spaces  of  varying  complexity,  with  access  todifferent  sensory  modalities.  Additionally,  we  measure  human  performance  in  thesame  environments.  We  find  that  a  classic  pipeline,  when  properly  tuned,  canperform  very  well  in  complex  cluttered  environments.  On  the  other  hand,  learnedsystems  can  operate  more  robustly  with  a  limited  sensor  suite.  Overall,  bothapproaches  are  still  far  from  human-level  performance.</p>
http://arxiv.org/abs/1902.03025|Parameterized  Analysis  of  Immediate  Observation  Petri  Nets.  (arXiv:1902.03025v2  [cs.LO]  UPDATED)|<p>We  introduce  immediate  observation  Petri  nets,  a  class  of  interest  in  thestudy  of  population  protocols  (a  model  of  distributed  computation),  andenzymatic  chemical  networks.  In  these  areas,  relevant  analysis  questionstranslate  into  parameterized  Petri  net  problems:  whether  an  infinite  set  ofPetri  nets  with  the  same  underlying  net,  but  different  initial  markings,satisfy  a  given  property.  We  study  the  parameterized  reachability,coverability,  and  liveness  problems  for  immediate  observation  Petri  nets.  Weshow  that  all  three  problems  are  in  PSPACE  for  infinite  sets  of  initialmarkings  defined  by  counting  constraints,  a  class  sufficiently  rich  for  theintended  application.  This  is  remarkable,  since  the  problems  are  alreadyPSPACE-hard  when  the  set  of  markings  is  a  singleton,  i.e.,  in  thenon-parameterized  case.  We  use  these  results  to  prove  that  the  correctnessproblem  for  immediate  observation  population  protocols  is  PSPACE-complete,answering  a  question  left  open  in  a  previous  paper.</p>
http://arxiv.org/abs/1902.08647|Better  Algorithms  for  Stochastic  Bandits  with  Adversarial  Corruptions.  (arXiv:1902.08647v2  [cs.LG]  UPDATED)|<p>We  study  the  stochastic  multi-armed  bandits  problem  in  the  presence  ofadversarial  corruption.  We  present  a  new  algorithm  for  this  problem  whoseregret  is  nearly  optimal,  substantially  improving  upon  previous  work.  Ouralgorithm  is  agnostic  to  the  level  of  adversarial  contamination  and  cantolerate  a  significant  amount  of  corruption  with  virtually  no  degradation  inperformance.</p>
http://arxiv.org/abs/1902.10068|Entity  Recognition  at  First  Sight:  Improving  NER  with  Eye  Movement  Information.  (arXiv:1902.10068v2  [cs.CL]  UPDATED)|<p>Previous  research  shows  that  eye-tracking  data  contains  information  about  thelexical  and  syntactic  properties  of  text,  which  can  be  used  to  improve  naturallanguage  processing  models.  In  this  work,  we  leverage  eye  movement  featuresfrom  three  corpora  with  recorded  gaze  information  to  augment  a  state-of-the-artneural  model  for  named  entity  recognition  (NER)  with  gaze  embeddings.  Thesecorpora  were  manually  annotated  with  named  entity  labels.  Moreover,  we  show  howgaze  features,  generalized  on  word  type  level,  eliminate  the  need  for  recordedeye-tracking  data  at  test  time.  The  gaze-augmented  models  for  NER  usingtoken-level  and  type-level  features  outperform  the  baselines.  We  present  thebenefits  of  eye-tracking  features  by  evaluating  the  NER  models  on  bothindividual  datasets  as  well  as  in  cross-domain  settings.</p>
http://arxiv.org/abs/1903.01784|Leveraging  Shape  Completion  for  3D  Siamese  Tracking.  (arXiv:1903.01784v2  [cs.CV]  UPDATED)|<p>Point  clouds  are  challenging  to  process  due  to  their  sparsity,  thereforeautonomous  vehicles  rely  more  on  appearance  attributes  than  pure  geometricfeatures.  However,  3D  LIDAR  perception  can  provide  crucial  information  forurban  navigation  in  challenging  light  or  weather  conditions.  In  this  paper,  weinvestigate  the  versatility  of  Shape  Completion  for  3D  Object  Tracking  in  LIDARpoint  clouds.  We  design  a  Siamese  tracker  that  encodes  model  and  candidateshapes  into  a  compact  latent  representation.  We  regularize  the  encoding  byenforcing  the  latent  representation  to  decode  into  an  object  model  shape.  Weobserve  that  3D  object  tracking  and  3D  shape  completion  complement  each  other.Learning  a  more  meaningful  latent  representation  shows  better  discriminatorycapabilities,  leading  to  improved  tracking  performance.  We  test  our  method  onthe  KITTI  Tracking  set  using  car  3D  bounding  boxes.  Our  model  reaches  a  76.94%Success  rate  and  81.38%  Precision  for  3D  Object  Tracking,  with  the  shapecompletion  regularization  leading  to  an  improvement  of  3%  in  both  metrics.</p>
http://arxiv.org/abs/1903.03404|Accelerating  Generalized  Linear  Models  with  MLWeaving:  A  One-Size-Fits-All  System  for  Any-precision  Learning  (Technical  Report).  (arXiv:1903.03404v2  [cs.DS]  UPDATED)|<p>Learning  from  the  data  stored  in  a  database  is  an  important  functionincreasingly  available  in  relational  engines.  Methods  using  lower  precisioninput  data  are  of  special  interest  given  their  overall  higher  efficiency  but,in  databases,  these  methods  have  a  hidden  cost:  the  quantization  of  the  realvalue  into  a  smaller  number  is  an  expensive  step.  To  address  the  issue,  in  thispaper  we  present  MLWeaving,  a  data  structure  and  hardware  accelerationtechnique  intended  to  speed  up  learning  of  generalized  linear  models  indatabases.  ML-Weaving  provides  a  compact,  in-memory  representation  enabling  theretrieval  of  data  at  any  level  of  precision.  MLWeaving  also  takes  advantage  ofthe  increasing  availability  of  FPGA-based  accelerators  to  provide  a  highlyefficient  implementation  of  stochastic  gradient  descent.  The  solution  adoptedin  MLWeaving  is  more  efficient  than  existing  designs  in  terms  of  space  (sinceit  can  process  any  resolution  on  the  same  design)  and  resources  (via  the  use  ofbit-serial  multipliers).  MLWeaving  also  enables  the  runtime  tuning  ofprecision,  instead  of  a  fixed  precision  level  during  the  training.  Weillustrate  this  using  a  simple,  dynamic  precision  schedule.  Experimentalresults  show  MLWeaving  achieves  up  to16  performance  improvement  overlow-precision  CPU  implementations  of  first-order  methods.</p>
http://arxiv.org/abs/1903.04448|Pragmatic  inference  and  visual  abstraction  enable  contextual  flexibility  during  visual  communication.  (arXiv:1903.04448v2  [cs.OH]  UPDATED)|<p>Visual  modes  of  communication  are  ubiquitous  in  modern  life  ---  from  maps  todata  plots  to  political  cartoons.  Here  we  investigate  drawing,  the  most  basicform  of  visual  communication.  Participants  were  paired  in  an  online  environmentto  play  a  drawing-based  reference  game.  On  each  trial,  both  participants  wereshown  the  same  four  objects,  but  in  different  locations.  The  sketcher's  goalwas  to  draw  one  of  these  objects  so  that  the  viewer  could  select  it  from  thearray.  On  `close'  trials,  objects  belonged  to  the  same  basic-level  category,whereas  on  `far'  trials  objects  belonged  to  different  categories.  We  found  thatpeople  exploited  shared  information  to  efficiently  communicate  about  the  targetobject:  on  far  trials,  sketchers  achieved  high  recognition  accuracy  whileapplying  fewer  strokes,  using  less  ink,  and  spending  less  time  on  theirdrawings  than  on  close  trials.  We  hypothesized  that  humans  succeed  in  this  taskby  recruiting  two  core  faculties:  visual  abstraction,  the  ability  to  perceivethe  correspondence  between  an  object  and  a  drawing  of  it;  and  pragmaticinference,  the  ability  to  judge  what  information  would  help  a  viewerdistinguish  the  target  from  distractors.  To  evaluate  this  hypothesis,  wedeveloped  a  computational  model  of  the  sketcher  that  embodied  both  faculties,instantiated  as  a  deep  convolutional  neural  network  nested  within  aprobabilistic  program.  We  found  that  this  model  fit  human  data  well  andoutperformed  lesioned  variants.  Together,  this  work  provides  the  firstalgorithmically  explicit  theory  of  how  visual  perception  and  social  cognitionjointly  support  contextual  flexibility  in  visual  communication.</p>
http://arxiv.org/abs/1903.04596|Quality-Gated  Convolutional  LSTM  for  Enhancing  Compressed  Video.  (arXiv:1903.04596v2  [cs.CV]  UPDATED)|"<p>The  past  decade  has  witnessed  great  success  in  applying  deep  learning  toenhance  the  quality  of  compressed  video.  However,  the  existing  approaches  aimat  quality  enhancement  on  a  single  frame,  or  only  using  fixed  neighboringframes.  Thus  they  fail  to  take  full  advantage  of  the  inter-frame  correlation  inthe  video.  This  paper  proposes  the  Quality-Gated  Convolutional  Long  Short-TermMemory  (QG-ConvLSTM)  network  with  bi-directional  recurrent  structure  to  fullyexploit  the  advantageous  information  in  a  large  range  of  frames.  Moreimportantly,  due  to  the  obvious  quality  fluctuation  among  compressed  frames,higher  quality  frames  can  provide  more  useful  information  for  other  frames  toenhance  quality.  Therefore,  we  propose  learning  the  ""forget""  and  ""input""  gatesin  the  ConvLSTM  cell  from  quality-related  features.  As  such,  the  frames  withvarious  quality  contribute  to  the  memory  in  ConvLSTM  with  different  importance,making  the  information  of  each  frame  reasonably  and  adequately  used.  Finally,the  experiments  validate  the  effectiveness  of  our  QG-ConvLSTM  approach  inadvancing  the  state-of-the-art  quality  enhancement  of  compressed  video,  and  theablation  study  shows  that  our  QG-ConvLSTM  approach  is  learnt  to  make  atrade-off  between  quality  and  correlation  when  leveraging  multi-frameinformation.</p>"
http://arxiv.org/abs/1903.05726|A  Multi-armed  Bandit  MCMC,  with  applications  in  sampling  from  doubly  intractable  posterior.  (arXiv:1903.05726v2  [stat.CO]  UPDATED)|<p>Markov  chain  Monte  Carlo  (MCMC)  algorithms  are  widely  used  to  sample  fromcomplicated  distributions,  especially  to  sample  from  the  posterior  distributionin  Bayesian  inference.  However,  MCMC  is  not  directly  applicable  when  facing  thedoubly  intractable  problem.  In  this  paper,  we  discussed  and  compared  twoexisting  solutions  --  Pseudo-marginal  Monte  Carlo  and  Exchange  Algorithm.  Thispaper  also  proposes  a  novel  algorithm:  Multi-armed  Bandit  MCMC  (MABMC),  whichchooses  between  two  (or  more)  randomized  acceptance  ratios  in  each  step.  MABMCcould  be  applied  directly  to  incorporate  Pseudo-marginal  Monte  Carlo  andExchange  algorithm,  with  higher  average  acceptance  probability.</p>
http://arxiv.org/abs/1903.05759|Consistent  Dialogue  Generation  with  Self-supervised  Feature  Learning.  (arXiv:1903.05759v2  [cs.CL]  UPDATED)|<p>Generating  responses  that  are  consistent  with  the  dialogue  context  is  one  ofthe  central  challenges  in  building  engaging  conversational  agents.  In  thispaper,  we  propose  a  neural  conversation  model  that  generates  consistentresponses  by  maintaining  certain  features  related  to  topics  and  personasthroughout  the  conversation.  Unlike  past  work  that  requires  externalsupervision  such  as  user  identities,  which  are  often  unavailable  or  classifiedas  sensitive  information,  our  approach  trains  topic  and  persona  featureextractors  in  a  self-supervised  way  by  utilizing  the  natural  structure  ofdialogue  data.  Moreover,  we  adopt  a  binary  feature  representation  and  introducea  feature  disentangling  loss  which,  paired  with  controllable  responsegeneration  techniques,  allows  us  to  promote  or  demote  certain  learned  topicsand  personas  features.  The  evaluation  result  demonstrates  the  model'scapability  of  capturing  meaningful  topics  and  personas  features,  and  theincorporation  of  the  learned  features  brings  significant  improvement  in  termsof  the  quality  of  generated  responses  on  two  datasets,  even  comparing  withmodel  which  explicit  persona  information.</p>
http://arxiv.org/abs/1903.06396|COCO:  The  Large  Scale  Black-Box  Optimization  Benchmarking  (bbob-largescale)  Test  Suite.  (arXiv:1903.06396v2  [math.OC]  UPDATED)|<p>The  bbob-largescale  test  suite,  containing  24  single-objective  functions  incontinuous  domain,  extends  the  well-known  single-objective  noiseless  bbob  testsuite,  which  has  been  used  since  2009  in  the  BBOB  workshop  series,  to  largedimension.  The  core  idea  is  to  make  the  rotational  transformations  R,  Q  insearch  space  that  appear  in  the  bbob  test  suite  computationally  cheaper  whileretaining  some  desired  properties.  This  documentation  presents  an  approach  thatreplaces  a  full  rotational  transformation  with  a  combination  of  ablock-diagonal  matrix  and  two  permutation  matrices  in  order  to  construct  testfunctions  whose  computational  and  memory  costs  scale  linearly  in  the  dimensionof  the  problem.</p>
http://arxiv.org/abs/1903.06473|DeepHuman:  3D  Human  Reconstruction  from  a  Single  Image.  (arXiv:1903.06473v2  [cs.CV]  UPDATED)|<p>We  propose  DeepHuman,  an  image-guided  volume-to-volume  translation  CNN  for  3Dhuman  reconstruction  from  a  single  RGB  image.  To  reduce  the  ambiguitiesassociated  with  the  surface  geometry  reconstruction,  even  for  thereconstruction  of  invisible  areas,  we  propose  and  leverage  a  dense  semanticrepresentation  generated  from  SMPL  model  as  an  additional  input.  One  keyfeature  of  our  network  is  that  it  fuses  different  scales  of  image  features  intothe  3D  space  through  volumetric  feature  transformation,  which  helps  to  recoveraccurate  surface  geometry.  The  visible  surface  details  are  further  refinedthrough  a  normal  refinement  network,  which  can  be  concatenated  with  the  volumegeneration  network  using  our  proposed  volumetric  normal  projection  layer.  Wealso  contribute  THuman,  a  3D  real-world  human  model  dataset  containing  about7000  models.  The  network  is  trained  using  training  data  generated  from  thedataset.  Overall,  due  to  the  specific  design  of  our  network  and  the  diversityin  our  dataset,  our  method  enables  3D  human  model  estimation  given  only  asingle  image  and  outperforms  state-of-the-art  approaches.</p>
http://arxiv.org/abs/1903.06494|Content  Differences  in  Syntactic  and  Semantic  Representations.  (arXiv:1903.06494v3  [cs.CL]  UPDATED)|<p>Syntactic  analysis  plays  an  important  role  in  semantic  parsing,  but  thenature  of  this  role  remains  a  topic  of  ongoing  debate.  The  debate  has  beenconstrained  by  the  scarcity  of  empirical  comparative  studies  between  syntacticand  semantic  schemes,  which  hinders  the  development  of  parsing  methods  informedby  the  details  of  target  schemes  and  constructions.  We  target  this  gap,  andtake  Universal  Dependencies  (UD)  and  UCCA  as  a  test  case.  After  abstractingaway  from  differences  of  convention  or  formalism,  we  find  that  most  contentdivergences  can  be  ascribed  to:  (1)  UCCA's  distinction  between  a  Scene  and  anon-Scene;  (2)  UCCA's  distinction  between  primary  relations,  secondary  ones  andparticipants;  (3)  different  treatment  of  multi-word  expressions,  and  (4)different  treatment  of  inter-clause  linkage.  We  further  discuss  the  long  tailof  cases  where  the  two  schemes  take  markedly  different  approaches.  Finally,  weshow  that  the  proposed  comparison  methodology  can  be  used  for  fine-grainedevaluation  of  UCCA  parsing,  highlighting  both  challenges  and  potential  sourcesfor  improvement.  The  substantial  differences  between  the  schemes  suggest  thatsemantic  parsers  are  likely  to  benefit  downstream  text  understandingapplications  beyond  their  syntactic  counterparts.</p>
http://arxiv.org/abs/1903.06888|Rethinking  Uplink  Hybrid  Processing:  When  is  Pure  Analog  Processing  Suggested?.  (arXiv:1903.06888v2  [cs.IT]  UPDATED)|<p>In  this  correspondence,  we  analytically  characterize  the  benefit  of  digitalprocessing  in  uplink  massive  multiple-input  multiple-output  (MIMO)  withsub-connected  hybrid  architecture.  By  assuming  that  the  number  of  radiofrequency  (RF)  chains  is  equal  to  that  of  users,  we  characterize  achievablerates  of  both  pure  analog  detection  and  hybrid  detection  under  the  i.i.d.Rayleigh  fading  channel  model.  From  the  derived  expressions,  we  discover  thatthe  analog  processing  can  outperform  the  hybrid  processing  using  the  maximalratio  combining  (MRC)  or  zero-forcing  (ZF)  criterion  in  cases  under  someengineering  assumptions.  Performance  comparison  of  the  schemes  are  presentedunder  tests  with  various  numbers  of  users  and  numbers  of  antennas  at  the  basestation.</p>
http://arxiv.org/abs/1903.06961|Entropy  modulo  a  prime.  (arXiv:1903.06961v2  [math.NT]  UPDATED)|"<p>Building  on  work  of  Kontsevich,  we  introduce  a  definition  of  the  entropy  of  afinite  probability  distribution  in  which  the  ""probabilities""  are  integersmodulo  a  prime  p.  The  entropy,  too,  is  an  integer  mod  p.  Entropy  mod  p  is  shownto  be  uniquely  characterized  by  a  functional  equation  identical  to  the  one  thatcharacterizes  ordinary  Shannon  entropy.  We  also  establish  a  sense  in  whichcertain  real  entropies  have  residues  mod  p,  connecting  the  concepts  of  entropyover  R  and  over  Z/pZ.  Finally,  entropy  mod  p  is  expressed  as  a  polynomial  whichis  shown  to  satisfy  several  identities,  linking  into  work  of  Cathelineau,Elbaz-Vincent  and  Gangl  on  polylogarithms.</p>"
http://arxiv.org/abs/1903.07009|Multi-Authority  Attribute-Based  Access  Control  with  Smart  Contract.  (arXiv:1903.07009v3  [cs.CR]  UPDATED)|<p>Attribute-based  access  control  makes  access  control  decisions  based  on  theassigned  attributes  of  subjects  and  the  access  policies  to  protect  objects  bymediating  operations  from  the  subjects.  Authority,  which  validates  attributesof  subjects,  is  one  key  component  to  facilitate  attribute-based  access  control.In  an  increasingly  decentralized  society,  multiple  attributes  possessed  bysubjects  may  need  to  be  validated  by  multiple  different  authorities.  This  paperproposes  a  multi-authority  attribute-based  access  control  scheme  by  usingEthereum's  smart  contracts.  In  the  proposed  scheme,  Ethereum  smart  contractsare  created  to  define  the  interactions  between  data  owner,  data  user,  andmultiple  attribute  authorities.  A  data  user  presents  its  attributes  todifferent  attribute  authorities,  and  after  successful  validation  of  attributes,obtains  attribute  tokens  from  respective  attribute  authorities.  Aftercollecting  enough  attribute  tokens,  a  smart  contract  will  be  executed  to  issuesecret  key  to  the  data  user  to  access  the  requested  object.  The  smart  contractsfor  multi-authority  attribute-based  access  control  have  been  prototyped  inSolidity,  and  their  performance  has  been  evaluated  on  the  Rinkeby  EthereumTestnet.</p>
http://arxiv.org/abs/1903.07807|Distributed  Kalman-filtering:  Distributed  optimization  viewpoint.  (arXiv:1903.07807v2  [math.OC]  UPDATED)|<p>We  consider  the  Kalman-filtering  problem  with  multiple  sensors  which  areconnected  through  a  communication  network.  If  all  measurements  are  delivered  toone  place  called  fusion  center  and  processed  together,  we  call  the  processcentralized  Kalman-filtering  (CKF).  When  there  is  no  fusion  center,  each  sensorcan  also  solve  the  problem  by  using  local  measurements  and  exchanginginformation  with  its  neighboring  sensors,  which  is  called  distributedKalman-filtering  (DKF).  Noting  that  CKF  problem  is  a  maximum  likelihoodestimation  problem,  which  is  a  quadratic  optimization  problem,  we  reformulateDKF  problem  as  a  consensus  optimization  problem,  resulting  in  that  DKF  problemcan  be  solved  by  many  existing  distributed  optimization  algorithms.  A  new  DKFalgorithm  employing  the  distributed  dual  ascent  method  is  provided  and  itsperformance  is  evaluated  through  numerical  experiments.</p>
http://arxiv.org/abs/1903.07864|Class-incremental  Learning  via  Deep  Model  Consolidation.  (arXiv:1903.07864v2  [cs.CV]  UPDATED)|"<p>Deep  neural  networks  (DNNs)  often  suffer  from  ""catastrophic  forgetting""during  incremental  learning  (IL)  ---  an  abrupt  degradation  of  performance  onthe  original  set  of  classes  when  the  training  objective  is  adapted  to  a  newlyadded  set  of  classes.  Existing  IL  approaches  tend  to  produce  a  model  that  isbiased  towards  either  the  old  classes  or  new  classes,  unless  with  the  help  ofexemplars  of  the  old  data.  To  address  this  issue,  we  propose  aclass-incremental  learning  paradigm  called  Deep  Model  Consolidation  (DMC),which  works  well  even  when  the  original  training  data  is  not  available.  Theidea  is  to  first  train  a  separate  model  only  for  the  new  classes,  and  thencombine  the  two  individual  models  trained  on  data  of  two  distinct  set  ofclasses  (old  classes  and  new  classes)  via  a  novel  dual  distillation  trainingobjective.  The  two  existing  models  are  consolidated  by  exploiting  publiclyavailable  unlabeled  auxiliary  data.  This  overcomes  the  potential  difficultiesdue  to  unavailability  of  original  training  data.  Compared  to  thestate-of-the-art  techniques,  DMC  demonstrates  significantly  better  performancein  CIFAR-100  image  classification  and  PASCAL  VOC  2007  object  detectionbenchmarks  in  the  single-headed  IL  setting.</p>"
http://arxiv.org/abs/1903.08510|Topological  Data  Analysis  in  Information  Space.  (arXiv:1903.08510v2  [cs.CG]  UPDATED)|<p>Various  kinds  of  data  are  routinely  represented  as  discrete  probabilitydistributions.  Examples  include  text  documents  summarized  by  histograms  of  wordoccurrences  and  images  represented  as  histograms  of  oriented  gradients.  Viewinga  discrete  probability  distribution  as  a  point  in  the  standard  simplex  of  theappropriate  dimension,  we  can  understand  collections  of  such  objects  ingeometric  and  topological  terms.  Importantly,  instead  of  using  the  standardEuclidean  distance,  we  look  into  dissimilarity  measures  withinformation-theoretic  justification,  and  we  develop  the  theory  needed  forapplying  topological  data  analysis  in  this  setting.  In  doing  so,  we  emphasizeconstructions  that  enable  usage  of  existing  computational  topology  software  inthis  context.</p>
http://arxiv.org/abs/1903.09465|Managing  Recurrent  Virtual  Network  Updates  in  Multi-Tenant  Datacenters:  A  System  Perspective.  (arXiv:1903.09465v2  [cs.CR]  UPDATED)|<p>With  the  advent  of  software-defined  networking,  network  configuration  throughprogrammable  interfaces  becomes  practical,  leading  to  various  on-demandopportunities  for  network  routing  update  in  multi-tenant  datacenters,  wheretenants  have  diverse  requirements  on  network  routings  such  as  short  latency,low  path  inflation,  large  bandwidth,  high  reliability,  etc.  Conventionalsolutions  that  rely  on  topology  search  coupled  with  an  objective  function  tofind  desired  routings  have  at  least  two  shortcomings:  (i)  they  run  intoscalability  issues  when  handling  consistent  and  frequent  routing  updates  and(ii)  they  restrict  the  flexibility  and  capability  to  satisfy  various  routingrequirements.  To  address  these  issues,  this  paper  proposes  a  novel  search  andoptimization  decoupled  design,  which  not  only  saves  considerable  topologysearch  costs  via  search  result  reuse,  but  also  avoids  possible  sub-optimalityin  greedy  routing  search  algorithms  by  making  decisions  based  on  the  globalview  of  all  possible  routings.  We  implement  a  prototype  of  our  proposed  system,OpReduce,  and  perform  extensive  evaluations  to  validate  its  design  goals.</p>
http://arxiv.org/abs/1903.09798|Spatially-weighted  Anomaly  Detection  with  Regression  Model.  (arXiv:1903.09798v2  [cs.CV]  UPDATED)|<p>Visual  anomaly  detection  is  common  in  several  applications  including  medicalscreening  and  production  quality  check.  Although  a  definition  of  the  anomaly  isan  unknown  trend  in  data,  in  many  cases  some  hints  or  samples  of  the  anomalyclass  can  be  given  in  advance.  Conventional  methods  cannot  use  the  availableanomaly  data,  and  also  do  not  have  a  robustness  of  noise.  In  this  paper,  wepropose  a  novel  spatially-weighted  reconstruction-loss-based  anomaly  detectionwith  a  likelihood  value  from  a  regression  model  trained  by  all  known  data.  Thespatial  weights  are  calculated  by  a  region  of  interest  generated  from  employingvisualization  of  the  regression  model.  We  introduce  some  ways  to  combine  withvarious  strategies  to  propose  a  state-of-the-art  method.  Comparing  with  othermethods  on  three  different  datasets,  we  empirically  verify  the  proposed  methodperforms  better  than  the  others.</p>
http://arxiv.org/abs/1903.10081|Determining  satisfiability  of  3-SAT  in  polynomial  time.  (arXiv:1903.10081v3  [cs.DS]  UPDATED)|<p>In  this  paper,  we  provide  a  polynomial  time  (and  space),  algorithm  thatdetermines  satisfiability  of  3-SAT.  The  complexity  analysis  for  the  algorithmtakes  into  account  no  efficiency  and  yet  provides  a  low  enough  bound,  thatefficient  versions  are  practical  with  respect  to  today's  hardware.  We  accompanythis  paper  with  a  serial  version  of  the  algorithm  without  non-trivialefficiencies.</p>
http://arxiv.org/abs/1903.10153|DenseBody:  Directly  Regressing  Dense  3D  Human  Pose  and  Shape  From  a  Single  Color  Image.  (arXiv:1903.10153v3  [cs.CV]  UPDATED)|<p>Recovering  3D  human  body  shape  and  pose  from  2D  images  is  a  challenging  taskdue  to  high  complexity  and  flexibility  of  human  body,  and  relatively  less  3Dlabeled  data.  Previous  methods  addressing  these  issues  typically  rely  onpredicting  intermediate  results  such  as  body  part  segmentation,  2D/3D  joints,silhouette  mask  to  decompose  the  problem  into  multiple  sub-tasks  in  order  toutilize  more  2D  labels.  Most  previous  works  incorporated  parametric  body  shapemodel  in  their  methods  and  predict  parameters  in  low-dimensional  space  torepresent  human  body.  In  this  paper,  we  propose  to  directly  regress  the  3Dhuman  mesh  from  a  single  color  image  using  Convolutional  Neural  Network(CNN).We  use  an  efficient  representation  of  3D  human  shape  and  pose  which  can  bepredicted  through  an  encoder-decoder  neural  network.  The  proposed  methodachieves  state-of-the-art  performance  on  several  3D  human  body  datasetsincluding  Human3.6M,  SURREAL  and  UP-3D  with  even  faster  running  speed.</p>
http://arxiv.org/abs/1903.10175|A  Novel  Method  for  the  Absolute  Pose  Problem  with  Pairwise  Constraints.  (arXiv:1903.10175v2  [cs.CV]  UPDATED)|<p>Absolute  pose  estimation  is  a  fundamental  problem  in  computer  vision,  and  itis  a  typical  parameter  estimation  problem,  meaning  that  efforts  to  solve  itwill  always  suffer  from  outlier-contaminated  data.  Conventionally,  for  a  fixeddimensionality  d  and  the  number  of  measurements  N,  a  robust  estimation  problemcannot  be  solved  faster  than  O(N^d).  Furthermore,  it  is  almost  impossible  toremove  d  from  the  exponent  of  the  runtime  of  a  globally  optimal  algorithm.However,  absolute  pose  estimation  is  a  geometric  parameter  estimation  problem,and  thus  has  special  constraints.  In  this  paper,  we  consider  pairwiseconstraints  and  propose  a  globally  optimal  algorithm  for  solving  the  absolutepose  estimation  problem.  The  proposed  algorithm  has  a  linear  complexity  in  thenumber  of  correspondences  at  a  given  outlier  ratio.  Concretely,  we  firstdecouple  the  rotation  and  the  translation  subproblems  by  utilizing  the  pairwiseconstraints,  and  then  we  solve  the  rotation  subproblem  using  thebranch-and-bound  algorithm.  Lastly,  we  estimate  the  translation  based  on  theknown  rotation  by  using  another  branch-and-bound  algorithm.  The  advantages  ofour  method  are  demonstrated  via  thorough  testing  on  both  synthetic  andreal-world  data</p>
http://arxiv.org/abs/1903.10831|Attention  Based  Glaucoma  Detection:  A  Large-scale  Database  and  CNN  Model.  (arXiv:1903.10831v2  [cs.CV]  UPDATED)|<p>Recently,  the  attention  mechanism  has  been  successfully  applied  inconvolutional  neural  networks  (CNNs),  significantly  boosting  the  performance  ofmany  computer  vision  tasks.  Unfortunately,  few  medical  image  recognitionapproaches  incorporate  the  attention  mechanism  in  the  CNNs.  In  particular,there  exists  high  redundancy  in  fundus  images  for  glaucoma  detection,  such  thatthe  attention  mechanism  has  potential  in  improving  the  performance  of  CNN-basedglaucoma  detection.  This  paper  proposes  an  attention-based  CNN  for  glaucomadetection  (AG-CNN).  Specifically,  we  first  establish  a  large-scale  attentionbased  glaucoma  (LAG)  database,  which  includes  5,824  fundus  images  labeled  witheither  positive  glaucoma  (2,392)  or  negative  glaucoma  (3,432).  The  attentionmaps  of  the  ophthalmologists  are  also  collected  in  LAG  database  through  asimulated  eye-tracking  experiment.  Then,  a  new  structure  of  AG-CNN  is  designed,including  an  attention  prediction  subnet,  a  pathological  area  localizationsubnet  and  a  glaucoma  classification  subnet.  Different  from  otherattention-based  CNN  methods,  the  features  are  also  visualized  as  the  localizedpathological  area,  which  can  advance  the  performance  of  glaucoma  detection.Finally,  the  experiment  results  show  that  the  proposed  AG-CNN  approachsignificantly  advances  state-of-the-art  glaucoma  detection.</p>
http://arxiv.org/abs/1903.11114|SUSI:  Supervised  Self-Organizing  Maps  for  Regression  and  Classification  in  Python.  (arXiv:1903.11114v2  [cs.LG]  UPDATED)|<p>In  many  research  fields,  the  sizes  of  the  existing  datasets  vary  widely.Hence,  there  is  a  need  for  machine  learning  techniques  which  are  well-suitedfor  these  different  datasets.  One  possible  technique  is  the  self-organizing  map(SOM),  a  type  of  artificial  neural  network  which  is,  so  far,  weakly  representedin  the  field  of  machine  learning.  The  SOM's  unique  characteristic  is  theneighborhood  relationship  of  the  output  neurons.  This  relationship  improves  theability  of  generalization  on  small  datasets.  SOMs  are  mostly  applied  inunsupervised  learning  and  few  studies  focus  on  using  SOMs  as  supervisedlearning  approach.  Furthermore,  no  appropriate  SOM  package  is  available  withrespect  to  machine  learning  standards  and  in  the  widely  used  programminglanguage  Python.  In  this  paper,  we  introduce  the  freely  available  SUpervisedSelf-organIzing  maps  (SUSI)  Python  package  which  performs  supervised  regressionand  classification.  The  implementation  of  SUSI  is  described  with  respect  to  theunderlying  mathematics.  Then,  we  present  first  evaluations  of  the  SOM  forregression  and  classification  datasets  from  two  different  domains  of  geospatialimage  analysis.  Despite  the  early  stage  of  its  development,  the  SUSI  frameworkperforms  well  and  is  characterized  by  only  small  performance  differencesbetween  the  training  and  the  test  datasets.  A  comparison  of  the  SUSI  frameworkwith  existing  Python  and  R  packages  demonstrates  the  importance  of  the  SUSIframework.  In  future  work,  the  SUSI  framework  will  be  extended,  optimized  andupgraded  e.g.  with  tools  to  better  understand  and  visualize  the  input  data  aswell  as  the  handling  of  missing  and  incomplete  data.</p>
http://arxiv.org/abs/1903.11210|Colorectal  cancer  diagnosis  from  histology  images:  A  comparative  study.  (arXiv:1903.11210v2  [cs.CV]  UPDATED)|<p>Computer-aided  diagnosis  (CAD)  based  on  histopathological  imaging  hasprogressed  rapidly  in  recent  years  with  the  rise  of  machine  learning  basedmethodologies.  Traditional  approaches  consist  of  training  a  classificationmodel  using  features  extracted  from  the  images,  based  on  textures  ormorphological  properties.  Recently,  deep-learning  based  methods  have  beenapplied  directly  to  the  raw  (unprocessed)  data.  However,  their  usability  isimpacted  by  the  paucity  of  annotated  data  in  the  biomedical  sector.  In  order  toleverage  the  learning  capabilities  of  deep  Convolutional  Neural  Nets  (CNNs)within  the  confines  of  limited  labelled  data,  in  this  study  we  shallinvestigate  the  transfer  learning  approaches  that  aim  to  apply  the  knowledgegained  from  solving  a  source  (e.g.,  non-medical)  problem,  to  learn  betterpredictive  models  for  the  target  (e.g.,  biomedical)  task.  As  an  alternative,  weshall  further  propose  a  new  adaptive  and  compact  CNN  based  architecture  thatcan  be  trained  from  scratch  even  on  scarce  and  low-resolution  data.  Moreover,we  conduct  quantitative  comparative  evaluations  among  the  traditional  methods,transfer  learning-based  methods  and  the  proposed  adaptive  approach  for  theparticular  task  of  cancer  detection  and  identification  from  scarce  andlow-resolution  histology  images.  Over  the  largest  benchmark  dataset  formed  forthis  purpose,  the  proposed  adaptive  approach  achieved  a  higher  cancer  detectionaccuracy  with  a  significant  gap,  whereas  the  deep  CNNs  with  transfer  learningachieved  a  superior  cancer  identification.</p>
http://arxiv.org/abs/1903.11367|Does  My  Rebuttal  Matter?  Insights  from  a  Major  NLP  Conference.  (arXiv:1903.11367v2  [cs.CL]  UPDATED)|<p>Peer  review  is  a  core  element  of  the  scientific  process,  particularly  inconference-centered  fields  such  as  ML  and  NLP.  However,  only  few  studies  haveevaluated  its  properties  empirically.  Aiming  to  fill  this  gap,  we  present  acorpus  that  contains  over  4k  reviews  and  1.2k  author  responses  from  ACL-2018.We  quantitatively  and  qualitatively  assess  the  corpus.  This  includes  a  pilotstudy  on  paper  weaknesses  given  by  reviewers  and  on  quality  of  authorresponses.  We  then  focus  on  the  role  of  the  rebuttal  phase,  and  propose  a  noveltask  to  predict  after-rebuttal  (i.e.,  final)  scores  from  initial  reviews  andauthor  responses.  Although  author  responses  do  have  a  marginal  (andstatistically  significant)  influence  on  the  final  scores,  especially  forborderline  papers,  our  results  suggest  that  a  reviewer's  final  score  is  largelydetermined  by  her  initial  score  and  the  distance  to  the  other  reviewers'initial  scores.  In  this  context,  we  discuss  the  conformity  bias  inherent  topeer  reviewing,  a  bias  that  has  largely  been  overlooked  in  previous  research.We  hope  our  analyses  will  help  better  assess  the  usefulness  of  the  rebuttalphase  in  NLP  conferences.</p>
http://arxiv.org/abs/1903.11397|Lost  in  translation:  Exposing  hidden  compiler  optimization  opportunities.  (arXiv:1903.11397v2  [cs.PL]  UPDATED)|<p>To  increase  productivity,  today's  compilers  offer  a  two-fold  abstraction:they  hide  hardware  complexity  from  the  software  developer,  and  they  supportmany  architectures  and  programming  languages.  At  the  same  time,  due  to  fiercemarket  competition,  most  processor  vendors  do  not  disclose  many  of  theirimplementation  details.  These  factors  force  software  developers  to  treat  bothcompilers  and  architectures  as  black  boxes.  In  practice,  this  leads  to  asuboptimal  compiler  behavior  where  the  maximum  potential  of  improving  anapplication's  resource  usage,  such  as  execution  time,  is  often  not  realized.This  paper  exposes  missed  optimization  opportunities  and  is  of  interest  to  allthree  communities,  compiler  engineers,  software  developers  and  hardwarearchitects.  By  exploiting  the  behavior  of  the  standard  optimization  levels,such  as  the  -O3,  of  the  LLVM  v6.0  compiler,  we  show  how  to  reveal  hiddencross-architecture  and  architecture-dependent  potential  optimizations  on  twopopular  processors:  the  Intel  i5-6300U,  widely  used  in  portable  PCs,  and  theARM  Cortex-A53-based  Broadcom  BCM2837  used  in  the  Raspberry  Pi  3B+.  The  classicnightly  regression  testing  can  then  be  extended  to  use  the  resource  usage  andcompilation  information  collected  while  exploiting  subsequences  of  the  standardoptimization  levels.  This  provides  a  systematic  means  of  detecting  and  trackingmissed  optimization  opportunities.  The  enhanced  nightly  regression  system  iscapable  of  driving  the  improvement  and  tuning  of  the  compiler's  commonoptimizer</p>
http://arxiv.org/abs/1903.11538|RF-Assisted  Free-Space  Optics  for  5G  Vehicle-to-Vehicle  Communications.  (arXiv:1903.11538v2  [cs.IT]  UPDATED)|<p>Vehicle-to-Vehicle  (V2V)  communications  are  being  proposed,  tested  anddeployed  to  improve  road  safety  and  traffic  efficiency.  However,  the  automotiveindustry  poses  strict  requirements  for  safety-critical  applications,  that  callfor  reliable,  low  latency  and  high  data  rate  communications.  In  this  context,it  is  widely  agreed  that  both  Radio-Frequency  (RF)  technologies  at  mmWaves  andFree-Space  Optics  (FSO)  represent  promising  solutions,  although  theirperformances  are  severely  degraded  by  transmitter-receiver  misalignment  due  tothe  challenging  high-mobility  conditions.  By  combining  RF  and  FSO  technologies,this  paper  proposes  a  FSO-based  V2V  communication  system  where  the  pointingcoordinates  of  laser  sources  are  based  on  vehicle's  information  exchanged  overa  reliable  low-rate  RF  link.  Numerical  simulations  demonstrate  that  suchcompensation  mechanism  is  mandatory  to  counteract  the  unavoidable  misalignmentsinduced  by  vehicle  dynamics,  and  thus  to  enable  FSO  technology  for  V2Vcommunications  even  in  high  mobility  scenarios.</p>
http://arxiv.org/abs/1902.02771|Impact  of  Fully  Connected  Layers  on  Performance  of  Convolutional  Neural  Networks  for  Image  Classification.  (arXiv:1902.02771v2  [cs.CV]  CROSS  LISTED)|<p>The  Convolutional  Neural  Networks  (CNNs),  in  domains  like  computer  vision,mostly  reduced  the  need  for  handcrafted  features  due  to  its  ability  to  learnthe  problem-specific  features  from  the  raw  input  data.  However,  the  selectionof  dataset-specific  CNN  architecture,  which  mostly  performed  by  eitherexperience  or  expertise  is  a  time-consuming  and  error-prone  process.  Toautomate  the  process  of  learning  a  CNN  architecture,  this  letter  attempts  atfinding  the  relationship  between  Fully  Connected  (FC)  layers  with  some  of  thecharacteristics  of  the  datasets.  The  CNN  architectures,  and  recently  data  setsalso,  are  categorized  as  deep,  shallow,  wide,  etc.  This  letter  tries  toformalize  these  terms  along  with  answering  the  following  questions.  (i)  What  isthe  impact  of  deeper/shallow  architectures  on  the  performance  of  the  CNN  w.r.tFC  layers?,  (ii)  How  the  deeper/wider  datasets  influence  the  performance  of  CNNw.r.t  FC  layers?,  and  (iii)  Which  kind  of  architecture  (deeper/  shallower)  isbetter  suitable  for  which  kind  of  (deeper/  wider)  datasets.  To  address  thesefindings,  we  have  performed  experiments  with  three  CNN  architectures  havingdifferent  depths.  The  experiments  are  conducted  by  varying  the  number  of  FClayers.  We  used  four  widely  used  datasets  including  CIFAR-10,  CIFAR-100,  TinyImageNet,  and  CRCHistoPhenotypes  to  justify  our  findings  in  the  context  of  theimage  classification  problem.  The  source  code  of  this  research  is  available  athttps://github.com/shabbeersh/Impact-of-FC-layers.</p>
http://arxiv.org/abs/1903.09354|A  Model  Counter's  Guide  to  Probabilistic  Systems.  (arXiv:1903.09354v1  [cs.LO]  CROSS  LISTED)|<p>In  this  paper,  we  systematize  the  modeling  of  probabilistic  systems  for  thepurpose  of  analyzing  them  with  model  counting  techniques.  Starting  fromunbiased  coin  flips,  we  show  how  to  model  biased  coins,  correlated  coins,  anddistributions  over  finite  sets.  From  there,  we  continue  with  modelingsequential  systems,  such  as  Markov  chains,  and  revisit  the  relationship  betweenweighted  and  unweighted  model  counting.  Thereby,  this  work  provides  aconceptual  framework  for  deriving  #SAT  encodings  for  probabilistic  inference.</p>
http://arxiv.org/abs/1903.11359|Scaling  up  the  randomized  gradient-free  adversarial  attack  reveals  overestimation  of  robustness  using  established  attacks.  (arXiv:1903.11359v1  [cs.LG]  CROSS  LISTED)|<p>Modern  neural  networks  are  highly  non-robust  against  adversarialmanipulation.  A  significant  amount  of  work  has  been  invested  in  techniques  tocompute  lower  bounds  on  robustness  through  formal  guarantees  and  to  buildprovably  robust  model.  However  it  is  still  difficult  to  apply  them  to  largernetworks  or  in  order  to  get  robustness  against  larger  perturbations.  Thusattack  strategies  are  needed  to  provide  tight  upper  bounds  on  the  actualrobustness.  We  significantly  improve  the  randomized  gradient-free  attack  forReLU  networks  [9],  in  particular  by  scaling  it  up  to  large  networks.  We  showthat  our  attack  achieves  similar  or  significantly  smaller  robust  accuracy  thanstate-of-the-art  attacks  like  PGD  or  the  one  of  Carlini  and  Wagner,  thusrevealing  an  overestimation  of  the  robustness  by  these  state-of-the-artmethods.  Our  attack  is  not  based  on  a  gradient  descent  scheme  and  in  this  sensegradient-free,  which  makes  it  less  sensitive  to  the  choice  of  hyperparametersas  no  careful  selection  of  the  stepsize  is  required.</p>
http://arxiv.org/abs/1903.11521|Yet  Another  Tensor  Toolbox  for  discontinuous  Galerkin  methods  and  other  applications.  (arXiv:1903.11521v1  [cs.MS]  CROSS  LISTED)|<p>The  numerical  solution  of  partial  differential  equations  is  at  the  heart  ofmany  grand  challenges  in  supercomputing.  Solvers  based  on  high-orderdiscontinuous  Galerkin  (DG)  discretisation  have  been  shown  to  scale  on  largesupercomputers  with  excellent  performance  and  efficiency,  if  the  implementationexploits  all  levels  of  parallelism  and  is  tailored  to  the  specificarchitecture.  However,  every  year  new  supercomputers  emerge  and  the  list  ofhardware-specific  considerations  grows,  simultaneously  with  the  list  of  desiredfeatures  in  a  DG  code.  Thus  we  believe  that  a  sustainable  DG  code  needs  anabstraction  layer  to  implement  the  numerical  scheme  in  a  suitable  language.  Weexplore  the  possibility  to  abstract  the  numerical  scheme  as  small  tensoroperations,  describe  them  in  a  domain-specific  language  (DSL)  resembling  theEinstein  notation,  and  to  map  them  to  existing  code  generators  which  generatesmall  matrix  matrix  multiplication  routines.  The  compiler  for  our  DSLimplements  classic  optimisations  that  are  used  for  large  tensor  contractions,and  we  present  novel  optimisation  techniques  such  as  equivalent  sparsitypatterns  and  optimal  index  permutations  for  temporary  tensors.  Our  applicationexamples,  which  include  the  earthquake  simulation  software  SeisSol,  show  thatthe  generated  kernels  achieve  over  50  %  peak  performance  while  the  DSLconsiderably  simplifies  the  implementation.</p>
